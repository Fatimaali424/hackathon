"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4401],{2604(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4/lab-12-vla-system","title":"Lab 12: Complete VLA System Implementation","description":"Overview","source":"@site/docs/module-4/lab-12-vla-system.md","sourceDirName":"module-4","slug":"/module-4/lab-12-vla-system","permalink":"/hackathon/docs/module-4/lab-12-vla-system","draft":false,"unlisted":false,"editUrl":"https://github.com/Fatimaali424/hackathon/edit/main/website/docs/module-4/lab-12-vla-system.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7},"sidebar":"tutorialSidebar","previous":{"title":"Lab 11: Voice Command Processing","permalink":"/hackathon/docs/module-4/lab-11-voice-command"},"next":{"title":"Module 4 Assignment: Vision-Language-Action System Implementation","permalink":"/hackathon/docs/module-4/assignment"}}');var i=t(4848),o=t(8453);const a={sidebar_position:7},r="Lab 12: Complete VLA System Implementation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Hardware and Software Requirements",id:"hardware-and-software-requirements",level:2},{value:"Required Hardware",id:"required-hardware",level:3},{value:"Required Software",id:"required-software",level:3},{value:"Lab Setup",id:"lab-setup",level:2},{value:"Environment Configuration",id:"environment-configuration",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: VLA System Architecture",id:"step-1-vla-system-architecture",level:3},{value:"Step 2: Real-time VLA System",id:"step-2-real-time-vla-system",level:3},{value:"Step 3: VLA System with ROS 2 Integration",id:"step-3-vla-system-with-ros-2-integration",level:3},{value:"Step 4: VLA System Optimization",id:"step-4-vla-system-optimization",level:3},{value:"Step 5: Complete VLA System Launch",id:"step-5-complete-vla-system-launch",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Comprehensive Testing Script",id:"comprehensive-testing-script",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:2},{value:"Lab Deliverables",id:"lab-deliverables",level:2},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Extensions (Optional)",id:"extensions-optional",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lab-12-complete-vla-system-implementation",children:"Lab 12: Complete VLA System Implementation"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"This capstone lab integrates all Vision-Language-Action (VLA) concepts into a complete system. You will implement a full pipeline that takes visual input and natural language commands to generate appropriate robotic actions. This represents the culmination of Module 4, combining perception, language understanding, and action execution in a unified framework."}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"After completing this lab, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate vision, language, and action components into a unified system"}),"\n",(0,i.jsx)(n.li,{children:"Implement end-to-end VLA pipeline for robotic applications"}),"\n",(0,i.jsx)(n.li,{children:"Optimize system performance for real-time operation"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate complete VLA system performance"}),"\n",(0,i.jsx)(n.li,{children:"Deploy VLA system on robotic platforms"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Completion of all previous modules (1-4)"}),"\n",(0,i.jsx)(n.li,{children:"Understanding of ROS 2, Isaac ROS, and perception systems"}),"\n",(0,i.jsx)(n.li,{children:"Knowledge of deep learning frameworks (PyTorch/TensorFlow)"}),"\n",(0,i.jsx)(n.li,{children:"Experience with multimodal AI systems"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"hardware-and-software-requirements",children:"Hardware and Software Requirements"}),"\n",(0,i.jsx)(n.h3,{id:"required-hardware",children:"Required Hardware"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"NVIDIA Jetson Orin AGX or equivalent (for edge deployment)"}),"\n",(0,i.jsx)(n.li,{children:"RGB-D camera (Intel RealSense D435 or equivalent)"}),"\n",(0,i.jsx)(n.li,{children:"Mobile robot platform (TurtleBot3, Jackal, or similar)"}),"\n",(0,i.jsx)(n.li,{children:"Microphone for voice commands"}),"\n",(0,i.jsx)(n.li,{children:"Adequate power supply and cooling for Jetson platform"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"required-software",children:"Required Software"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"ROS 2 Humble with Isaac ROS packages"}),"\n",(0,i.jsx)(n.li,{children:"PyTorch with CUDA support"}),"\n",(0,i.jsx)(n.li,{children:"Transformers library"}),"\n",(0,i.jsx)(n.li,{children:"OpenCV and PIL"}),"\n",(0,i.jsx)(n.li,{children:"Gazebo or Isaac Sim for simulation"}),"\n",(0,i.jsx)(n.li,{children:"Docker for containerized deployment"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"lab-setup",children:"Lab Setup"}),"\n",(0,i.jsx)(n.h3,{id:"environment-configuration",children:"Environment Configuration"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Verify Jetson setup:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check JetPack version\ncat /etc/nv_tegra_release\n\n# Check GPU status\nnvidia-smi\n\n# Install required packages\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/l4t-cp38\npip install transformers\npip install torch_tensorrt\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Verify Isaac ROS installation:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check for Isaac ROS packages\napt list --installed | grep isaac-ros\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Set up workspace:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/vla_ws/src\ncd ~/vla_ws\ncolcon build\nsource install/setup.bash\n"})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-vla-system-architecture",children:"Step 1: VLA System Architecture"}),"\n",(0,i.jsx)(n.p,{children:"Create the main VLA system architecture that integrates all components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# vla_system.py\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nimport time\nfrom typing import Dict, Any, Optional, Tuple\nimport threading\nimport queue\nfrom dataclasses import dataclass\n\n@dataclass\nclass VLAInput:\n    """Input data structure for VLA system"""\n    visual_input: torch.Tensor  # [batch, channels, height, width]\n    language_input: str         # Natural language command\n    robot_state: Dict[str, Any] # Current robot state\n    timestamp: float           # Timestamp for synchronization\n\n@dataclass\nclass VLAPrediction:\n    """Output prediction from VLA system"""\n    action: torch.Tensor       # [batch, action_dim] - predicted action\n    confidence: float          # Confidence in prediction\n    attention_map: Optional[torch.Tensor] = None  # Visual attention map\n    language_attention: Optional[torch.Tensor] = None  # Language attention\n\nclass VisionEncoder(nn.Module):\n    """Vision encoder for processing visual input"""\n    def __init__(self, output_dim: int = 512):\n        super().__init__()\n        import torchvision.models as models\n\n        # Use pre-trained ResNet as backbone\n        self.backbone = models.resnet18(pretrained=True)\n\n        # Replace final layer for feature extraction\n        num_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Linear(num_features, output_dim)\n\n        # Add normalization\n        self.norm = nn.LayerNorm(output_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """Forward pass through vision encoder"""\n        features = self.backbone(x)\n        normalized_features = self.norm(features)\n        return normalized_features\n\nclass LanguageEncoder(nn.Module):\n    """Language encoder for processing text input"""\n    def __init__(self, vocab_size: int = 10000, embedding_dim: int = 256, output_dim: int = 512):\n        super().__init__()\n\n        # Embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        # LSTM for sequence processing\n        self.lstm = nn.LSTM(\n            embedding_dim, output_dim // 2,\n            batch_first=True, bidirectional=True\n        )\n\n        # Linear projection to output dimension\n        self.projection = nn.Linear(output_dim, output_dim)\n\n        # Normalization\n        self.norm = nn.LayerNorm(output_dim)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """Forward pass through language encoder"""\n        embedded = self.embedding(x)\n        lstm_out, (hidden, _) = self.lstm(embedded)\n\n        # Use final hidden state (concatenate forward and backward)\n        final_hidden = torch.cat([hidden[-2], hidden[-1]], dim=-1)\n\n        projected = self.projection(final_hidden)\n        normalized = self.norm(projected)\n        return normalized\n\nclass CrossModalAttention(nn.Module):\n    """Cross-modal attention between vision and language"""\n    def __init__(self, dim: int):\n        super().__init__()\n        self.dim = dim\n        self.scale = dim ** -0.5\n\n        # Linear projections\n        self.q_proj = nn.Linear(dim, dim)\n        self.k_proj = nn.Linear(dim, dim)\n        self.v_proj = nn.Linear(dim, dim)\n\n        # Output projection\n        self.out_proj = nn.Linear(dim, dim)\n\n    def forward(self, vision_features: torch.Tensor, language_features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """Apply cross-modal attention"""\n        # Project features\n        Q = self.q_proj(vision_features)\n        K = self.k_proj(language_features)\n        V = self.v_proj(language_features)\n\n        # Compute attention scores\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n\n        # Apply attention\n        attended_features = torch.matmul(attn_weights, V)\n\n        # Output projection\n        output = self.out_proj(attended_features)\n\n        return output, attn_weights\n\nclass ActionDecoder(nn.Module):\n    """Action decoder that generates robot actions"""\n    def __init__(self, input_dim: int, action_space_dim: int, hidden_dim: int = 512):\n        super().__init__()\n\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim // 2, action_space_dim)\n        )\n\n        # Action normalization (for continuous control)\n        self.action_norm = nn.Tanh()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """Generate action from fused features"""\n        raw_action = self.network(x)\n        normalized_action = self.action_norm(raw_action)\n        return normalized_action\n\nclass VLAModel(nn.Module):\n    """Complete VLA model combining vision, language, and action"""\n    def __init__(self,\n                 vocab_size: int = 10000,\n                 action_space_dim: int = 6,  # Example: 6DOF control\n                 feature_dim: int = 512):\n        super().__init__()\n\n        self.vision_encoder = VisionEncoder(feature_dim)\n        self.language_encoder = LanguageEncoder(vocab_size, output_dim=feature_dim)\n        self.cross_attention = CrossModalAttention(feature_dim)\n        self.action_decoder = ActionDecoder(feature_dim * 2, action_space_dim)\n\n        # Fusion layer to combine attended features\n        self.fusion = nn.Linear(feature_dim * 2, feature_dim * 2)\n\n    def forward(self, images: torch.Tensor, text_tokens: torch.Tensor) -> VLAPrediction:\n        """Forward pass through complete VLA model"""\n        # Encode vision and language\n        vision_features = self.vision_encoder(images)\n        language_features = self.language_encoder(text_tokens)\n\n        # Apply cross-modal attention\n        attended_vision, v2l_attention = self.cross_attention(vision_features, language_features)\n        attended_language, l2v_attention = self.cross_attention(language_features, vision_features)\n\n        # Fuse attended features\n        fused_features = torch.cat([attended_vision, attended_language], dim=-1)\n        fused_features = self.fusion(fused_features)\n\n        # Generate action\n        action = self.action_decoder(fused_features)\n\n        # Calculate confidence (simplified)\n        confidence = torch.mean(torch.abs(action)).item()\n\n        return VLAPrediction(\n            action=action,\n            confidence=min(confidence, 1.0),  # Clamp confidence to [0,1]\n            attention_map=v2l_attention,\n            language_attention=l2v_attention\n        )\n\nclass VLAPipeline:\n    """Complete VLA pipeline with preprocessing and postprocessing"""\n    def __init__(self, model: VLAModel, device: str = \'cuda\'):\n        self.model = model\n        self.device = torch.device(device if torch.cuda.is_available() else \'cpu\')\n        self.model.to(self.device)\n\n        # Text tokenizer (simplified - in practice, use proper tokenizer)\n        self.tokenizer = self._create_simple_tokenizer()\n\n        # Image preprocessing\n        self.image_transform = self._create_image_transform()\n\n        # Action postprocessing\n        self.action_scaler = ActionScaler()\n\n    def _create_simple_tokenizer(self):\n        """Create a simple tokenizer for demonstration"""\n        # In practice, use transformers tokenizer\n        vocab = {\n            \'pad\': 0, \'unk\': 1, \'start\': 2, \'end\': 3,\n            \'go\': 4, \'to\': 5, \'the\': 6, \'kitchen\': 7,\n            \'pick\': 8, \'up\': 9, \'red\': 10, \'cup\': 11,\n            \'stop\': 12, \'start\': 13, \'follow\': 14, \'me\': 15\n        }\n        return vocab\n\n    def _create_image_transform(self):\n        """Create image transformation pipeline"""\n        import torchvision.transforms as transforms\n        return transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n    def preprocess_input(self, input_data: VLAInput) -> Tuple[torch.Tensor, torch.Tensor]:\n        """Preprocess input data for model"""\n        # Process image\n        if isinstance(input_data.visual_input, np.ndarray):\n            image_tensor = self.image_transform(input_data.visual_input)\n        else:\n            image_tensor = input_data.visual_input\n\n        # Add batch dimension\n        if len(image_tensor.shape) == 3:\n            image_tensor = image_tensor.unsqueeze(0)\n\n        image_tensor = image_tensor.to(self.device)\n\n        # Process text\n        tokens = self.tokenize(input_data.language_input)\n        text_tensor = torch.tensor([tokens], dtype=torch.long).to(self.device)\n\n        return image_tensor, text_tensor\n\n    def tokenize(self, text: str) -> list:\n        """Convert text to token IDs"""\n        # Simple tokenization for demonstration\n        words = text.lower().split()\n        tokens = []\n        for word in words:\n            token_id = self.tokenizer.get(word, self.tokenizer[\'unk\'])\n            tokens.append(token_id)\n\n        # Add start and end tokens\n        tokens = [self.tokenizer[\'start\']] + tokens + [self.tokenizer[\'end\']]\n\n        # Pad to fixed length (simplified)\n        max_length = 20\n        if len(tokens) < max_length:\n            tokens.extend([self.tokenizer[\'pad\']] * (max_length - len(tokens)))\n        else:\n            tokens = tokens[:max_length]\n\n        return tokens\n\n    def process(self, input_data: VLAInput) -> VLAPrediction:\n        """Process input through VLA pipeline"""\n        # Preprocess\n        images, text_tokens = self.preprocess_input(input_data)\n\n        # Run model\n        with torch.no_grad():\n            prediction = self.model(images, text_tokens)\n\n        return prediction\n\n    def postprocess_action(self, prediction: VLAPrediction, robot_state: Dict[str, Any]) -> Dict[str, Any]:\n        """Postprocess action for robot execution"""\n        # Convert tensor to numpy\n        action_np = prediction.action.cpu().numpy()\n\n        # Scale action to robot-specific ranges\n        scaled_action = self.action_scaler.scale(action_np, robot_state)\n\n        # Convert to robot command format\n        robot_command = {\n            \'linear_velocity\': scaled_action[0] if len(scaled_action) > 0 else 0.0,\n            \'angular_velocity\': scaled_action[1] if len(scaled_action) > 1 else 0.0,\n            \'gripper_position\': scaled_action[2] if len(scaled_action) > 2 else 0.5,\n            \'confidence\': prediction.confidence,\n            \'timestamp\': time.time()\n        }\n\n        return robot_command\n\nclass ActionScaler:\n    """Scale actions to appropriate ranges for robot control"""\n    def __init__(self):\n        # Define action ranges for different robot types\n        self.action_ranges = {\n            \'navigation\': {\n                \'linear_velocity\': (-1.0, 1.0),      # m/s\n                \'angular_velocity\': (-1.0, 1.0),     # rad/s\n            },\n            \'manipulation\': {\n                \'gripper_position\': (0.0, 1.0),      # normalized\n                \'arm_velocity\': (-0.5, 0.5),         # m/s\n            }\n        }\n\n    def scale(self, action: np.ndarray, robot_state: Dict[str, Any]) -> np.ndarray:\n        """Scale action to appropriate range"""\n        # This is a simplified scaling - in practice, use robot-specific kinematics\n        scaled_action = np.copy(action)\n\n        # Clamp to reasonable ranges\n        scaled_action = np.clip(scaled_action, -1.0, 1.0)\n\n        return scaled_action\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-real-time-vla-system",children:"Step 2: Real-time VLA System"}),"\n",(0,i.jsx)(n.p,{children:"Create a real-time system that can process inputs continuously:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# real_time_vla.py\n\nimport threading\nimport queue\nimport time\nfrom collections import deque\nimport cv2\n\nclass RealTimeVLASystem:\n    """Real-time VLA system with buffering and scheduling"""\n    def __init__(self, vla_pipeline: VLAPipeline):\n        self.pipeline = vla_pipeline\n        self.running = False\n\n        # Input queues\n        self.image_queue = queue.Queue(maxsize=10)\n        self.command_queue = queue.Queue(maxsize=10)\n\n        # Output queue\n        self.action_queue = queue.Queue(maxsize=10)\n\n        # Processing threads\n        self.processing_thread = None\n        self.output_thread = None\n\n        # Performance tracking\n        self.processing_times = deque(maxlen=100)\n        self.fps_counter = deque(maxlen=30)\n\n        # Robot state\n        self.robot_state = {\n            \'position\': [0.0, 0.0, 0.0],\n            \'orientation\': [0.0, 0.0, 0.0, 1.0],\n            \'velocity\': [0.0, 0.0],\n            \'gripper\': 0.5,\n            \'timestamp\': time.time()\n        }\n\n    def start(self):\n        """Start the real-time VLA system"""\n        if self.running:\n            return\n\n        self.running = True\n\n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self._processing_loop)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n        # Start output thread\n        self.output_thread = threading.Thread(target=self._output_loop)\n        self.output_thread.daemon = True\n        self.output_thread.start()\n\n        print("Real-time VLA system started")\n\n    def stop(self):\n        """Stop the real-time VLA system"""\n        self.running = False\n\n        if self.processing_thread:\n            self.processing_thread.join(timeout=2)\n        if self.output_thread:\n            self.output_thread.join(timeout=2)\n\n        print("Real-time VLA system stopped")\n\n    def add_image(self, image: np.ndarray):\n        """Add image to processing queue"""\n        try:\n            self.image_queue.put_nowait(image)\n        except queue.Full:\n            # Drop oldest image if queue is full\n            try:\n                self.image_queue.get_nowait()\n                self.image_queue.put_nowait(image)\n            except:\n                pass  # Queue might be empty\n\n    def add_command(self, command: str):\n        """Add command to processing queue"""\n        try:\n            self.command_queue.put_nowait(command)\n        except queue.Full:\n            # Drop oldest command if queue is full\n            try:\n                self.command_queue.get_nowait()\n                self.command_queue.put_nowait(command)\n            except:\n                pass\n\n    def get_action(self) -> Optional[Dict[str, Any]]:\n        """Get next action from output queue"""\n        try:\n            return self.action_queue.get_nowait()\n        except queue.Empty:\n            return None\n\n    def _processing_loop(self):\n        """Main processing loop"""\n        last_command = ""\n\n        while self.running:\n            try:\n                # Get latest image and command\n                image = None\n                command = None\n\n                # Get latest image (non-blocking)\n                try:\n                    while not self.image_queue.empty():\n                        image = self.image_queue.get_nowait()\n                except queue.Empty:\n                    pass\n\n                # Get latest command (non-blocking)\n                try:\n                    while not self.command_queue.empty():\n                        last_command = self.command_queue.get_nowait()\n                except queue.Empty:\n                    pass\n\n                if image is not None and last_command:\n                    # Process with VLA pipeline\n                    start_time = time.time()\n\n                    input_data = VLAInput(\n                        visual_input=image,\n                        language_input=last_command,\n                        robot_state=self.robot_state,\n                        timestamp=time.time()\n                    )\n\n                    prediction = self.pipeline.process(input_data)\n                    robot_command = self.pipeline.postprocess_action(prediction, self.robot_state)\n\n                    # Calculate processing time\n                    processing_time = time.time() - start_time\n                    self.processing_times.append(processing_time)\n\n                    # Add to output queue\n                    try:\n                        self.action_queue.put_nowait(robot_command)\n                    except queue.Full:\n                        pass  # Drop if output queue is full\n\n                    # Update FPS counter\n                    self.fps_counter.append(time.time())\n\n                # Control processing rate\n                time.sleep(0.01)  # ~100Hz processing\n\n            except Exception as e:\n                print(f"Error in processing loop: {e}")\n                time.sleep(0.1)\n\n    def _output_loop(self):\n        """Output loop for sending commands to robot"""\n        while self.running:\n            # In a real system, this would send commands to robot hardware\n            # For this example, we\'ll just print commands\n            time.sleep(0.05)  # 20Hz output rate\n\n    def get_performance_stats(self) -> Dict[str, float]:\n        """Get performance statistics"""\n        if not self.processing_times:\n            avg_processing_time = 0.0\n        else:\n            avg_processing_time = sum(self.processing_times) / len(self.processing_times)\n\n        # Calculate FPS\n        if len(self.fps_counter) > 1:\n            time_window = self.fps_counter[-1] - self.fps_counter[0]\n            if time_window > 0:\n                fps = len(self.fps_counter) / time_window\n            else:\n                fps = 0.0\n        else:\n            fps = 0.0\n\n        return {\n            \'avg_processing_time_ms\': avg_processing_time * 1000,\n            \'std_processing_time_ms\': np.std(self.processing_times) * 1000 if self.processing_times else 0,\n            \'fps\': fps,\n            \'queue_sizes\': {\n                \'image_queue\': self.image_queue.qsize(),\n                \'command_queue\': self.command_queue.qsize(),\n                \'action_queue\': self.action_queue.qsize()\n            }\n        }\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-vla-system-with-ros-2-integration",children:"Step 3: VLA System with ROS 2 Integration"}),"\n",(0,i.jsx)(n.p,{children:"Create the ROS 2 node that integrates the VLA system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# vla_ros_node.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom cv_bridge import CvBridge\nimport numpy as np\nfrom vla_system import VLAModel, VLAPipeline, VLAInput\nfrom real_time_vla import RealTimeVLASystem\n\nclass VLAROSNode(Node):\n    def __init__(self):\n        super().__init__(\'vla_system_node\')\n\n        # Initialize VLA components\n        self.vla_model = VLAModel(vocab_size=10000, action_space_dim=6)\n        self.vla_pipeline = VLAPipeline(self.vla_model)\n        self.vla_system = RealTimeVLASystem(self.vla_pipeline)\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n        self.vla_status_pub = self.create_publisher(String, \'/vla_status\', 10)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, \'/vla_command\', self.command_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'/camera/camera_info\', self.camera_info_callback, 10\n        )\n\n        # Timer for performance monitoring\n        self.monitor_timer = self.create_timer(1.0, self.monitor_performance)\n\n        # Internal state\n        self.current_image = None\n        self.current_command = ""\n        self.camera_info = None\n\n        # Start real-time system\n        self.vla_system.start()\n\n        self.get_logger().info(\'VLA System ROS Node initialized\')\n\n    def image_callback(self, msg):\n        """Process incoming camera image"""\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, \'bgr8\')\n\n            # Add to VLA system\n            self.vla_system.add_image(cv_image)\n\n            # Store for potential use with command\n            self.current_image = cv_image\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def command_callback(self, msg):\n        """Process incoming voice command"""\n        command = msg.data\n        self.get_logger().info(f\'Received command: {command}\')\n\n        # Add to VLA system\n        self.vla_system.add_command(command)\n\n        # Store for potential use with image\n        self.current_command = command\n\n    def camera_info_callback(self, msg):\n        """Store camera calibration information"""\n        self.camera_info = msg\n\n    def monitor_performance(self):\n        """Monitor and publish performance statistics"""\n        stats = self.vla_system.get_performance_stats()\n\n        status_msg = String()\n        status_msg.data = (\n            f"VLA System Status - "\n            f"FPS: {stats[\'fps\']:.1f}, "\n            f"Processing: {stats[\'avg_processing_time_ms\']:.1f}ms, "\n            f"Queue Sizes: {stats[\'queue_sizes\']}"\n        )\n\n        self.vla_status_pub.publish(status_msg)\n\n        # Log performance if processing is slow\n        if stats[\'avg_processing_time_ms\'] > 100:  # 100ms threshold\n            self.get_logger().warn(\n                f\'High processing time: {stats["avg_processing_time_ms"]:.1f}ms\'\n            )\n\n    def process_vla_output(self):\n        """Process VLA system output and send to robot"""\n        action = self.vla_system.get_action()\n        if action:\n            # Convert to Twist message for differential drive robot\n            twist_msg = Twist()\n            twist_msg.linear.x = float(action.get(\'linear_velocity\', 0.0))\n            twist_msg.angular.z = float(action.get(\'angular_velocity\', 0.0))\n\n            # Publish command\n            self.cmd_vel_pub.publish(twist_msg)\n\n            self.get_logger().info(\n                f\'Sent command: linear={twist_msg.linear.x:.2f}, \'\n                f\'angular={twist_msg.angular.z:.2f}, \'\n                f\'confidence={action.get("confidence", 0.0):.2f}\'\n            )\n\n    def timer_callback(self):\n        """Timer callback for processing VLA output"""\n        self.process_vla_output()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vla_node = VLAROSNode()\n\n    # Create timer for processing VLA output\n    timer = vla_node.create_timer(0.05, vla_node.timer_callback)  # 20Hz\n\n    try:\n        rclpy.spin(vla_node)\n    except KeyboardInterrupt:\n        vla_node.get_logger().info(\'Shutting down VLA System\')\n    finally:\n        vla_node.vla_system.stop()\n        vla_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-4-vla-system-optimization",children:"Step 4: VLA System Optimization"}),"\n",(0,i.jsx)(n.p,{children:"Create optimization techniques for the VLA system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# vla_optimization.py\n\nimport torch\nimport torch_tensorrt\nimport tensorrt as trt\nimport numpy as np\nfrom typing import Dict, Any\n\nclass VLAModelOptimizer:\n    """Optimization techniques for VLA models"""\n\n    def __init__(self, model: torch.nn.Module, device: str = \'cuda\'):\n        self.model = model\n        self.device = device\n        self.optimized_model = None\n\n    def optimize_with_tensorrt(self,\n                             example_image: torch.Tensor,\n                             example_text: torch.Tensor,\n                             precision: str = \'fp16\') -> torch.nn.Module:\n        """Optimize VLA model using TensorRT"""\n        self.model.eval()\n\n        try:\n            # Trace the model\n            traced_model = torch.jit.trace(\n                self.model,\n                (example_image, example_text)\n            )\n\n            # Compile with Torch-TensorRT\n            if precision == \'fp16\':\n                precision_set = {torch.half, torch.float}\n            else:\n                precision_set = {torch.float}\n\n            optimized_model = torch_tensorrt.compile(\n                traced_model,\n                inputs=[\n                    torch_tensorrt.Input(\n                        min_shape=[1, 3, 224, 224],\n                        opt_shape=[8, 3, 224, 224],\n                        max_shape=[16, 3, 224, 224]\n                    ),\n                    torch_tensorrt.Input(\n                        min_shape=[1, 20],\n                        opt_shape=[8, 20],\n                        max_shape=[16, 20],\n                        dtype=torch.long\n                    )\n                ],\n                enabled_precisions=precision_set,\n                workspace_size=1 << 30,  # 1GB\n                max_batch_size=16\n            )\n\n            self.optimized_model = optimized_model\n            print(f"Model optimized with TensorRT using {precision} precision")\n            return optimized_model\n\n        except Exception as e:\n            print(f"TensorRT optimization failed: {e}")\n            return self.model\n\n    def quantize_model(self) -> torch.nn.Module:\n        """Apply quantization to reduce model size and improve speed"""\n        self.model.eval()\n\n        # Use PyTorch\'s quantization\n        quantized_model = torch.quantization.quantize_dynamic(\n            self.model,\n            {torch.nn.Linear, torch.nn.LSTM},\n            dtype=torch.qint8\n        )\n\n        print("Model quantized to INT8")\n        return quantized_model\n\n    def prune_model(self, sparsity: float = 0.2) -> torch.nn.Module:\n        """Apply pruning to reduce model size"""\n        import torch.nn.utils.prune as prune\n\n        model = self.model\n        model.eval()\n\n        # Prune linear layers\n        for name, module in model.named_modules():\n            if isinstance(module, torch.nn.Linear):\n                prune.l1_unstructured(module, name=\'weight\', amount=sparsity)\n                prune.remove(module, \'weight\')\n\n        print(f"Model pruned to {sparsity*100}% sparsity")\n        return model\n\n    def benchmark_model(self,\n                       model: torch.nn.Module,\n                       test_data: list,\n                       num_runs: int = 100) -> Dict[str, float]:\n        """Benchmark model performance"""\n        model.eval()\n\n        times = []\n        for i in range(num_runs):\n            images, text_tokens = test_data[i % len(test_data)]\n\n            start_time = time.time()\n            with torch.no_grad():\n                _ = model(images, text_tokens)\n            end_time = time.time()\n\n            times.append((end_time - start_time) * 1000)  # Convert to ms\n\n        avg_time = sum(times) / len(times)\n        fps = 1000.0 / avg_time if avg_time > 0 else 0\n\n        return {\n            \'avg_time_ms\': avg_time,\n            \'std_time_ms\': np.std(times),\n            \'fps\': fps,\n            \'num_runs\': num_runs\n        }\n\nclass MemoryManager:\n    """Memory management for VLA system"""\n\n    def __init__(self, max_memory_mb: int = 2048):\n        self.max_memory_mb = max_memory_mb\n        self.current_allocation = 0\n        self.tensor_cache = {}\n\n    def allocate_tensor(self, shape: tuple, dtype: torch.dtype = torch.float32):\n        """Efficiently allocate tensor with memory management"""\n        element_size = torch.tensor([], dtype=dtype).element_size()\n        size_bytes = np.prod(shape) * element_size\n        size_mb = size_bytes / (1024 * 1024)\n\n        if self.current_allocation + size_mb > self.max_memory_mb:\n            self._try_free_memory(size_mb)\n\n        tensor = torch.zeros(shape, dtype=dtype, device=\'cuda\')\n        self.current_allocation += size_mb\n\n        return tensor\n\n    def _try_free_memory(self, needed_mb: float):\n        """Try to free memory by clearing cache"""\n        torch.cuda.empty_cache()\n        # Additional memory management strategies can be added here\n\n    def get_memory_stats(self) -> Dict[str, float]:\n        """Get memory usage statistics"""\n        allocated = torch.cuda.memory_allocated() / (1024 * 1024)  # MB\n        reserved = torch.cuda.memory_reserved() / (1024 * 1024)    # MB\n\n        return {\n            \'allocated_mb\': allocated,\n            \'reserved_mb\': reserved,\n            \'max_allowed_mb\': self.max_memory_mb,\n            \'utilization_percent\': (allocated / self.max_memory_mb) * 100\n        }\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-5-complete-vla-system-launch",children:"Step 5: Complete VLA System Launch"}),"\n",(0,i.jsx)(n.p,{children:"Create a comprehensive launch system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# complete_vla_launcher.py\n\nimport subprocess\nimport threading\nimport time\nimport signal\nimport os\nfrom typing import List, Dict\n\nclass CompleteVLASystem:\n    \"\"\"Complete VLA system launcher and manager\"\"\"\n\n    def __init__(self):\n        self.processes = []\n        self.is_running = False\n\n    def launch_vla_system(self):\n        \"\"\"Launch the complete VLA system\"\"\"\n        print(\"Launching Complete VLA System...\")\n\n        # Start individual components in separate processes\n        components = [\n            {\n                'name': 'camera_driver',\n                'command': ['ros2', 'launch', 'realsense2_camera', 'rs_launch.py'],\n                'optional': True\n            },\n            {\n                'name': 'vla_node',\n                'command': ['ros2', 'run', 'vla_system', 'vla_ros_node.py'],\n                'optional': False\n            },\n            {\n                'name': 'navigation',\n                'command': ['ros2', 'launch', 'nav2_bringup', 'navigation_launch.py'],\n                'optional': True\n            }\n        ]\n\n        for component in components:\n            try:\n                if component['name'] == 'vla_node':\n                    # Start VLA node with our implementation\n                    process = subprocess.Popen([\n                        'python3', '-c',\n                        '''\nimport sys\nsys.path.append('.')\nfrom vla_ros_node import main\nmain()\n                        '''\n                    ])\n                    self.processes.append({\n                        'name': component['name'],\n                        'process': process,\n                        'optional': component['optional']\n                    })\n                else:\n                    # For other components, try to launch\n                    try:\n                        process = subprocess.Popen(component['command'])\n                        self.processes.append({\n                            'name': component['name'],\n                            'process': process,\n                            'optional': component['optional']\n                        })\n                        print(f\"Started {component['name']}\")\n                    except FileNotFoundError:\n                        if not component['optional']:\n                            print(f\"Warning: Could not start {component['name']}\")\n                        continue\n\n            except Exception as e:\n                if not component['optional']:\n                    print(f\"Error starting {component['name']}: {e}\")\n                continue\n\n        self.is_running = True\n        print(\"VLA System launched successfully\")\n\n    def monitor_system(self):\n        \"\"\"Monitor the health of the VLA system\"\"\"\n        while self.is_running:\n            for proc_info in self.processes:\n                process = proc_info['process']\n                if process.poll() is not None:\n                    if not proc_info['optional']:\n                        print(f\"Critical process {proc_info['name']} died with code {process.returncode}\")\n                        self.shutdown()\n                        return\n                    else:\n                        print(f\"Optional process {proc_info['name']} died, restarting...\")\n                        # Restart optional process\n                        self.restart_process(proc_info)\n\n            time.sleep(1)  # Check every second\n\n    def restart_process(self, proc_info: Dict):\n        \"\"\"Restart a process\"\"\"\n        try:\n            # Try to restart the process\n            if proc_info['name'] == 'vla_node':\n                process = subprocess.Popen([\n                    'python3', '-c',\n                    '''\nimport sys\nsys.path.append('.')\nfrom vla_ros_node import main\nmain()\n                    '''\n                ])\n            else:\n                process = subprocess.Popen(proc_info['command'])\n\n            proc_info['process'] = process\n            print(f\"Restarted {proc_info['name']}\")\n        except Exception as e:\n            print(f\"Failed to restart {proc_info['name']}: {e}\")\n\n    def shutdown(self):\n        \"\"\"Shutdown the VLA system\"\"\"\n        print(\"Shutting down VLA System...\")\n        self.is_running = False\n\n        for proc_info in self.processes:\n            try:\n                proc_info['process'].terminate()\n                proc_info['process'].wait(timeout=5)\n            except subprocess.TimeoutExpired:\n                proc_info['process'].kill()\n            except Exception as e:\n                print(f\"Error terminating {proc_info['name']}: {e}\")\n\n        self.processes.clear()\n        print(\"VLA System shutdown complete\")\n\n    def start_monitoring_thread(self):\n        \"\"\"Start monitoring in a separate thread\"\"\"\n        monitor_thread = threading.Thread(target=self.monitor_system)\n        monitor_thread.daemon = True\n        monitor_thread.start()\n        return monitor_thread\n\ndef main():\n    \"\"\"Main function to run the complete VLA system\"\"\"\n    vla_system = CompleteVLASystem()\n\n    try:\n        # Launch the system\n        vla_system.launch_vla_system()\n\n        # Start monitoring\n        monitor_thread = vla_system.start_monitoring_thread()\n\n        # Keep running until interrupted\n        print(\"VLA System is running. Press Ctrl+C to stop.\")\n        while vla_system.is_running:\n            time.sleep(0.1)\n\n    except KeyboardInterrupt:\n        print(\"\\nReceived interrupt signal...\")\n    finally:\n        vla_system.shutdown()\n\nif __name__ == \"__main__\":\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,i.jsx)(n.h3,{id:"comprehensive-testing-script",children:"Comprehensive Testing Script"}),"\n",(0,i.jsx)(n.p,{children:"Create a comprehensive test script for the VLA system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# test_vla_system.py\n\nimport torch\nimport numpy as np\nimport time\nfrom vla_system import VLAModel, VLAPipeline, VLAInput, VLAPrediction\nfrom real_time_vla import RealTimeVLASystem\n\ndef test_vla_model():\n    """Test the VLA model with synthetic data"""\n    print("Testing VLA Model...")\n\n    # Create model\n    model = VLAModel(vocab_size=10000, action_space_dim=6)\n    model.eval()\n\n    # Create synthetic input\n    batch_size = 1\n    images = torch.randn(batch_size, 3, 224, 224)  # RGB image\n    text_tokens = torch.randint(0, 10000, (batch_size, 20))  # 20 tokens\n\n    # Test forward pass\n    with torch.no_grad():\n        start_time = time.time()\n        prediction = model(images, text_tokens)\n        inference_time = time.time() - start_time\n\n    print(f"  Inference time: {inference_time*1000:.2f}ms")\n    print(f"  Action shape: {prediction.action.shape}")\n    print(f"  Confidence: {prediction.confidence:.3f}")\n    print("  \u2713 Model test passed")\n\ndef test_vla_pipeline():\n    """Test the complete VLA pipeline"""\n    print("\\nTesting VLA Pipeline...")\n\n    # Create model and pipeline\n    model = VLAModel(vocab_size=10000, action_space_dim=6)\n    pipeline = VLAPipeline(model)\n\n    # Create test input\n    test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n    test_command = "go to the kitchen"\n\n    input_data = VLAInput(\n        visual_input=test_image,\n        language_input=test_command,\n        robot_state={\'position\': [0, 0, 0]},\n        timestamp=time.time()\n    )\n\n    # Test pipeline\n    start_time = time.time()\n    prediction = pipeline.process(input_data)\n    processing_time = time.time() - start_time\n\n    robot_command = pipeline.postprocess_action(prediction, input_data.robot_state)\n\n    print(f"  Processing time: {processing_time*1000:.2f}ms")\n    print(f"  Action: {robot_command}")\n    print(f"  Confidence: {prediction.confidence:.3f}")\n    print("  \u2713 Pipeline test passed")\n\ndef test_real_time_system():\n    """Test the real-time VLA system"""\n    print("\\nTesting Real-time System...")\n\n    # Create pipeline\n    model = VLAModel(vocab_size=10000, action_space_dim=6)\n    pipeline = VLAPipeline(model)\n\n    # Create real-time system\n    rt_system = RealTimeVLASystem(pipeline)\n\n    # Start system\n    rt_system.start()\n\n    # Add test data\n    test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n    test_command = "pick up the red cup"\n\n    rt_system.add_image(test_image)\n    rt_system.add_command(test_command)\n\n    # Wait for processing\n    time.sleep(0.1)\n\n    # Get action\n    action = rt_system.get_action()\n    if action:\n        print(f"  Generated action: {action}")\n    else:\n        print("  No action generated (this may be expected in test)")\n\n    # Get performance stats\n    stats = rt_system.get_performance_stats()\n    print(f"  Performance: {stats}")\n\n    # Stop system\n    rt_system.stop()\n    print("  \u2713 Real-time system test passed")\n\ndef benchmark_vla_system():\n    """Benchmark the VLA system performance"""\n    print("\\nBenchmarking VLA System...")\n\n    model = VLAModel(vocab_size=10000, action_space_dim=6)\n    pipeline = VLAPipeline(model)\n\n    # Generate test data\n    test_inputs = []\n    for i in range(100):\n        image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)\n        command = f"command {i % 10}"  # Cycle through 10 different commands\n        input_data = VLAInput(\n            visual_input=image,\n            language_input=command,\n            robot_state={\'position\': [0, 0, 0]},\n            timestamp=time.time()\n        )\n        test_inputs.append(input_data)\n\n    # Benchmark\n    start_time = time.time()\n    for input_data in test_inputs:\n        pipeline.process(input_data)\n    total_time = time.time() - start_time\n\n    avg_time = total_time / len(test_inputs)\n    fps = 1.0 / avg_time if avg_time > 0 else 0\n\n    print(f"  Processed {len(test_inputs)} inputs")\n    print(f"  Total time: {total_time:.3f}s")\n    print(f"  Average time: {avg_time*1000:.2f}ms")\n    print(f"  Average FPS: {fps:.2f}")\n    print("  \u2713 Benchmark test passed")\n\ndef run_comprehensive_tests():\n    """Run all VLA system tests"""\n    print("Running Comprehensive VLA System Tests\\n")\n\n    test_vla_model()\n    test_vla_pipeline()\n    test_real_time_system()\n    benchmark_vla_system()\n\n    print("\\nAll tests completed successfully!")\n    print("\\nVLA System is ready for deployment.")\n\nif __name__ == "__main__":\n    run_comprehensive_tests()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,i.jsx)(n.p,{children:"Create evaluation metrics for the complete VLA system:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# evaluate_vla_system.py\n\nimport time\nimport statistics\nfrom typing import List, Dict, Any\nimport numpy as np\n\nclass VLASystemEvaluator:\n    """Evaluator for complete VLA system performance"""\n\n    def __init__(self):\n        self.metrics = {\n            \'accuracy\': [],\n            \'latency\': [],\n            \'throughput\': [],\n            \'memory_usage\': [],\n            \'success_rate\': []\n        }\n\n    def evaluate_end_to_end(self,\n                           vla_system: RealTimeVLASystem,\n                           test_scenarios: List[Dict[str, Any]],\n                           num_runs: int = 10) -> Dict[str, float]:\n        """Evaluate end-to-end VLA system performance"""\n\n        latencies = []\n        success_count = 0\n\n        for scenario in test_scenarios:\n            for _ in range(num_runs):\n                start_time = time.time()\n\n                # Add inputs to system\n                vla_system.add_image(scenario[\'image\'])\n                vla_system.add_command(scenario[\'command\'])\n\n                # Wait for response\n                timeout = 5.0  # 5 second timeout\n                start_wait = time.time()\n                while time.time() - start_wait < timeout:\n                    action = vla_system.get_action()\n                    if action is not None:\n                        end_time = time.time()\n                        latencies.append(end_time - start_time)\n\n                        # Check if action is reasonable (simplified success check)\n                        if self._is_reasonable_action(action):\n                            success_count += 1\n                        break\n                    time.sleep(0.01)  # 10ms polling\n\n        # Calculate metrics\n        total_requests = len(test_scenarios) * num_runs\n        success_rate = success_count / total_requests if total_requests > 0 else 0\n\n        avg_latency = statistics.mean(latencies) if latencies else 0\n        std_latency = statistics.stdev(latencies) if len(latencies) > 1 else 0\n\n        return {\n            \'avg_latency_ms\': avg_latency * 1000,\n            \'std_latency_ms\': std_latency * 1000,\n            \'success_rate\': success_rate,\n            \'total_requests\': total_requests,\n            \'successful_requests\': success_count,\n            \'requests_per_second\': len(latencies) / sum(latencies) if latencies else 0\n        }\n\n    def _is_reasonable_action(self, action: Dict[str, Any]) -> bool:\n        """Check if generated action is reasonable"""\n        # Check if action values are within expected ranges\n        linear_vel = action.get(\'linear_velocity\', 0)\n        angular_vel = action.get(\'angular_velocity\', 0)\n\n        # Reasonable ranges for differential drive robot\n        return (-2.0 <= linear_vel <= 2.0) and (-2.0 <= angular_vel <= 2.0)\n\n    def evaluate_perception_accuracy(self,\n                                   model: VLAModel,\n                                   test_dataset: List[Dict[str, Any]]) -> Dict[str, float]:\n        """Evaluate perception component accuracy"""\n        correct_predictions = 0\n        total_predictions = 0\n\n        for sample in test_dataset:\n            images = sample[\'images\']\n            expected_actions = sample[\'expected_actions\']\n\n            with torch.no_grad():\n                prediction = model(images, sample[\'text_tokens\'])\n\n            # Compare predicted action with expected action (simplified)\n            predicted_action = prediction.action.cpu().numpy()\n            expected_action = expected_actions.cpu().numpy()\n\n            # Calculate similarity (simplified as mean absolute difference)\n            similarity = 1.0 / (1.0 + np.mean(np.abs(predicted_action - expected_action)))\n\n            if similarity > 0.5:  # Threshold for "correct"\n                correct_predictions += 1\n            total_predictions += 1\n\n        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n\n        return {\n            \'perception_accuracy\': accuracy,\n            \'correct_predictions\': correct_predictions,\n            \'total_predictions\': total_predictions\n        }\n\n    def generate_evaluation_report(self,\n                                 e2e_metrics: Dict[str, float],\n                                 perception_metrics: Dict[str, float]) -> str:\n        """Generate comprehensive evaluation report"""\n        report = """\n# VLA System Evaluation Report\n\n## End-to-End Performance\n- Average Latency: {:.2f}ms (\xb1{:.2f}ms)\n- Success Rate: {:.2f}%\n- Throughput: {:.2f} requests/sec\n- Total Requests: {}\n\n## Perception Accuracy\n- Accuracy: {:.2f}%\n- Correct Predictions: {}/{}\n\n## Summary\nThe VLA system demonstrates {} performance with {} latency and {} success rate.\n        """.format(\n            e2e_metrics[\'avg_latency_ms\'],\n            e2e_metrics[\'std_latency_ms\'],\n            e2e_metrics[\'success_rate\'] * 100,\n            e2e_metrics[\'requests_per_second\'],\n            e2e_metrics[\'total_requests\'],\n            perception_metrics[\'perception_accuracy\'] * 100,\n            perception_metrics[\'correct_predictions\'],\n            perception_metrics[\'total_predictions\'],\n            "good" if e2e_metrics[\'success_rate\'] > 0.8 else "acceptable" if e2e_metrics[\'success_rate\'] > 0.6 else "poor",\n            "low" if e2e_metrics[\'avg_latency_ms\'] < 100 else "acceptable" if e2e_metrics[\'avg_latency_ms\'] < 500 else "high",\n            "high" if e2e_metrics[\'success_rate\'] > 0.9 else "medium" if e2e_metrics[\'success_rate\'] > 0.7 else "low"\n        )\n\n        return report\n\ndef run_complete_evaluation():\n    """Run complete evaluation of the VLA system"""\n    print("Running Complete VLA System Evaluation...")\n\n    evaluator = VLASystemEvaluator()\n\n    # This would typically involve loading test data\n    # For this example, we\'ll demonstrate the evaluation structure\n\n    print("Evaluation framework ready.")\n    print("To run full evaluation, implement data loading and call:")\n    print("  evaluator.evaluate_end_to_end(vla_system, test_scenarios)")\n    print("  evaluator.evaluate_perception_accuracy(model, test_dataset)")\n\n    print("\\nKey evaluation metrics:")\n    print("- Response latency (should be < 100ms for real-time)")\n    print("- Success rate (should be > 80%)")\n    print("- Memory efficiency (should fit in robot\'s memory)")\n    print("- Robustness to noise and variations")\n\nif __name__ == "__main__":\n    run_complete_evaluation()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"lab-deliverables",children:"Lab Deliverables"}),"\n",(0,i.jsx)(n.p,{children:"Complete the following tasks to finish the lab:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement the complete VLA model"})," with vision, language, and action components"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Create the real-time processing system"})," with buffering and scheduling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrate with ROS 2"})," for robotic applications"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Optimize the system"})," for edge deployment on Jetson platforms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Test and validate"})," the complete system with various scenarios"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deploy and demonstrate"})," the system with actual robot hardware"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Document your results"})," including:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"System architecture and design decisions"}),"\n",(0,i.jsx)(n.li,{children:"Performance metrics achieved"}),"\n",(0,i.jsx)(n.li,{children:"Challenges encountered and solutions"}),"\n",(0,i.jsx)(n.li,{children:"Suggestions for improvement"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,i.jsx)(n.p,{children:"Your lab implementation will be assessed based on:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integration"}),": How well do all components work together?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance"}),": Are latency and accuracy within acceptable ranges?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness"}),": How well does the system handle variations and noise?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Efficiency"}),": Is the system optimized for real-time operation?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Documentation"}),": Quality of code documentation and results presentation."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"extensions-optional",children:"Extensions (Optional)"}),"\n",(0,i.jsx)(n.p,{children:"For advanced students, consider implementing:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-modal fusion"})," with additional sensors (LiDAR, IMU)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reinforcement learning"})," for action optimization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Online adaptation"})," to new environments and tasks"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Human feedback integration"})," for system improvement"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety mechanisms"})," for fail-safe operation"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,i.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Memory errors on Jetson:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Reduce model size or batch processing"}),"\n",(0,i.jsx)(n.li,{children:"Use model optimization (quantization, pruning)"}),"\n",(0,i.jsx)(n.li,{children:"Implement memory management strategies"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"High latency issues:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Optimize with TensorRT"}),"\n",(0,i.jsx)(n.li,{children:"Reduce input resolution"}),"\n",(0,i.jsx)(n.li,{children:"Use lighter model variants"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Poor action quality:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Improve training data quality"}),"\n",(0,i.jsx)(n.li,{children:"Add more diverse training scenarios"}),"\n",(0,i.jsx)(n.li,{children:"Implement action refinement techniques"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Integration problems:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Verify ROS 2 message formats"}),"\n",(0,i.jsx)(n.li,{children:"Check timing and synchronization"}),"\n",(0,i.jsx)(n.li,{children:"Use appropriate middleware configurations"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"This capstone lab integrated all Vision-Language-Action concepts into a complete system. You learned to build, optimize, and deploy sophisticated AI systems that can understand natural language commands, perceive visual environments, and generate appropriate robotic actions. This represents the state-of-the-art in embodied AI and provides the foundation for advanced robotic applications."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const i={},o=s.createContext(i);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);