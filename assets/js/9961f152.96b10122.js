"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[5840],{5996(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"capstone/perception","title":"Perception System Integration","description":"Overview","source":"@site/docs/capstone/perception.md","sourceDirName":"capstone","slug":"/capstone/perception","permalink":"/hackathon/docs/capstone/perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Fatimaali424/hackathon/edit/main/website/docs/capstone/perception.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Navigation System Integration","permalink":"/hackathon/docs/capstone/navigation"},"next":{"title":"Manipulation System Integration","permalink":"/hackathon/docs/capstone/manipulation"}}');var s=t(4848),a=t(8453);const o={sidebar_position:5},r="Perception System Integration",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Perception System Architecture",id:"perception-system-architecture",level:2},{value:"Multi-Sensor Integration Framework",id:"multi-sensor-integration-framework",level:3},{value:"Sensor Integration",id:"sensor-integration",level:2},{value:"Camera System Integration",id:"camera-system-integration",level:3},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:2},{value:"Deep Learning-Based Object Detection",id:"deep-learning-based-object-detection",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:2},{value:"Scene Understanding with Semantic Segmentation",id:"scene-understanding-with-semantic-segmentation",level:3},{value:"3D Perception and Reconstruction",id:"3d-perception-and-reconstruction",level:2},{value:"Depth Processing and 3D Reconstruction",id:"depth-processing-and-3d-reconstruction",level:3},{value:"Multi-Modal Perception Fusion",id:"multi-modal-perception-fusion",level:2},{value:"Sensor Fusion Framework",id:"sensor-fusion-framework",level:3},{value:"Integration with Downstream Systems",id:"integration-with-downstream-systems",level:2},{value:"Perception Interface for Navigation and Manipulation",id:"perception-interface-for-navigation-and-manipulation",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Efficient Perception Pipeline",id:"efficient-perception-pipeline",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"perception-system-integration",children:"Perception System Integration"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"The perception system is the sensory foundation of the autonomous humanoid robot, enabling it to understand and interact with its environment. This system integrates multiple sensor modalities including vision, depth sensing, and other environmental sensors to create a comprehensive understanding of the world around the robot. The perception system must provide real-time processing capabilities while maintaining accuracy and reliability for safe robot operation."}),"\n",(0,s.jsx)(n.p,{children:"The perception system integrates closely with other modules including navigation (for obstacle detection and mapping), manipulation (for object recognition and grasping), and voice command processing (for visual grounding of language commands)."}),"\n",(0,s.jsx)(n.h2,{id:"perception-system-architecture",children:"Perception System Architecture"}),"\n",(0,s.jsx)(n.h3,{id:"multi-sensor-integration-framework",children:"Multi-Sensor Integration Framework"}),"\n",(0,s.jsx)(n.p,{children:"The perception system follows a modular architecture that allows for flexible integration of different sensor types:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    SENSOR ACQUISITION                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 RGB Camera      \u2502  \u2502 Depth Camera    \u2502  \u2502 LiDAR           \u2502 \u2502\n\u2502  \u2502 (Intel Realsense\u2502  \u2502 (Intel Realsense\u2502  \u2502 (Hokuyo/VELDDYNE\u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  SENSOR PREPROCESSING                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Color Correction\u2502  \u2502 Depth Filtering \u2502  \u2502 Point Cloud     \u2502 \u2502\n\u2502  \u2502 & Calibration   \u2502  \u2502 & Registration  \u2502  \u2502 Processing      \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   PERCEPTION MODULES                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Object Detection\u2502  \u2502 Semantic        \u2502  \u2502 Instance        \u2502 \u2502\n\u2502  \u2502 & Recognition   \u2502  \u2502 Segmentation    \u2502  \u2502 Segmentation    \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Pose Estimation \u2502  \u2502 Scene           \u2502  \u2502 Activity        \u2502 \u2502\n\u2502  \u2502 (6D Pose)       \u2502  \u2502 Understanding   \u2502  \u2502 Recognition     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                FUSION & REASONING                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Multi-View      \u2502  \u2502 Temporal        \u2502  \u2502 Spatial         \u2502 \u2502\n\u2502  \u2502 Fusion          \u2502  \u2502 Fusion          \u2502  \u2502 Reasoning       \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502               OUTPUT REPRESENTATION                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Object          \u2502  \u2502 Semantic        \u2502  \u2502 Navigation      \u2502 \u2502\n\u2502  \u2502 Representations \u2502  \u2502 Maps            \u2502  \u2502 Grids           \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              DOWNSTREAM APPLICATIONS                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Navigation      \u2502  \u2502 Manipulation    \u2502  \u2502 Human-Robot     \u2502 \u2502\n\u2502  \u2502 System          \u2502  \u2502 System          \u2502  \u2502 Interaction     \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(n.h2,{id:"sensor-integration",children:"Sensor Integration"}),"\n",(0,s.jsx)(n.h3,{id:"camera-system-integration",children:"Camera System Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\nimport threading\nimport queue\nimport time\n\n@dataclass\nclass CameraIntrinsics:\n    """Camera intrinsics parameters"""\n    fx: float  # Focal length x\n    fy: float  # Focal length y\n    cx: float  # Principal point x\n    cy: float  # Principal point y\n    width: int  # Image width\n    height: int  # Image height\n\n@dataclass\nclass CameraExtrinsics:\n    """Camera extrinsics parameters"""\n    rotation: np.ndarray  # 3x3 rotation matrix\n    translation: np.ndarray  # 3x1 translation vector\n\nclass CameraManager:\n    """Manages camera acquisition and calibration"""\n\n    def __init__(self, camera_id: int = 0, width: int = 640, height: int = 480):\n        self.camera_id = camera_id\n        self.width = width\n        self.height = height\n        self.cap = None\n        self.intrinsics = None\n        self.extrinsics = None\n        self.is_open = False\n        self.frame_queue = queue.Queue(maxsize=5)\n        self.capture_thread = None\n        self.stop_capture = threading.Event()\n\n    def open_camera(self):\n        """Open camera and start capture thread"""\n        self.cap = cv2.VideoCapture(self.camera_id)\n        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)\n        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)\n        self.cap.set(cv2.CAP_PROP_FPS, 30)\n\n        if not self.cap.isOpened():\n            raise RuntimeError(f"Cannot open camera {self.camera_id}")\n\n        self.is_open = True\n        self.capture_thread = threading.Thread(target=self._capture_worker)\n        self.capture_thread.daemon = True\n        self.capture_thread.start()\n\n    def close_camera(self):\n        """Close camera and stop capture thread"""\n        self.stop_capture.set()\n        if self.capture_thread:\n            self.capture_thread.join(timeout=2.0)\n        if self.cap:\n            self.cap.release()\n        self.is_open = False\n\n    def _capture_worker(self):\n        """Capture worker thread"""\n        while not self.stop_capture.is_set():\n            ret, frame = self.cap.read()\n            if ret:\n                try:\n                    self.frame_queue.put_nowait(frame)\n                except queue.Full:\n                    # Drop oldest frame if queue is full\n                    try:\n                        self.frame_queue.get_nowait()\n                        self.frame_queue.put_nowait(frame)\n                    except:\n                        pass\n            else:\n                time.sleep(0.01)  # Brief pause if no frame\n\n    def get_latest_frame(self) -> Optional[np.ndarray]:\n        """Get the latest captured frame"""\n        frame = None\n        try:\n            while not self.frame_queue.empty():\n                frame = self.frame_queue.get_nowait()\n        except queue.Empty:\n            pass\n        return frame\n\n    def set_intrinsics(self, intrinsics: CameraIntrinsics):\n        """Set camera intrinsics"""\n        self.intrinsics = intrinsics\n\n    def get_intrinsics(self) -> Optional[CameraIntrinsics]:\n        """Get camera intrinsics"""\n        return self.intrinsics\n\n    def undistort_image(self, image: np.ndarray) -> np.ndarray:\n        """Undistort image using calibration parameters"""\n        if self.intrinsics is None:\n            return image\n\n        # Create camera matrix\n        camera_matrix = np.array([\n            [self.intrinsics.fx, 0, self.intrinsics.cx],\n            [0, self.intrinsics.fy, self.intrinsics.cy],\n            [0, 0, 1]\n        ])\n\n        # For now, assume no distortion coefficients\n        dist_coeffs = np.zeros((4, 1))\n\n        # Undistort image\n        undistorted = cv2.undistort(image, camera_matrix, dist_coeffs)\n        return undistorted\n\n    def project_3d_to_2d(self, points_3d: np.ndarray) -> np.ndarray:\n        """Project 3D points to 2D image coordinates"""\n        if self.intrinsics is None:\n            raise ValueError("Camera intrinsics not set")\n\n        # Convert to homogeneous coordinates\n        points_homo = np.column_stack([points_3d, np.ones(points_3d.shape[0])])\n\n        # Create camera matrix\n        camera_matrix = np.array([\n            [self.intrinsics.fx, 0, self.intrinsics.cx],\n            [0, self.intrinsics.fy, self.intrinsics.cy],\n            [0, 0, 1]\n        ])\n\n        # Project to 2D\n        points_2d_homo = points_homo @ camera_matrix.T\n        points_2d = points_2d_homo[:, :2] / points_2d_homo[:, 2:3]\n\n        return points_2d\n\n    def project_2d_to_3d(self, points_2d: np.ndarray, depth: np.ndarray) -> np.ndarray:\n        """Project 2D image coordinates to 3D world coordinates"""\n        if self.intrinsics is None:\n            raise ValueError("Camera intrinsics not set")\n\n        # Convert 2D points to 3D using depth\n        u, v = points_2d[:, 0], points_2d[:, 1]\n        z = depth  # Depth values corresponding to 2D points\n\n        # Calculate 3D coordinates\n        x = (u - self.intrinsics.cx) * z / self.intrinsics.fx\n        y = (v - self.intrinsics.cy) * z / self.intrinsics.fy\n\n        points_3d = np.stack([x, y, z], axis=1)\n        return points_3d\n'})}),"\n",(0,s.jsx)(n.h2,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,s.jsx)(n.h3,{id:"deep-learning-based-object-detection",children:"Deep Learning-Based Object Detection"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torchvision.transforms as transforms\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nclass ObjectDetectionModule:\n    \"\"\"Object detection using deep learning models\"\"\"\n\n    def __init__(self, model_name: str = \"fasterrcnn_resnet50_fpn\", device: str = \"cuda\"):\n        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n        self.model_name = model_name\n        self.model = self._load_model(model_name)\n        self.transforms = self._get_transforms()\n        self.class_names = self._get_class_names()  # COCO class names\n\n    def _load_model(self, model_name: str):\n        \"\"\"Load pre-trained object detection model\"\"\"\n        if model_name == \"fasterrcnn_resnet50_fpn\":\n            model = fasterrcnn_resnet50_fpn(pretrained=True)\n            # Replace the classifier with a new one for custom classes if needed\n            in_features = model.roi_heads.box_predictor.cls_score.in_features\n            model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 91)  # COCO classes\n        else:\n            raise ValueError(f\"Unsupported model: {model_name}\")\n\n        model.to(self.device)\n        model.eval()\n        return model\n\n    def _get_transforms(self):\n        \"\"\"Get image preprocessing transforms\"\"\"\n        return transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.ToTensor(),\n        ])\n\n    def _get_class_names(self) -> List[str]:\n        \"\"\"Get COCO class names\"\"\"\n        # COCO dataset class names\n        return [\n            '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n            'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n            'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n            'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n            'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n            'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\n            'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n            'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n            'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n    def detect_objects(self, image: np.ndarray, confidence_threshold: float = 0.5) -> Dict:\n        \"\"\"Detect objects in image\"\"\"\n        # Preprocess image\n        input_tensor = self.transforms(image).unsqueeze(0).to(self.device)\n\n        # Run inference\n        with torch.no_grad():\n            predictions = self.model(input_tensor)\n\n        # Process predictions\n        boxes = predictions[0]['boxes'].cpu().numpy()\n        labels = predictions[0]['labels'].cpu().numpy()\n        scores = predictions[0]['scores'].cpu().numpy()\n\n        # Filter by confidence threshold\n        valid_indices = scores >= confidence_threshold\n        filtered_boxes = boxes[valid_indices]\n        filtered_labels = labels[valid_indices]\n        filtered_scores = scores[valid_indices]\n\n        # Create result dictionary\n        detection_results = {\n            'boxes': filtered_boxes,\n            'labels': [self.class_names[label] for label in filtered_labels],\n            'scores': filtered_scores,\n            'num_detections': len(filtered_boxes)\n        }\n\n        return detection_results\n\n    def draw_detections(self, image: np.ndarray, detections: Dict) -> np.ndarray:\n        \"\"\"Draw detection results on image\"\"\"\n        output_image = image.copy()\n\n        for i in range(len(detections['boxes'])):\n            box = detections['boxes'][i]\n            label = detections['labels'][i]\n            score = detections['scores'][i]\n\n            # Draw bounding box\n            x1, y1, x2, y2 = map(int, box)\n            cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            # Draw label and confidence\n            text = f\"{label}: {score:.2f}\"\n            cv2.putText(output_image, text, (x1, y1-10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n        return output_image\n\nclass CustomObjectDetector(ObjectDetectionModule):\n    \"\"\"Custom object detector for specific robot applications\"\"\"\n\n    def __init__(self, custom_classes: List[str] = None, device: str = \"cuda\"):\n        super().__init__(device=device)\n\n        # Define custom classes if provided\n        self.custom_classes = custom_classes or [\n            'background', 'cup', 'bottle', 'book', 'phone', 'keys',\n            'chair', 'table', 'kitchen_counter', 'refrigerator', 'microwave',\n            'bed', 'sofa', 'lamp', 'computer'\n        ]\n\n        # Load custom model if available\n        self.custom_model = self._load_custom_model()\n\n    def _load_custom_model(self):\n        \"\"\"Load custom trained model for robot-specific objects\"\"\"\n        # In practice, this would load a model trained on robot-specific data\n        # For now, we'll use the base model but with custom class handling\n        return self.model\n\n    def detect_robot_objects(self, image: np.ndarray, confidence_threshold: float = 0.6) -> Dict:\n        \"\"\"Detect objects relevant to robot tasks\"\"\"\n        # Use base detection\n        detections = self.detect_objects(image, confidence_threshold)\n\n        # Filter for robot-relevant objects\n        robot_relevant_objects = [\n            'person', 'cup', 'bottle', 'book', 'phone', 'keys', 'chair', 'table',\n            'kitchen_counter', 'refrigerator', 'microwave', 'bed', 'sofa', 'lamp', 'computer'\n        ]\n\n        # Filter detections\n        relevant_indices = [\n            i for i, label in enumerate(detections['labels'])\n            if label.lower() in [obj.lower() for obj in robot_relevant_objects]\n        ]\n\n        relevant_detections = {\n            'boxes': detections['boxes'][relevant_indices],\n            'labels': [detections['labels'][i] for i in relevant_indices],\n            'scores': detections['scores'][relevant_indices],\n            'num_detections': len(relevant_indices)\n        }\n\n        return relevant_detections\n"})}),"\n",(0,s.jsx)(n.h2,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,s.jsx)(n.h3,{id:"scene-understanding-with-semantic-segmentation",children:"Scene Understanding with Semantic Segmentation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch.nn.functional as F\nfrom torchvision.models.segmentation import deeplabv3_resnet50\n\nclass SemanticSegmentationModule:\n    """Semantic segmentation for scene understanding"""\n\n    def __init__(self, device: str = "cuda"):\n        self.device = torch.device(device if torch.cuda.is_available() else "cpu")\n        self.model = self._load_model()\n        self.transforms = self._get_transforms()\n        self.color_palette = self._create_color_palette()\n\n    def _load_model(self):\n        """Load pre-trained semantic segmentation model"""\n        model = deeplabv3_resnet50(pretrained=True)\n        model.to(self.device)\n        model.eval()\n        return model\n\n    def _get_transforms(self):\n        """Get image preprocessing transforms for segmentation"""\n        return transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((520, 520)),  # DeepLab expects this size\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n    def _create_color_palette(self) -> np.ndarray:\n        """Create color palette for segmentation visualization"""\n        # COCO colors (21 classes)\n        colors = [\n            [0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0, 0, 128],\n            [128, 0, 128], [0, 128, 128], [128, 128, 128], [64, 0, 0], [192, 0, 0],\n            [64, 128, 0], [192, 128, 0], [64, 0, 128], [192, 0, 128], [64, 128, 128],\n            [192, 128, 128], [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n            [0, 64, 128]\n        ]\n        return np.array(colors, dtype=np.uint8)\n\n    def segment_image(self, image: np.ndarray) -> Dict:\n        """Perform semantic segmentation on image"""\n        # Get original image dimensions\n        orig_h, orig_w = image.shape[:2]\n\n        # Preprocess image\n        input_tensor = self.transforms(image).unsqueeze(0).to(self.device)\n\n        # Run inference\n        with torch.no_grad():\n            output = self.model(input_tensor)[\'out\']\n            output = F.interpolate(output, size=(orig_h, orig_w), mode=\'bilinear\', align_corners=False)\n            predicted = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n\n        # Create segmentation mask\n        segmentation_mask = predicted.astype(np.uint8)\n\n        # Get unique classes in the image\n        unique_classes = np.unique(segmentation_mask)\n        class_counts = [(cls, np.sum(segmentation_mask == cls)) for cls in unique_classes]\n\n        return {\n            \'segmentation_mask\': segmentation_mask,\n            \'unique_classes\': unique_classes,\n            \'class_counts\': class_counts,\n            \'color_map\': self._apply_color_map(segmentation_mask)\n        }\n\n    def _apply_color_map(self, segmentation_mask: np.ndarray) -> np.ndarray:\n        """Apply color map to segmentation mask"""\n        # Ensure we have enough colors\n        color_map = np.zeros((segmentation_mask.shape[0], segmentation_mask.shape[1], 3), dtype=np.uint8)\n\n        for class_idx in np.unique(segmentation_mask):\n            mask = segmentation_mask == class_idx\n            if class_idx < len(self.color_palette):\n                color_map[mask] = self.color_palette[class_idx]\n            else:\n                # Use a default color for unknown classes\n                color_map[mask] = [255, 255, 255]\n\n        return color_map\n\n    def get_object_boundaries(self, segmentation_mask: np.ndarray) -> Dict[int, np.ndarray]:\n        """Extract object boundaries from segmentation mask"""\n        boundaries = {}\n\n        for class_id in np.unique(segmentation_mask):\n            if class_id == 0:  # Skip background\n                continue\n\n            # Create binary mask for this class\n            class_mask = (segmentation_mask == class_id).astype(np.uint8)\n\n            # Find contours\n            contours, _ = cv2.findContours(class_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            boundaries[class_id] = contours\n\n        return boundaries\n\n    def overlay_segmentation(self, image: np.ndarray, segmentation_result: Dict) -> np.ndarray:\n        """Overlay segmentation results on original image"""\n        color_map = segmentation_result[\'color_map\']\n        alpha = 0.5  # Transparency\n\n        overlay = cv2.addWeighted(image, 1-alpha, color_map, alpha, 0)\n        return overlay\n\nclass CustomSemanticSegmenter(SemanticSegmentationModule):\n    """Custom semantic segmentation for robot-specific scenes"""\n\n    def __init__(self, device: str = "cuda"):\n        super().__init__(device=device)\n\n        # Robot-specific class names\n        self.robot_classes = [\n            \'background\', \'floor\', \'wall\', \'ceiling\', \'furniture\', \'kitchen_appliance\',\n            \'electronics\', \'person\', \'plant\', \'obstacle\', \'navigation_area\', \'no_go_zone\'\n        ]\n\n    def segment_robot_environment(self, image: np.ndarray) -> Dict:\n        """Segment robot environment with robot-specific classes"""\n        # Perform base segmentation\n        base_result = self.segment_image(image)\n\n        # Filter for robot-relevant classes\n        robot_relevant_classes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]  # Example mappings\n        robot_mask = np.isin(base_result[\'segmentation_mask\'], robot_relevant_classes)\n\n        # Create robot-specific segmentation result\n        robot_result = {\n            \'segmentation_mask\': base_result[\'segmentation_mask\'] * robot_mask,\n            \'unique_classes\': np.unique(base_result[\'segmentation_mask\'][robot_mask]),\n            \'class_counts\': [(cls, np.sum(base_result[\'segmentation_mask\'] == cls))\n                           for cls in np.unique(base_result[\'segmentation_mask\'][robot_mask])],\n            \'color_map\': self._apply_color_map(base_result[\'segmentation_mask\'] * robot_mask),\n            \'is_traversable\': self._analyze_traversability(base_result[\'segmentation_mask\'])\n        }\n\n        return robot_result\n\n    def _analyze_traversability(self, segmentation_mask: np.ndarray) -> np.ndarray:\n        """Analyze traversability based on segmentation"""\n        # Define traversable classes (example: floor, carpet, etc.)\n        traversable_classes = [1, 2, 3]  # Example class IDs\n        traversable_mask = np.isin(segmentation_mask, traversable_classes)\n\n        return traversable_mask.astype(bool)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"3d-perception-and-reconstruction",children:"3D Perception and Reconstruction"}),"\n",(0,s.jsx)(n.h3,{id:"depth-processing-and-3d-reconstruction",children:"Depth Processing and 3D Reconstruction"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class DepthProcessor:\n    """Process depth information for 3D scene understanding"""\n\n    def __init__(self, camera_intrinsics: CameraIntrinsics):\n        self.intrinsics = camera_intrinsics\n        self.depth_scale = 1.0  # Scale factor for depth values\n\n    def create_point_cloud(self, rgb_image: np.ndarray, depth_image: np.ndarray) -> np.ndarray:\n        """Create colored point cloud from RGB-D data"""\n        h, w = depth_image.shape\n\n        # Create coordinate grids\n        x_coords, y_coords = np.meshgrid(np.arange(w), np.arange(h))\n\n        # Convert pixel coordinates to 3D points\n        x_3d = (x_coords - self.intrinsics.cx) * depth_image / self.intrinsics.fx\n        y_3d = (y_coords - self.intrinsics.cy) * depth_image / self.intrinsics.fy\n        z_3d = depth_image\n\n        # Stack coordinates\n        points_3d = np.stack([x_3d, y_3d, z_3d], axis=-1).reshape(-1, 3)\n\n        # Get corresponding colors\n        colors = rgb_image.reshape(-1, 3)\n\n        # Filter out invalid points (zero depth)\n        valid_mask = z_3d.reshape(-1) > 0\n        valid_points = points_3d[valid_mask]\n        valid_colors = colors[valid_mask]\n\n        # Combine points and colors\n        point_cloud = np.concatenate([valid_points, valid_colors], axis=1)\n\n        return point_cloud\n\n    def filter_depth_outliers(self, depth_image: np.ndarray,\n                            min_depth: float = 0.1, max_depth: float = 10.0) -> np.ndarray:\n        """Filter depth outliers"""\n        filtered_depth = depth_image.copy()\n\n        # Set values outside valid range to zero\n        filtered_depth[(filtered_depth < min_depth) | (filtered_depth > max_depth)] = 0\n\n        # Apply median filter to remove salt-and-pepper noise\n        filtered_depth = cv2.medianBlur(filtered_depth.astype(np.float32), 5)\n\n        return filtered_depth\n\n    def compute_surface_normals(self, point_cloud: np.ndarray, k: int = 20) -> np.ndarray:\n        """Compute surface normals for point cloud"""\n        from sklearn.neighbors import NearestNeighbors\n\n        # Separate 3D coordinates from colors\n        coords = point_cloud[:, :3]\n\n        # Find k-nearest neighbors\n        nbrs = NearestNeighbors(n_neighbors=k, algorithm=\'auto\').fit(coords)\n        distances, indices = nbrs.kneighbors(coords)\n\n        # Compute normals using PCA\n        normals = np.zeros_like(coords)\n\n        for i in range(len(coords)):\n            # Get k nearest points\n            neighbor_points = coords[indices[i]]\n\n            # Center the points\n            centered_points = neighbor_points - coords[i]\n\n            # Compute covariance matrix\n            cov_matrix = centered_points.T @ centered_points / k\n\n            # Compute eigenvectors\n            eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n            # The normal is the eigenvector corresponding to smallest eigenvalue\n            normal = eigenvectors[:, 0]\n\n            # Orient consistently (point towards camera)\n            if normal[2] < 0:\n                normal = -normal\n\n            normals[i] = normal\n\n        return normals\n\n    def segment_planes(self, point_cloud: np.ndarray, distance_threshold: float = 0.01,\n                     max_iterations: int = 1000) -> Tuple[np.ndarray, np.ndarray]:\n        """Segment planes in point cloud using RANSAC"""\n        from sklearn.linear_model import RANSACRegressor\n        from sklearn.preprocessing import PolynomialFeatures\n\n        coords = point_cloud[:, :3]\n\n        all_plane_indices = []\n        remaining_indices = np.arange(len(coords))\n\n        # Segment multiple planes\n        for _ in range(5):  # Max 5 planes\n            if len(remaining_indices) < 100:  # Need at least 100 points\n                break\n\n            # Prepare data for plane fitting (z = ax + by + c)\n            X = coords[remaining_indices, :2]  # x, y coordinates\n            y = coords[remaining_indices, 2]  # z coordinates\n\n            # Fit plane using RANSAC\n            ransac = RANSACRegressor(\n                estimator=None,  # Use default (LinearRegression)\n                min_samples=3,\n                residual_threshold=distance_threshold,\n                max_trials=max_iterations\n            )\n\n            try:\n                ransac.fit(X, y)\n\n                # Get inlier indices\n                inlier_mask = ransac.inlier_mask_\n                inlier_indices = remaining_indices[inlier_mask]\n\n                if len(inlier_indices) > 100:  # Only keep substantial planes\n                    all_plane_indices.extend(inlier_indices)\n                    remaining_indices = remaining_indices[~inlier_mask]\n\n            except:\n                break  # Could not fit plane\n\n        # Create plane segmentation mask\n        plane_mask = np.zeros(len(coords), dtype=bool)\n        plane_mask[all_plane_indices] = True\n\n        return plane_mask, np.array(all_plane_indices)\n\nclass SceneReconstructor:\n    """3D scene reconstruction from multiple views"""\n\n    def __init__(self, camera_intrinsics: CameraIntrinsics):\n        self.intrinsics = camera_intrinsics\n        self.keyframes = []\n        self.global_point_cloud = np.empty((0, 6))  # [x, y, z, r, g, b]\n\n    def add_keyframe(self, rgb_image: np.ndarray, depth_image: np.ndarray,\n                    camera_pose: np.ndarray):\n        """Add a keyframe to the reconstruction"""\n        # Create point cloud from RGB-D data\n        point_cloud = DepthProcessor(self.intrinsics).create_point_cloud(\n            rgb_image, depth_image\n        )\n\n        # Transform to global coordinate system\n        if camera_pose is not None:\n            global_points = self._transform_to_global(point_cloud, camera_pose)\n        else:\n            global_points = point_cloud\n\n        # Add to global point cloud\n        self.global_point_cloud = np.vstack([self.global_point_cloud, global_points])\n\n        # Store keyframe\n        keyframe = {\n            \'rgb_image\': rgb_image,\n            \'depth_image\': depth_image,\n            \'camera_pose\': camera_pose,\n            \'point_cloud\': point_cloud\n        }\n        self.keyframes.append(keyframe)\n\n    def _transform_to_global(self, point_cloud: np.ndarray,\n                           camera_pose: np.ndarray) -> np.ndarray:\n        """Transform point cloud to global coordinate system"""\n        # Separate coordinates and colors\n        coords = point_cloud[:, :3]\n        colors = point_cloud[:, 3:]\n\n        # Apply transformation (rotation and translation)\n        rotation = camera_pose[:3, :3]\n        translation = camera_pose[:3, 3]\n\n        # Transform coordinates\n        global_coords = coords @ rotation.T + translation\n\n        # Combine with colors\n        global_point_cloud = np.concatenate([global_coords, colors], axis=1)\n\n        return global_point_cloud\n\n    def integrate_voxel_grid(self, voxel_size: float = 0.01):\n        """Integrate point cloud into voxel grid for reconstruction"""\n        # This would implement TSDF fusion or similar voxel integration\n        # For now, return the global point cloud\n        return self.global_point_cloud\n\n    def extract_mesh(self, method: str = \'poisson\') -> Dict:\n        """Extract mesh from point cloud"""\n        # This would use Poisson surface reconstruction or similar\n        # For now, return a simplified representation\n        return {\n            \'vertices\': self.global_point_cloud[:, :3],\n            \'faces\': np.empty((0, 3)),  # Placeholder\n            \'normals\': np.empty((0, 3))  # Placeholder\n        }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"multi-modal-perception-fusion",children:"Multi-Modal Perception Fusion"}),"\n",(0,s.jsx)(n.h3,{id:"sensor-fusion-framework",children:"Sensor Fusion Framework"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class PerceptionFusion:\n    \"\"\"Fuses information from multiple perception modules\"\"\"\n\n    def __init__(self):\n        self.object_detector = CustomObjectDetector()\n        self.segmentation_module = CustomSemanticSegmenter()\n        self.depth_processor = DepthProcessor(None)  # Will be set later\n        self.scene_reconstructor = SceneReconstructor(None)  # Will be set later\n\n        # Tracking and association\n        self.trackers = {}\n        self.next_track_id = 0\n        self.association_threshold = 0.3  # IoU threshold for association\n\n    def fuse_multi_modal_data(self, rgb_image: np.ndarray,\n                            depth_image: np.ndarray = None,\n                            camera_pose: np.ndarray = None) -> Dict:\n        \"\"\"Fuse data from multiple modalities\"\"\"\n        fusion_result = {\n            'timestamp': time.time(),\n            'objects': [],\n            'scene_structure': {},\n            'spatial_relations': [],\n            'semantic_map': {},\n            'confidence_map': {}\n        }\n\n        # Perform object detection\n        if rgb_image is not None:\n            object_detections = self.object_detector.detect_robot_objects(rgb_image)\n            fusion_result['objects'] = self._process_detections(object_detections, rgb_image, depth_image)\n\n        # Perform semantic segmentation\n        if rgb_image is not None:\n            segmentation_result = self.segmentation_module.segment_robot_environment(rgb_image)\n            fusion_result['semantic_map'] = segmentation_result\n\n        # Process depth information if available\n        if depth_image is not None and rgb_image is not None:\n            fusion_result['depth_analysis'] = self._analyze_depth(\n                rgb_image, depth_image, fusion_result['objects']\n            )\n\n        # Update tracking\n        self._update_tracking(fusion_result['objects'])\n\n        # Analyze spatial relations\n        fusion_result['spatial_relations'] = self._analyze_spatial_relations(\n            fusion_result['objects']\n        )\n\n        # Create confidence map\n        fusion_result['confidence_map'] = self._create_confidence_map(\n            fusion_result['objects'], segmentation_result\n        )\n\n        return fusion_result\n\n    def _process_detections(self, detections: Dict, rgb_image: np.ndarray,\n                          depth_image: np.ndarray = None) -> List[Dict]:\n        \"\"\"Process object detection results with 3D information\"\"\"\n        objects = []\n\n        for i in range(len(detections['boxes'])):\n            box = detections['boxes'][i]\n            label = detections['labels'][i]\n            score = detections['scores'][i]\n\n            # Extract object properties\n            x1, y1, x2, y2 = map(int, box)\n            center_x = int((x1 + x2) / 2)\n            center_y = int((y1 + y2) / 2)\n\n            # Get 3D information if depth is available\n            centroid_3d = None\n            size_3d = None\n\n            if depth_image is not None and center_y < depth_image.shape[0] and center_x < depth_image.shape[1]:\n                # Get depth at object center\n                avg_depth = np.mean(depth_image[y1:y2, x1:x2][depth_image[y1:y2, x1:x2] > 0])\n\n                if avg_depth > 0:\n                    # Convert 2D center to 3D\n                    fx, fy = self.depth_processor.intrinsics.fx, self.depth_processor.intrinsics.fy\n                    cx, cy = self.depth_processor.intrinsics.cx, self.depth_processor.intrinsics.cy\n\n                    x_3d = (center_x - cx) * avg_depth / fx\n                    y_3d = (center_y - cy) * avg_depth / fy\n                    z_3d = avg_depth\n\n                    centroid_3d = (x_3d, y_3d, z_3d)\n\n                    # Estimate 3D size\n                    width_2d = x2 - x1\n                    height_2d = y2 - y1\n                    # Simple approximation: assume object is at depth avg_depth\n                    width_3d = width_2d * avg_depth / fx\n                    height_3d = height_2d * avg_depth / fy\n                    size_3d = (width_3d, height_3d, avg_depth)  # Approximate depth\n\n            # Create object representation\n            obj = {\n                'id': self._get_object_id(label, (x1, y1, x2, y2)),\n                'label': label,\n                'confidence': float(score),\n                'bbox_2d': (int(x1), int(y1), int(x2), int(y2)),\n                'centroid_2d': (center_x, center_y),\n                'centroid_3d': centroid_3d,\n                'size_3d': size_3d,\n                'color': self._get_dominant_color(rgb_image, x1, y1, x2, y2),\n                'track_id': self._associate_with_track((x1, y1, x2, y2), label)\n            }\n\n            objects.append(obj)\n\n        return objects\n\n    def _get_object_id(self, label: str, bbox: Tuple[int, int, int, int]) -> str:\n        \"\"\"Generate unique object ID\"\"\"\n        # Simple ID based on label and approximate position\n        x1, y1, x2, y2 = bbox\n        center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n        return f\"{label}_{center_x}_{center_y}\"\n\n    def _get_dominant_color(self, image: np.ndarray, x1: int, y1: int, x2: int, y2: int) -> Tuple[int, int, int]:\n        \"\"\"Get dominant color in bounding box\"\"\"\n        roi = image[y1:y2, x1:x2]\n        if roi.size == 0:\n            return (128, 128, 128)  # Default gray\n\n        # Simple dominant color calculation (mean of each channel)\n        mean_color = np.mean(roi, axis=(0, 1))\n        return tuple(map(int, mean_color))\n\n    def _associate_with_track(self, bbox: Tuple[int, int, int, int], label: str) -> int:\n        \"\"\"Associate detection with existing track\"\"\"\n        best_match = None\n        best_iou = 0\n\n        for track_id, track_info in self.trackers.items():\n            if track_info['label'] == label:\n                iou = self._calculate_iou(bbox, track_info['bbox'])\n                if iou > best_iou and iou > self.association_threshold:\n                    best_match = track_id\n                    best_iou = iou\n\n        if best_match is not None:\n            # Update track\n            self.trackers[best_match]['bbox'] = bbox\n            self.trackers[best_match]['last_seen'] = time.time()\n            return best_match\n        else:\n            # Create new track\n            new_track_id = self.next_track_id\n            self.trackers[new_track_id] = {\n                'label': label,\n                'bbox': bbox,\n                'first_seen': time.time(),\n                'last_seen': time.time()\n            }\n            self.next_track_id += 1\n            return new_track_id\n\n    def _calculate_iou(self, bbox1: Tuple[int, int, int, int],\n                      bbox2: Tuple[int, int, int, int]) -> float:\n        \"\"\"Calculate Intersection over Union\"\"\"\n        x1_1, y1_1, x2_1, y2_1 = bbox1\n        x1_2, y1_2, x2_2, y2_2 = bbox2\n\n        # Calculate intersection\n        x1_inter = max(x1_1, x1_2)\n        y1_inter = max(y1_1, y1_2)\n        x2_inter = min(x2_1, x2_2)\n        y2_inter = min(y2_1, y2_2)\n\n        if x2_inter <= x1_inter or y2_inter <= y1_inter:\n            return 0.0\n\n        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n        area2 = (x2_2 - x2_2) * (y2_2 - y1_2)\n        union_area = area1 + area2 - inter_area\n\n        return inter_area / union_area if union_area > 0 else 0.0\n\n    def _update_tracking(self, objects: List[Dict]):\n        \"\"\"Update object tracking information\"\"\"\n        # Clean up old tracks (those not seen in a while)\n        current_time = time.time()\n        tracks_to_remove = []\n\n        for track_id, track_info in self.trackers.items():\n            if current_time - track_info['last_seen'] > 5.0:  # 5 seconds\n                tracks_to_remove.append(track_id)\n\n        for track_id in tracks_to_remove:\n            del self.trackers[track_id]\n\n    def _analyze_spatial_relations(self, objects: List[Dict]) -> List[Dict]:\n        \"\"\"Analyze spatial relations between objects\"\"\"\n        relations = []\n\n        for i, obj1 in enumerate(objects):\n            for j, obj2 in enumerate(objects):\n                if i != j:\n                    # Calculate spatial relation\n                    rel = self._calculate_spatial_relation(obj1, obj2)\n                    if rel:\n                        relations.append(rel)\n\n        return relations\n\n    def _calculate_spatial_relation(self, obj1: Dict, obj2: Dict) -> Optional[Dict]:\n        \"\"\"Calculate spatial relation between two objects\"\"\"\n        if obj1['centroid_3d'] is None or obj2['centroid_3d'] is None:\n            return None\n\n        # Calculate 3D distance\n        pos1 = np.array(obj1['centroid_3d'])\n        pos2 = np.array(obj2['centroid_3d'])\n        distance = np.linalg.norm(pos1 - pos2)\n\n        # Determine spatial relation based on distance and relative position\n        direction_vector = pos2 - pos1\n        dx, dy, dz = direction_vector\n\n        # Determine qualitative spatial relation\n        if distance < 0.5:  # Very close\n            relation = 'adjacent'\n        elif distance < 2.0:  # Close\n            relation = 'near'\n        else:  # Far\n            relation = 'far'\n\n        # Determine directional relation\n        if abs(dx) > abs(dy) and abs(dx) > abs(dz):\n            if dx > 0:\n                direction = 'right'\n            else:\n                direction = 'left'\n        elif abs(dy) > abs(dz):\n            if dy > 0:\n                direction = 'down'\n            else:\n                direction = 'up'\n        else:\n            if dz > 0:\n                direction = 'behind'\n            else:\n                direction = 'in_front'\n\n        return {\n            'object1': obj1['id'],\n            'object2': obj2['id'],\n            'relation': relation,\n            'direction': direction,\n            'distance': float(distance),\n            'vector': (float(dx), float(dy), float(dz))\n        }\n\n    def _analyze_depth(self, rgb_image: np.ndarray, depth_image: np.ndarray,\n                      objects: List[Dict]) -> Dict:\n        \"\"\"Analyze depth information in context of detected objects\"\"\"\n        analysis = {\n            'surface_types': [],\n            'traversability': {},\n            'obstacle_map': {},\n            'object_depths': []\n        }\n\n        # Analyze each detected object's depth context\n        for obj in objects:\n            if obj['centroid_2d']:\n                cx, cy = obj['centroid_2d']\n                if 0 <= cy < depth_image.shape[0] and 0 <= cx < depth_image.shape[1]:\n                    depth_at_object = depth_image[cy, cx]\n                    analysis['object_depths'].append({\n                        'object_id': obj['id'],\n                        'depth': float(depth_at_object),\n                        'reliable': depth_at_object > 0\n                    })\n\n        # Analyze overall scene traversability\n        traversable_regions = self._analyze_traversability(depth_image)\n        analysis['traversability'] = traversable_regions\n\n        # Create obstacle map\n        obstacle_map = self._create_obstacle_map(depth_image)\n        analysis['obstacle_map'] = obstacle_map\n\n        return analysis\n\n    def _analyze_traversability(self, depth_image: np.ndarray) -> Dict:\n        \"\"\"Analyze scene traversability based on depth\"\"\"\n        # Simple traversability analysis\n        # In practice, this would use more sophisticated methods\n\n        # Assume traversable regions are at floor level (not too high, not too low)\n        traversable = (depth_image > 0.1) & (depth_image < 0.5)  # Ground level\n        semi_traversable = (depth_image > 0.5) & (depth_image < 1.0)  # Maybe traversable\n        non_traversable = depth_image >= 1.0  # Too high or unknown\n\n        return {\n            'traversable': traversable.astype(np.uint8) * 255,\n            'semi_traversable': semi_traversable.astype(np.uint8) * 128,\n            'non_traversable': non_traversable.astype(np.uint8) * 64\n        }\n\n    def _create_obstacle_map(self, depth_image: np.ndarray) -> np.ndarray:\n        \"\"\"Create obstacle map from depth image\"\"\"\n        # Simple obstacle detection: anything closer than threshold\n        obstacle_threshold = 0.8  # meters\n        obstacles = (depth_image > 0) & (depth_image < obstacle_threshold)\n        return obstacles.astype(np.uint8) * 255\n\n    def _create_confidence_map(self, objects: List[Dict],\n                             segmentation_result: Dict) -> np.ndarray:\n        \"\"\"Create confidence map combining object detection and segmentation confidences\"\"\"\n        if not objects:\n            return np.zeros((480, 640), dtype=np.float32)  # Default size\n\n        # Create confidence map based on detection confidences\n        confidence_map = np.zeros((480, 640), dtype=np.float32)  # Default size\n\n        for obj in objects:\n            x1, y1, x2, y2 = obj['bbox_2d']\n            confidence = obj['confidence']\n\n            # Spread confidence within bounding box\n            confidence_map[y1:y2, x1:x2] = np.maximum(\n                confidence_map[y1:y2, x1:x2], confidence\n            )\n\n        return confidence_map\n"})}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-downstream-systems",children:"Integration with Downstream Systems"}),"\n",(0,s.jsx)(n.h3,{id:"perception-interface-for-navigation-and-manipulation",children:"Perception Interface for Navigation and Manipulation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class PerceptionInterface:\n    \"\"\"Interface between perception system and downstream applications\"\"\"\n\n    def __init__(self, fusion_module: PerceptionFusion):\n        self.fusion = fusion_module\n        self.last_fusion_result = None\n        self.fusion_lock = threading.Lock()\n\n    def get_navigation_map(self) -> Dict:\n        \"\"\"Get map information for navigation system\"\"\"\n        with self.fusion_lock:\n            if self.last_fusion_result is None:\n                return {\n                    'obstacle_map': np.zeros((100, 100), dtype=np.uint8),\n                    'traversable_map': np.ones((100, 100), dtype=np.uint8),\n                    'object_locations': [],\n                    'update_time': time.time()\n                }\n\n            # Extract navigation-relevant information\n            obstacle_map = self.last_fusion_result.get('depth_analysis', {}).get('obstacle_map',\n                                                                               np.zeros((480, 640), dtype=np.uint8))\n\n            traversable_map = self.last_fusion_result.get('depth_analysis', {}).get('traversability', {}).get('traversable',\n                                                                                                           np.ones((480, 640), dtype=np.uint8))\n\n            object_locations = []\n            for obj in self.last_fusion_result.get('objects', []):\n                if obj.get('centroid_3d') is not None:\n                    x, y, z = obj['centroid_3d']\n                    object_locations.append({\n                        'id': obj['id'],\n                        'position': (x, y, z),\n                        'label': obj['label'],\n                        'type': 'obstacle' if obj['label'] in ['chair', 'table', 'person'] else 'landmark'\n                    })\n\n            return {\n                'obstacle_map': obstacle_map,\n                'traversable_map': traversable_map,\n                'object_locations': object_locations,\n                'update_time': self.last_fusion_result.get('timestamp', time.time())\n            }\n\n    def get_manipulation_targets(self) -> List[Dict]:\n        \"\"\"Get objects suitable for manipulation\"\"\"\n        with self.fusion_lock:\n            if self.last_fusion_result is None:\n                return []\n\n            manipulation_targets = []\n\n            for obj in self.last_fusion_result.get('objects', []):\n                # Check if object is suitable for manipulation\n                if (obj['label'] in ['cup', 'bottle', 'book', 'phone', 'keys'] and\n                    obj.get('centroid_3d') is not None and\n                    obj.get('size_3d') is not None):\n\n                    x, y, z = obj['centroid_3d']\n                    width, height, depth = obj['size_3d']\n\n                    # Check if object is within manipulator reach\n                    # (Assuming robot base is at origin and manipulator reach is 1.0m)\n                    distance = np.sqrt(x**2 + y**2 + z**2)\n\n                    if distance < 1.0 and z < 1.5:  # Within reach and not too high\n                        target = {\n                            'id': obj['id'],\n                            'label': obj['label'],\n                            'position': (float(x), float(y), float(z)),\n                            'size': (float(width), float(height), float(depth)),\n                            'color': obj.get('color', (128, 128, 128)),\n                            'confidence': obj['confidence'],\n                            'approach_direction': self._determine_approach_direction(obj),\n                            'grasp_points': self._estimate_grasp_points(obj)\n                        }\n                        manipulation_targets.append(target)\n\n            return manipulation_targets\n\n    def _determine_approach_direction(self, obj: Dict) -> Tuple[float, float, float]:\n        \"\"\"Determine best approach direction for manipulation\"\"\"\n        # For now, approach from the side opposite to the robot\n        # In practice, this would consider object shape and orientation\n        x, y, z = obj['centroid_3d']\n\n        # Approach from the front (positive x direction)\n        approach_dir = (1.0, 0.0, 0.0)\n\n        return approach_dir\n\n    def _estimate_grasp_points(self, obj: Dict) -> List[Tuple[float, float, float]]:\n        \"\"\"Estimate potential grasp points on object\"\"\"\n        # For now, return the centroid and a couple of offset points\n        # In practice, this would use shape analysis and grasp planning\n        x, y, z = obj['centroid_3d']\n        width, height, depth = obj['size_3d']\n\n        grasp_points = [\n            (x, y, z),  # Center\n            (x + width/2, y, z),  # Side 1\n            (x - width/2, y, z),  # Side 2\n            (x, y + depth/2, z),  # Front\n            (x, y - depth/2, z),  # Back\n        ]\n\n        return grasp_points\n\n    def get_language_grounding(self, command_entities: Dict) -> Dict:\n        \"\"\"Provide visual grounding for language entities\"\"\"\n        with self.fusion_lock:\n            if self.last_fusion_result is None:\n                return {'grounding_success': False, 'entities': {}}\n\n            grounding_result = {\n                'grounding_success': True,\n                'entities': {}\n            }\n\n            for entity_type, entity_value in command_entities.items():\n                if entity_type == 'object':\n                    # Find object in perception results\n                    matched_objects = [\n                        obj for obj in self.last_fusion_result.get('objects', [])\n                        if entity_value.lower() in obj['label'].lower()\n                    ]\n\n                    if matched_objects:\n                        # Take the highest confidence match\n                        best_match = max(matched_objects, key=lambda x: x['confidence'])\n\n                        grounding_result['entities'][entity_value] = {\n                            'found': True,\n                            'position_3d': best_match.get('centroid_3d'),\n                            'position_2d': best_match.get('centroid_2d'),\n                            'bbox': best_match.get('bbox_2d'),\n                            'confidence': best_match['confidence'],\n                            'size_3d': best_match.get('size_3d'),\n                            'color': best_match.get('color')\n                        }\n                    else:\n                        grounding_result['entities'][entity_value] = {\n                            'found': False,\n                            'position_3d': None,\n                            'position_2d': None,\n                            'bbox': None,\n                            'confidence': 0.0,\n                            'size_3d': None,\n                            'color': None\n                        }\n                elif entity_type == 'location':\n                    # For locations, find semantic regions\n                    semantic_map = self.last_fusion_result.get('semantic_map', {})\n                    # This would involve finding regions labeled with the location name\n                    # For now, return a placeholder\n                    grounding_result['entities'][entity_value] = {\n                        'found': True,\n                        'region_centroid': (0.0, 0.0, 0.0),  # Placeholder\n                        'region_area': 0,  # Placeholder\n                        'confidence': 0.5  # Placeholder\n                    }\n\n            return grounding_result\n\n    def update_perception(self, rgb_image: np.ndarray, depth_image: np.ndarray = None,\n                         camera_pose: np.ndarray = None):\n        \"\"\"Update perception with new sensor data\"\"\"\n        fusion_result = self.fusion.fuse_multi_modal_data(\n            rgb_image, depth_image, camera_pose\n        )\n\n        with self.fusion_lock:\n            self.last_fusion_result = fusion_result\n\n    def get_fresh_perception_data(self) -> Dict:\n        \"\"\"Get the most recent perception data\"\"\"\n        with self.fusion_lock:\n            return self.last_fusion_result.copy() if self.last_fusion_result else {}\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"efficient-perception-pipeline",children:"Efficient Perception Pipeline"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class OptimizedPerceptionPipeline:\n    """Optimized perception pipeline for real-time performance"""\n\n    def __init__(self, device: str = "cuda"):\n        self.device = torch.device(device if torch.cuda.is_available() else "cpu")\n\n        # Initialize modules\n        self.object_detector = CustomObjectDetector(device=self.device)\n        self.segmentation_module = CustomSemanticSegmenter(device=self.device)\n        self.fusion_module = PerceptionFusion()\n\n        # Performance optimization\n        self.input_queue = queue.Queue(maxsize=3)  # Limit input queue\n        self.output_queue = queue.Queue(maxsize=3)  # Limit output queue\n        self.processing_thread = None\n        self.is_running = False\n        self.fps_counter = 0\n        self.last_fps_update = time.time()\n\n        # Threading\n        self.processing_lock = threading.Lock()\n        self.shutdown_event = threading.Event()\n\n    def start_pipeline(self):\n        """Start the perception pipeline"""\n        if self.is_running:\n            return\n\n        self.is_running = True\n        self.processing_thread = threading.Thread(target=self._processing_worker)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n    def stop_pipeline(self):\n        """Stop the perception pipeline"""\n        if not self.is_running:\n            return\n\n        self.shutdown_event.set()\n        self.is_running = False\n\n        if self.processing_thread:\n            self.processing_thread.join(timeout=2.0)\n\n    def _processing_worker(self):\n        """Processing worker thread"""\n        while not self.shutdown_event.is_set():\n            try:\n                # Get input data\n                data = self.input_queue.get(timeout=0.1)\n\n                # Process data\n                rgb_image, depth_image, camera_pose = data\n                result = self.process_frame(rgb_image, depth_image, camera_pose)\n\n                # Put result in output queue\n                try:\n                    self.output_queue.put_nowait(result)\n                except queue.Full:\n                    # Drop result if output queue is full\n                    pass\n\n                # Update FPS counter\n                self.fps_counter += 1\n                current_time = time.time()\n                if current_time - self.last_fps_update >= 1.0:\n                    print(f"Perception FPS: {self.fps_counter}")\n                    self.fps_counter = 0\n                    self.last_fps_update = current_time\n\n            except queue.Empty:\n                continue  # No data to process\n            except Exception as e:\n                print(f"Error in perception pipeline: {e}")\n                continue\n\n    def process_frame(self, rgb_image: np.ndarray, depth_image: np.ndarray = None,\n                     camera_pose: np.ndarray = None) -> Dict:\n        """Process a single frame through the optimized pipeline"""\n        start_time = time.time()\n\n        try:\n            # Perform perception tasks\n            result = self.fusion_module.fuse_multi_modal_data(\n                rgb_image, depth_image, camera_pose\n            )\n\n            result[\'processing_time\'] = time.time() - start_time\n            result[\'timestamp\'] = time.time()\n\n            return result\n\n        except Exception as e:\n            print(f"Error processing frame: {e}")\n            return {\n                \'error\': str(e),\n                \'processing_time\': time.time() - start_time,\n                \'timestamp\': time.time()\n            }\n\n    def submit_frame(self, rgb_image: np.ndarray, depth_image: np.ndarray = None,\n                    camera_pose: np.ndarray = None):\n        """Submit frame for processing"""\n        try:\n            self.input_queue.put_nowait((rgb_image, depth_image, camera_pose))\n        except queue.Full:\n            # Drop frame if queue is full\n            pass\n\n    def get_result(self, timeout: float = 0.0) -> Optional[Dict]:\n        """Get processed result"""\n        try:\n            return self.output_queue.get(timeout=timeout)\n        except queue.Empty:\n            return None\n\n    def get_performance_stats(self) -> Dict:\n        """Get performance statistics"""\n        return {\n            \'input_queue_size\': self.input_queue.qsize(),\n            \'output_queue_size\': self.output_queue.qsize(),\n            \'is_running\': self.is_running\n        }\n\n    def optimize_for_robot_tasks(self):\n        """Optimize perception pipeline for specific robot tasks"""\n        # Focus on relevant object classes\n        self.object_detector.custom_classes = [\n            \'background\', \'cup\', \'bottle\', \'book\', \'phone\', \'keys\',\n            \'chair\', \'table\', \'kitchen_counter\', \'refrigerator\', \'microwave\'\n        ]\n\n        # Adjust detection thresholds for robot tasks\n        # This would involve fine-tuning model parameters\n        pass\n'})}),"\n",(0,s.jsx)(n.p,{children:"The perception system integration provides comprehensive capabilities for the autonomous humanoid robot to understand its environment through multiple sensor modalities. The system efficiently fuses information from cameras, depth sensors, and other environmental sensors to create a rich understanding of the world that supports navigation, manipulation, and human-robot interaction tasks."})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>r});var i=t(6540);const s={},a=i.createContext(s);function o(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);