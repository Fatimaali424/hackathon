"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8899],{8453(e,n,i){i.d(n,{R:()=>o,x:()=>r});var s=i(6540);const t={},a=s.createContext(t);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:n},e.children)}},8500(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4/lab-10-vision-language","title":"Lab 10: Basic Vision-Language Integration","description":"Overview","source":"@site/docs/module-4/lab-10-vision-language.md","sourceDirName":"module-4","slug":"/module-4/lab-10-vision-language","permalink":"/hackathon/docs/module-4/lab-10-vision-language","draft":false,"unlisted":false,"editUrl":"https://github.com/Fatimaali424/hackathon/edit/main/website/docs/module-4/lab-10-vision-language.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Human-Robot Interaction","permalink":"/hackathon/docs/module-4/human-robot-interaction"},"next":{"title":"Lab 11: Voice Command Processing","permalink":"/hackathon/docs/module-4/lab-11-voice-command"}}');var t=i(4848),a=i(8453);const o={sidebar_position:5},r="Lab 10: Basic Vision-Language Integration",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Hardware and Software Requirements",id:"hardware-and-software-requirements",level:2},{value:"Required Hardware",id:"required-hardware",level:3},{value:"Required Software",id:"required-software",level:3},{value:"Lab Setup",id:"lab-setup",level:2},{value:"Environment Configuration",id:"environment-configuration",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Basic Vision-Language Model",id:"step-1-basic-vision-language-model",level:3},{value:"Step 2: Image-Text Matching Implementation",id:"step-2-image-text-matching-implementation",level:3},{value:"Step 3: Simple Image Captioning",id:"step-3-simple-image-captioning",level:3},{value:"Step 4: Visual Question Answering",id:"step-4-visual-question-answering",level:3},{value:"Step 5: Complete Vision-Language System",id:"step-5-complete-vision-language-system",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Basic Functionality Test",id:"basic-functionality-test",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:3},{value:"Integration with Robotics",id:"integration-with-robotics",level:2},{value:"ROS 2 Integration Example",id:"ros-2-integration-example",level:3},{value:"Lab Deliverables",id:"lab-deliverables",level:2},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Extensions (Optional)",id:"extensions-optional",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lab-10-basic-vision-language-integration",children:"Lab 10: Basic Vision-Language Integration"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This lab provides hands-on experience with basic vision-language integration techniques, focusing on connecting visual perception with natural language understanding. You will implement fundamental approaches for grounding language in visual contexts and explore how robots can use both modalities together for more intelligent behavior."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this lab, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement basic vision-language models that connect images with text"}),"\n",(0,t.jsx)(n.li,{children:"Ground language expressions in visual scenes"}),"\n",(0,t.jsx)(n.li,{children:"Create simple image captioning and visual question answering systems"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate vision-language model performance"}),"\n",(0,t.jsx)(n.li,{children:"Integrate vision-language capabilities with robotic systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completion of Module 1 (ROS 2 fundamentals)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Module 2 (Simulation concepts)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Module 3 (Isaac perception)"}),"\n",(0,t.jsx)(n.li,{children:"Basic understanding of deep learning and PyTorch"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with computer vision concepts"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hardware-and-software-requirements",children:"Hardware and Software Requirements"}),"\n",(0,t.jsx)(n.h3,{id:"required-hardware",children:"Required Hardware"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"GPU with CUDA support (NVIDIA RTX 3060 or better recommended)"}),"\n",(0,t.jsx)(n.li,{children:"System with 16GB+ RAM"}),"\n",(0,t.jsx)(n.li,{children:"Web camera or image dataset for testing"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"required-software",children:"Required Software"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Python 3.8+ with PyTorch and torchvision"}),"\n",(0,t.jsx)(n.li,{children:"Transformers library (Hugging Face)"}),"\n",(0,t.jsx)(n.li,{children:"OpenCV and PIL for image processing"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 Humble with vision packages"}),"\n",(0,t.jsx)(n.li,{children:"Jupyter notebook or Python IDE"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"lab-setup",children:"Lab Setup"}),"\n",(0,t.jsx)(n.h3,{id:"environment-configuration",children:"Environment Configuration"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Install required packages:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install torch torchvision torchaudio\npip install transformers datasets\npip install opencv-python pillow\npip install jupyter notebook\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Verify GPU availability:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nprint(f"CUDA available: {torch.cuda.is_available()}")\nprint(f"GPU count: {torch.cuda.device_count()}")\nif torch.cuda.is_available():\n    print(f"Current GPU: {torch.cuda.get_device_name()}")\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Download sample dataset:"}),"\nFor this lab, we'll use a small sample dataset or synthetic data to demonstrate concepts."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-basic-vision-language-model",children:"Step 1: Basic Vision-Language Model"}),"\n",(0,t.jsx)(n.p,{children:"Create a simple model that can connect visual features with text embeddings:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# vision_language_model.py\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.nn.functional as F\n\nclass BasicVisionLanguageModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512):\n        super().__init__()\n\n        # Vision encoder (using pre-trained ResNet)\n        self.vision_encoder = models.resnet18(pretrained=True)\n        # Replace final layer for feature extraction\n        num_features = self.vision_encoder.fc.in_features\n        self.vision_encoder.fc = nn.Linear(num_features, hidden_dim)\n\n        # Text encoder (simple embedding + LSTM)\n        self.text_embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.text_encoder = nn.LSTM(\n            embedding_dim, hidden_dim, batch_first=True\n        )\n\n        # Vision-language fusion layer\n        self.fusion = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n\n        # Output heads for different tasks\n        self.classification_head = nn.Linear(hidden_dim, 10)  # Example: 10 classes\n        self.similarity_head = nn.Linear(hidden_dim, 1)      # For image-text matching\n\n    def encode_image(self, image):\n        """Encode image to feature vector"""\n        features = self.vision_encoder(image)\n        return F.normalize(features, dim=-1)\n\n    def encode_text(self, text):\n        """Encode text to feature vector"""\n        embedded = self.text_embedding(text)\n        encoded, (hidden, _) = self.text_encoder(embedded)\n        # Use final hidden state\n        text_features = hidden[-1]\n        return F.normalize(text_features, dim=-1)\n\n    def forward(self, image, text):\n        """Forward pass combining image and text"""\n        # Encode both modalities\n        image_features = self.encode_image(image)\n        text_features = self.encode_text(text)\n\n        # Concatenate and fuse features\n        combined_features = torch.cat([image_features, text_features], dim=-1)\n        fused_features = self.fusion(combined_features)\n\n        # Generate outputs\n        classification_logits = self.classification_head(fused_features)\n        similarity_score = self.similarity_head(fused_features)\n\n        return classification_logits, similarity_score\n\n# Example usage\ndef create_model():\n    """Create and return a vision-language model"""\n    model = BasicVisionLanguageModel(vocab_size=10000)  # Example vocab size\n    return model\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-image-text-matching-implementation",children:"Step 2: Image-Text Matching Implementation"}),"\n",(0,t.jsx)(n.p,{children:"Implement a simple image-text matching system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# image_text_matching.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom vision_language_model import BasicVisionLanguageModel\n\nclass ImageTextMatcher(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.vl_model = model\n\n    def forward(self, images, texts):\n        """Compute similarity between images and texts"""\n        batch_size = images.size(0)\n\n        # Encode all images and texts\n        image_features = []\n        text_features = []\n\n        for i in range(batch_size):\n            img_feat = self.vl_model.encode_image(images[i:i+1])\n            txt_feat = self.vl_model.encode_text(texts[i:i+1])\n            image_features.append(img_feat)\n            text_features.append(txt_feat)\n\n        image_features = torch.cat(image_features, dim=0)\n        text_features = torch.cat(text_features, dim=0)\n\n        # Compute similarity matrix\n        similarity_matrix = torch.matmul(image_features, text_features.t())\n\n        return similarity_matrix\n\n    def compute_loss(self, images, texts, labels):\n        """Compute contrastive loss for image-text matching"""\n        similarity_matrix = self(images, texts)\n\n        # Labels should be diagonal (correct pairs)\n        batch_size = images.size(0)\n        target = torch.arange(batch_size).to(images.device)\n\n        # Cross-entropy loss\n        loss_i2t = F.cross_entropy(similarity_matrix, target)\n        loss_t2i = F.cross_entropy(similarity_matrix.t(), target)\n\n        return (loss_i2t + loss_t2i) / 2\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-simple-image-captioning",children:"Step 3: Simple Image Captioning"}),"\n",(0,t.jsx)(n.p,{children:"Create a basic image captioning system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# image_captioning.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass ImageCaptioner(nn.Module):\n    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, max_length=20):\n        super().__init__()\n\n        # Vision encoder\n        self.vision_encoder = models.resnet18(pretrained=True)\n        num_features = self.vision_encoder.fc.in_features\n        self.vision_encoder.fc = nn.Linear(num_features, hidden_dim)\n\n        # Text decoder\n        self.text_embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.text_decoder = nn.LSTM(\n            embedding_dim, hidden_dim, batch_first=True\n        )\n        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n\n    def forward(self, image, target_captions=None):\n        """Generate captions for images"""\n        batch_size = image.size(0)\n\n        # Encode image\n        image_features = self.vision_encoder(image)  # [batch, hidden_dim]\n\n        if target_captions is not None:\n            # Training mode: use teacher forcing\n            embedded = self.text_embedding(target_captions)\n            decoder_output, _ = self.text_decoder(embedded)\n            logits = self.output_projection(decoder_output)\n            return logits\n        else:\n            # Inference mode: generate caption word by word\n            return self.generate_caption(image_features)\n\n    def generate_caption(self, image_features):\n        """Generate caption using greedy decoding"""\n        batch_size = image_features.size(0)\n\n        # Start with start token (assuming token 1 is start token)\n        current_tokens = torch.ones(batch_size, 1, dtype=torch.long) * 1\n\n        all_tokens = [current_tokens]\n\n        for _ in range(self.max_length):\n            embedded = self.text_embedding(current_tokens)\n\n            # For the first step, use image features as initial hidden state\n            if len(all_tokens) == 1:\n                h0 = image_features.unsqueeze(0).repeat(1, 1, 1)  # [1, batch, hidden]\n                c0 = torch.zeros_like(h0)\n                decoder_output, (h_n, c_n) = self.text_decoder(embedded, (h0, c0))\n            else:\n                decoder_output, (h_n, c_n) = self.text_decoder(embedded, (h_n, c_n))\n\n            logits = self.output_projection(decoder_output)\n            next_tokens = torch.argmax(logits, dim=-1)\n            current_tokens = next_tokens[:, -1:]  # Take last prediction\n\n            all_tokens.append(current_tokens)\n\n            # Stop if end token is generated (assuming token 2 is end token)\n            if torch.all(current_tokens == 2):\n                break\n\n        return torch.cat(all_tokens, dim=1)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-4-visual-question-answering",children:"Step 4: Visual Question Answering"}),"\n",(0,t.jsx)(n.p,{children:"Implement a basic visual question answering system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# visual_qa.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass VisualQAModel(nn.Module):\n    def __init__(self, vocab_size, answer_vocab_size, hidden_dim=512):\n        super().__init__()\n\n        # Vision encoder\n        self.vision_encoder = models.resnet18(pretrained=True)\n        num_features = self.vision_encoder.fc.in_features\n        self.vision_encoder.fc = nn.Linear(num_features, hidden_dim)\n\n        # Question encoder (LSTM-based)\n        self.question_embedding = nn.Embedding(vocab_size, hidden_dim // 2)\n        self.question_encoder = nn.LSTM(\n            hidden_dim // 2, hidden_dim // 2, batch_first=True\n        )\n\n        # Fusion layer\n        self.fusion = nn.Sequential(\n            nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3)\n        )\n\n        # Answer classifier\n        self.answer_classifier = nn.Linear(hidden_dim, answer_vocab_size)\n\n    def forward(self, image, question):\n        """Answer questions about images"""\n        # Encode image\n        image_features = self.vision_encoder(image)\n\n        # Encode question\n        question_embedded = self.question_embedding(question)\n        question_encoded, (hidden, _) = self.question_encoder(question_embedded)\n        # Use last hidden state\n        question_features = hidden[-1]\n\n        # Fuse image and question features\n        combined_features = torch.cat([image_features, question_features], dim=-1)\n        fused_features = self.fusion(combined_features)\n\n        # Generate answer\n        answer_logits = self.answer_classifier(fused_features)\n\n        return answer_logits\n\n    def predict_answer(self, image, question, answer_vocab):\n        """Predict the most likely answer"""\n        logits = self(image, question)\n        predicted_idx = torch.argmax(logits, dim=-1)\n\n        # Convert to answer text\n        answers = []\n        for idx in predicted_idx:\n            answer = answer_vocab.get(idx.item(), "unknown")\n            answers.append(answer)\n\n        return answers\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-5-complete-vision-language-system",children:"Step 5: Complete Vision-Language System"}),"\n",(0,t.jsx)(n.p,{children:"Create a complete system that integrates all components:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# complete_vl_system.py\n\nimport torch\nimport torch.nn as nn\nfrom vision_language_model import BasicVisionLanguageModel\nfrom image_text_matching import ImageTextMatcher\nfrom image_captioning import ImageCaptioner\nfrom visual_qa import VisualQAModel\n\nclass CompleteVisionLanguageSystem(nn.Module):\n    def __init__(self, vocab_size, answer_vocab_size):\n        super().__init__()\n\n        # Initialize all components\n        self.vision_language_model = BasicVisionLanguageModel(vocab_size)\n        self.image_text_matcher = ImageTextMatcher(self.vision_language_model)\n        self.image_captioner = ImageCaptioner(vocab_size)\n        self.visual_qa = VisualQAModel(vocab_size, answer_vocab_size)\n\n        self.task_specific_weights = nn.ParameterDict({\n            'matching': nn.Parameter(torch.tensor(1.0)),\n            'captioning': nn.Parameter(torch.tensor(1.0)),\n            'qa': nn.Parameter(torch.tensor(1.0))\n        })\n\n    def forward(self, task, **kwargs):\n        \"\"\"Route to appropriate task\"\"\"\n        if task == 'matching':\n            return self.image_text_matcher(kwargs['images'], kwargs['texts'])\n        elif task == 'captioning':\n            return self.image_captioner(kwargs['image'], kwargs.get('target_captions', None))\n        elif task == 'qa':\n            return self.visual_qa(kwargs['image'], kwargs['question'])\n        else:\n            raise ValueError(f\"Unknown task: {task}\")\n\n    def compute_total_loss(self, images, texts, questions, answers, captions):\n        \"\"\"Compute combined loss for all tasks\"\"\"\n        # Image-text matching loss\n        matching_loss = self.image_text_matcher.compute_loss(\n            images, texts, labels=torch.arange(images.size(0)).to(images.device)\n        )\n\n        # Captioning loss (simplified)\n        caption_logits = self.image_captioner(images, captions)\n        caption_loss = F.cross_entropy(\n            caption_logits.view(-1, caption_logits.size(-1)),\n            captions.view(-1)\n        )\n\n        # QA loss\n        qa_logits = self.visual_qa(images, questions)\n        qa_loss = F.cross_entropy(qa_logits, answers)\n\n        # Weighted combination\n        total_loss = (\n            self.task_specific_weights['matching'] * matching_loss +\n            self.task_specific_weights['captioning'] * caption_loss +\n            self.task_specific_weights['qa'] * qa_loss\n        )\n\n        return total_loss, {\n            'matching_loss': matching_loss,\n            'caption_loss': caption_loss,\n            'qa_loss': qa_loss\n        }\n"})}),"\n",(0,t.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,t.jsx)(n.h3,{id:"basic-functionality-test",children:"Basic Functionality Test"}),"\n",(0,t.jsx)(n.p,{children:"Create a test script to validate your implementations:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# test_vision_language.py\n\nimport torch\nfrom complete_vl_system import CompleteVisionLanguageSystem\n\ndef test_basic_functionality():\n    """Test basic functionality of vision-language components"""\n\n    # Parameters\n    vocab_size = 10000\n    answer_vocab_size = 500\n    batch_size = 4\n    image_channels = 3\n    image_height = 224\n    image_width = 224\n\n    print("Testing Vision-Language System...")\n\n    # Create model\n    model = CompleteVisionLanguageSystem(vocab_size, answer_vocab_size)\n\n    # Test image-text matching\n    print("1. Testing Image-Text Matching...")\n    images = torch.randn(batch_size, image_channels, image_height, image_width)\n    texts = torch.randint(0, vocab_size, (batch_size, 10))  # 10 tokens per text\n\n    matcher_output = model(\'matching\', images=images, texts=texts)\n    print(f"   Matching output shape: {matcher_output.shape}")\n\n    # Test image captioning\n    print("2. Testing Image Captioning...")\n    caption_output = model(\'captioning\', image=images[:1])  # Single image for generation\n    print(f"   Caption output shape: {caption_output.shape}")\n\n    # Test visual QA\n    print("3. Testing Visual QA...")\n    questions = torch.randint(0, vocab_size, (batch_size, 8))  # 8 tokens per question\n    answers = torch.randint(0, answer_vocab_size, (batch_size,))\n\n    qa_output = model(\'qa\', image=images, question=questions)\n    print(f"   QA output shape: {qa_output.shape}")\n\n    # Test combined loss\n    print("4. Testing Combined Loss...")\n    total_loss, loss_components = model.compute_total_loss(\n        images, texts, questions, answers, texts  # Using texts as dummy captions\n    )\n    print(f"   Total loss: {total_loss.item():.4f}")\n    print(f"   Loss components: {loss_components}")\n\n    print("All tests passed!")\n\nif __name__ == "__main__":\n    test_basic_functionality()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,t.jsx)(n.p,{children:"Create evaluation metrics for your vision-language system:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# evaluate_vision_language.py\n\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport numpy as np\n\nclass VisionLanguageEvaluator:\n    def __init__(self):\n        self.metrics = {}\n\n    def evaluate_image_text_matching(self, model, test_loader):\n        """Evaluate image-text matching performance"""\n        model.eval()\n        all_predictions = []\n        all_targets = []\n\n        with torch.no_grad():\n            for batch in test_loader:\n                images, texts, targets = batch\n                outputs = model(\'matching\', images=images, texts=texts)\n\n                # For image-text matching, we want to find correct pairs\n                # This is typically done by finding highest similarity scores\n                predictions = torch.argmax(outputs, dim=1)\n                all_predictions.extend(predictions.cpu().numpy())\n                all_targets.extend(targets.cpu().numpy())\n\n        accuracy = accuracy_score(all_targets, all_predictions)\n        precision, recall, f1, _ = precision_recall_fscore_support(\n            all_targets, all_predictions, average=\'weighted\'\n        )\n\n        return {\n            \'accuracy\': accuracy,\n            \'precision\': precision,\n            \'recall\': recall,\n            \'f1_score\': f1\n        }\n\n    def evaluate_captioning(self, model, test_loader, vocab):\n        """Evaluate image captioning performance"""\n        model.eval()\n        bleu_scores = []\n        meteor_scores = []  # Simplified version\n\n        with torch.no_grad():\n            for batch in test_loader:\n                images, target_captions = batch\n\n                # Generate captions\n                generated_captions = model(\'captioning\', image=images)\n\n                # Calculate BLEU scores (simplified)\n                for gen_cap, target_cap in zip(generated_captions, target_captions):\n                    bleu = self.calculate_bleu_score(gen_cap, target_cap)\n                    bleu_scores.append(bleu)\n\n        return {\n            \'avg_bleu\': np.mean(bleu_scores),\n            \'std_bleu\': np.std(bleu_scores)\n        }\n\n    def calculate_bleu_score(self, generated, reference, n=2):\n        """Simplified BLEU score calculation"""\n        # This is a very simplified version - in practice, use nltk.translate.bleu_score\n        gen_tokens = generated.cpu().numpy()\n        ref_tokens = reference.cpu().numpy()\n\n        # Calculate n-gram overlap\n        matches = 0\n        total = 0\n\n        for i in range(len(ref_tokens) - n + 1):\n            if i + n <= len(gen_tokens):\n                if tuple(ref_tokens[i:i+n]) in [tuple(gen_tokens[j:j+n])\n                                              for j in range(len(gen_tokens) - n + 1)]:\n                    matches += 1\n            total += 1\n\n        return matches / total if total > 0 else 0\n\ndef run_comprehensive_evaluation():\n    """Run comprehensive evaluation of the vision-language system"""\n    evaluator = VisionLanguageEvaluator()\n\n    print("Running comprehensive evaluation...")\n\n    # This would typically load a test dataset\n    # For this lab, we\'ll demonstrate the evaluation structure\n    print("Evaluation framework ready.")\n    print("To run full evaluation, implement data loading and call:")\n    print("  evaluator.evaluate_image_text_matching(model, test_loader)")\n    print("  evaluator.evaluate_captioning(model, test_loader, vocab)")\n\nif __name__ == "__main__":\n    run_comprehensive_evaluation()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-robotics",children:"Integration with Robotics"}),"\n",(0,t.jsx)(n.h3,{id:"ros-2-integration-example",children:"ROS 2 Integration Example"}),"\n",(0,t.jsx)(n.p,{children:"Show how to integrate vision-language capabilities with ROS 2:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# vision_language_ros_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Point\nfrom cv_bridge import CvBridge\nimport torch\nimport torchvision.transforms as transforms\nimport numpy as np\n\nclass VisionLanguageROSNode(Node):\n    def __init__(self):\n        super().__init__(\'vision_language_ros_node\')\n\n        # Initialize components\n        self.cv_bridge = CvBridge()\n        self.image_transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n        # Load vision-language model (in practice, you\'d load a trained model)\n        # For this example, we\'ll use a dummy model\n        self.model = self.load_model()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, \'/vl_command\', self.command_callback, 10\n        )\n\n        # Publishers\n        self.response_pub = self.create_publisher(String, \'/vl_response\', 10)\n        self.gaze_pub = self.create_publisher(Point, \'/robot_gaze_target\', 10)\n\n        # Internal state\n        self.current_image = None\n        self.current_command = None\n\n        self.get_logger().info(\'Vision-Language ROS Node initialized\')\n\n    def load_model(self):\n        """Load or initialize the vision-language model"""\n        # In practice, load a pre-trained model\n        # For this example, return a placeholder\n        class DummyModel:\n            def encode_image(self, image):\n                # Return dummy features\n                return torch.randn(1, 512)\n\n        return DummyModel()\n\n    def image_callback(self, msg):\n        """Process incoming camera image"""\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, \'bgr8\')\n\n            # Transform for model input\n            tensor_image = self.image_transform(cv_image)\n            tensor_image = tensor_image.unsqueeze(0)  # Add batch dimension\n\n            self.current_image = tensor_image\n\n            # If we have both image and command, process them\n            if self.current_command:\n                self.process_vision_language_request()\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def command_callback(self, msg):\n        """Process incoming language command"""\n        self.current_command = msg.data\n\n        # If we have both image and command, process them\n        if self.current_image is not None:\n            self.process_vision_language_request()\n\n    def process_vision_language_request(self):\n        """Process combined vision and language input"""\n        if self.current_image is None or self.current_command is None:\n            return\n\n        try:\n            # In a real implementation, this would:\n            # 1. Ground the language command in the visual scene\n            # 2. Generate appropriate robot action or response\n            # 3. Publish results\n\n            # For this example, we\'ll simulate processing\n            response = self.simulate_vision_language_processing(\n                self.current_image, self.current_command\n            )\n\n            # Publish response\n            response_msg = String()\n            response_msg.data = response\n            self.response_pub.publish(response_msg)\n\n            # Reset for next request\n            self.current_command = None\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in vision-language processing: {e}\')\n\n    def simulate_vision_language_processing(self, image, command):\n        """Simulate vision-language processing"""\n        # This would contain the actual vision-language model inference\n        # For simulation, return a dummy response based on command keywords\n\n        command_lower = command.lower()\n\n        if \'red\' in command_lower:\n            return "I see a red object in the scene."\n        elif \'person\' in command_lower or \'human\' in command_lower:\n            return "I see a person in the scene."\n        elif \'find\' in command_lower or \'look\' in command_lower:\n            return "I\'m analyzing the scene to find what you\'re looking for."\n        else:\n            return f"Processing command: {command}"\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vl_node = VisionLanguageROSNode()\n\n    try:\n        rclpy.spin(vl_node)\n    except KeyboardInterrupt:\n        vl_node.get_logger().info(\'Shutting down Vision-Language ROS Node\')\n    finally:\n        vl_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"lab-deliverables",children:"Lab Deliverables"}),"\n",(0,t.jsx)(n.p,{children:"Complete the following tasks to finish the lab:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement the basic vision-language model"})," with image and text encoding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create image-text matching functionality"})," with contrastive learning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement simple image captioning"})," system"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Develop visual question answering"})," capability"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integrate components into a complete system"})," with multiple tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Test and validate"})," your implementations with sample data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Document your results"})," including:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Model architecture and design decisions"}),"\n",(0,t.jsx)(n.li,{children:"Performance metrics achieved"}),"\n",(0,t.jsx)(n.li,{children:"Challenges encountered and solutions"}),"\n",(0,t.jsx)(n.li,{children:"Suggestions for improvement"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,t.jsx)(n.p,{children:"Your lab implementation will be assessed based on:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Functionality"}),": Do all components work correctly?"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integration"}),": How well do the different components work together?"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance"}),": Are the models efficient and accurate?"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Code Quality"}),": Is the code well-structured and documented?"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Problem Solving"}),": How effectively did you implement the vision-language concepts?"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"extensions-optional",children:"Extensions (Optional)"}),"\n",(0,t.jsx)(n.p,{children:"For advanced students, consider implementing:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Attention mechanisms"})," for better vision-language alignment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Transformer-based models"})," for state-of-the-art performance"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multimodal pretraining"})," on larger datasets"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time processing"})," optimizations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Human evaluation"})," of generated captions or responses"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Memory errors during training:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Reduce batch size"}),"\n",(0,t.jsx)(n.li,{children:"Use gradient checkpointing"}),"\n",(0,t.jsxs)(n.li,{children:["Clear GPU cache: ",(0,t.jsx)(n.code,{children:"torch.cuda.empty_cache()"})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Poor matching performance:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Ensure proper normalization of embeddings"}),"\n",(0,t.jsx)(n.li,{children:"Try different loss functions (triplet loss, InfoNCE)"}),"\n",(0,t.jsx)(n.li,{children:"Increase training data diversity"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Caption generation issues:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Check vocabulary coverage"}),"\n",(0,t.jsx)(n.li,{children:"Implement beam search for better generation"}),"\n",(0,t.jsx)(n.li,{children:"Add length normalization"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Integration problems:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Verify data format compatibility"}),"\n",(0,t.jsx)(n.li,{children:"Check tensor dimensions throughout pipeline"}),"\n",(0,t.jsx)(n.li,{children:"Use consistent preprocessing across components"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This lab provided hands-on experience with fundamental vision-language integration techniques. You learned to connect visual perception with natural language understanding, creating systems that can ground language in visual contexts and respond intelligently to multimodal inputs. These capabilities are essential for advanced human-robot interaction systems that can understand and respond to natural language commands in real-world environments."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);