"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[2973],{4189(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3/edge-deployment","title":"Edge Deployment & Optimization","description":"Overview","source":"@site/docs/module-3/edge-deployment.md","sourceDirName":"module-3","slug":"/module-3/edge-deployment","permalink":"/hackathon/docs/module-3/edge-deployment","draft":false,"unlisted":false,"editUrl":"https://github.com/Fatimaali424/hackathon/edit/main/website/docs/module-3/edge-deployment.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Motion Planning & Trajectory Generation","permalink":"/hackathon/docs/module-3/motion-planning"},"next":{"title":"Lab 7: Basic Perception Pipeline with Isaac","permalink":"/hackathon/docs/module-3/lab-7-perception-pipeline"}}');var r=t(4848),o=t(8453);const a={sidebar_position:4},s="Edge Deployment & Optimization",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Jetson Platform Architecture",id:"jetson-platform-architecture",level:2},{value:"Jetson Hardware Overview",id:"jetson-hardware-overview",level:3},{value:"Jetson Orin Series",id:"jetson-orin-series",level:4},{value:"Key Hardware Components",id:"key-hardware-components",level:4},{value:"Compute Architecture for Robotics",id:"compute-architecture-for-robotics",level:3},{value:"Deployment Strategies",id:"deployment-strategies",level:2},{value:"Container-Based Deployment",id:"container-based-deployment",level:3},{value:"Isaac ROS Hardware Acceleration",id:"isaac-ros-hardware-acceleration",level:3},{value:"GPU-Accelerated Nodes",id:"gpu-accelerated-nodes",level:4},{value:"Hardware Interface Optimization",id:"hardware-interface-optimization",level:4},{value:"Performance Optimization Techniques",id:"performance-optimization-techniques",level:2},{value:"Model Optimization",id:"model-optimization",level:3},{value:"TensorRT Optimization",id:"tensorrt-optimization",level:4},{value:"Model Quantization",id:"model-quantization",level:4},{value:"Memory Management",id:"memory-management",level:3},{value:"Efficient Memory Allocation",id:"efficient-memory-allocation",level:4},{value:"Real-Time Performance",id:"real-time-performance",level:2},{value:"Real-Time Scheduling",id:"real-time-scheduling",level:3},{value:"Pipeline Optimization",id:"pipeline-optimization",level:3},{value:"Power and Thermal Management",id:"power-and-thermal-management",level:2},{value:"Power Optimization",id:"power-optimization",level:3},{value:"Thermal Management",id:"thermal-management",level:3},{value:"Deployment Validation",id:"deployment-validation",level:2},{value:"Performance Testing",id:"performance-testing",level:3},{value:"Resource Utilization Monitoring",id:"resource-utilization-monitoring",level:3},{value:"Deployment Best Practices",id:"deployment-best-practices",level:2},{value:"Configuration Management",id:"configuration-management",level:3},{value:"Deployment Scripts",id:"deployment-scripts",level:3},{value:"Troubleshooting and Debugging",id:"troubleshooting-and-debugging",level:2},{value:"Common Deployment Issues",id:"common-deployment-issues",level:3},{value:"Memory Issues",id:"memory-issues",level:4},{value:"Performance Issues",id:"performance-issues",level:4},{value:"Hardware Issues",id:"hardware-issues",level:4},{value:"Summary",id:"summary",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"edge-deployment--optimization",children:"Edge Deployment & Optimization"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"Edge deployment represents the critical step of moving AI-powered robotic systems from development environments to real-world applications. This chapter explores the deployment of NVIDIA Isaac applications on edge computing platforms, particularly the Jetson series, focusing on optimization techniques for real-time performance, power efficiency, and reliability in resource-constrained environments."}),"\n",(0,r.jsx)(n.p,{children:"Edge deployment requires careful consideration of computational constraints, power limitations, thermal management, and real-time performance requirements that characterize robotic applications."}),"\n",(0,r.jsx)(n.h2,{id:"jetson-platform-architecture",children:"Jetson Platform Architecture"}),"\n",(0,r.jsx)(n.h3,{id:"jetson-hardware-overview",children:"Jetson Hardware Overview"}),"\n",(0,r.jsx)(n.p,{children:"The NVIDIA Jetson platform provides AI computing capabilities optimized for edge robotics:"}),"\n",(0,r.jsx)(n.h4,{id:"jetson-orin-series",children:"Jetson Orin Series"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Jetson Orin AGX"}),": Up to 275 TOPS for AI inference"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Jetson Orin NX"}),": Up to 100 TOPS with smaller form factor"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Jetson Orin Nano"}),": Up to 40 TOPS for entry-level applications"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"key-hardware-components",children:"Key Hardware Components"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GPU"}),": NVIDIA Ampere architecture with Tensor Cores"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CPU"}),": ARM-based multi-core processors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"DLA"}),": Dedicated Deep Learning Accelerators"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VPU"}),": Video Processing Units for multimedia processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory"}),": LPDDR5 with high bandwidth for AI workloads"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"compute-architecture-for-robotics",children:"Compute Architecture for Robotics"}),"\n",(0,r.jsx)(n.p,{children:"The Jetson architecture is specifically designed for robotics applications:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    A[Sensor Inputs] --\x3e B{Data Preprocessing}\n    B --\x3e C[AI Inference]\n    C --\x3e D[Control Algorithms]\n    D --\x3e E[Actuator Outputs]\n    C --\x3e F[Perception Processing]\n    F --\x3e G[Path Planning]\n    G --\x3e D\n    B -.-> H[GPU Acceleration]\n    C -.-> H\n    F -.-> H\n    G -.-> H\n"})}),"\n",(0,r.jsx)(n.h2,{id:"deployment-strategies",children:"Deployment Strategies"}),"\n",(0,r.jsx)(n.h3,{id:"container-based-deployment",children:"Container-Based Deployment"}),"\n",(0,r.jsx)(n.p,{children:"Docker containers provide consistent deployment across development and production environments:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-dockerfile",children:'# Dockerfile for Isaac application on Jetson\nFROM nvcr.io/nvidia/isaac-ros:galactic-ros-base-l4t-r35.2.1\n\n# Set environment variables\nENV DEBIAN_FRONTEND=noninteractive\nENV NVIDIA_VISIBLE_DEVICES=all\nENV NVIDIA_DRIVER_CAPABILITIES=compute,utility\n\n# Install dependencies\nRUN apt-get update && apt-get install -y \\\n    python3-pip \\\n    python3-dev \\\n    build-essential \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Copy application code\nCOPY . /app\nWORKDIR /app\n\n# Install Python dependencies\nRUN pip3 install -r requirements.txt\n\n# Set up ROS workspace\nCOPY src /opt/ros_ws/src\nRUN cd /opt/ros_ws && colcon build --packages-select my_robot_app\n\n# Source ROS environment\nSHELL ["/bin/bash", "-c"]\nRUN echo "source /opt/ros_ws/install/setup.bash" >> ~/.bashrc\n\nCMD ["ros2", "launch", "my_robot_app", "deploy.launch.py"]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"isaac-ros-hardware-acceleration",children:"Isaac ROS Hardware Acceleration"}),"\n",(0,r.jsx)(n.p,{children:"Leverage hardware acceleration for optimal performance:"}),"\n",(0,r.jsx)(n.h4,{id:"gpu-accelerated-nodes",children:"GPU-Accelerated Nodes"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Isaac ROS GPU-accelerated node example\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport torch\nimport torch_tensorrt\n\nclass IsaacGPUPerceptionNode(Node):\n    def __init__(self):\n        super().__init__('isaac_gpu_perception')\n\n        # Initialize CUDA context\n        if torch.cuda.is_available():\n            self.device = torch.device('cuda')\n            self.get_logger().info('CUDA acceleration enabled')\n        else:\n            self.device = torch.device('cpu')\n            self.get_logger().warn('CUDA not available, using CPU')\n\n        # Load and optimize model with TensorRT\n        self.model = self.load_optimized_model()\n\n        # Initialize subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n    def load_optimized_model(self):\n        # Load pre-trained model\n        model = torch.hub.load('pytorch/vision:v0.10.0',\n                              'deeplabv3_resnet50',\n                              pretrained=True)\n        model.eval()\n\n        # Optimize with TensorRT\n        example_input = torch.randn(1, 3, 480, 640).to(self.device)\n\n        traced_model = torch.jit.trace(model, example_input)\n\n        optimized_model = torch_tensorrt.compile(\n            traced_model,\n            inputs=[example_input],\n            enabled_precisions={torch.float, torch.half},\n            workspace_size=1 << 20  # 1MB workspace\n        )\n\n        return optimized_model\n\n    def image_callback(self, msg):\n        try:\n            # Convert ROS image to tensor\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, 'bgr8')\n            tensor_image = self.preprocess_image(cv_image)\n\n            # Run inference on GPU\n            with torch.no_grad():\n                result = self.model(tensor_image)\n\n            # Process results\n            self.process_inference_result(result)\n\n        except Exception as e:\n            self.get_logger().error(f'Error in GPU processing: {e}')\n\n    def preprocess_image(self, image):\n        # Preprocess image for model input\n        image_tensor = torch.from_numpy(image).permute(2, 0, 1).float()\n        image_tensor = image_tensor.unsqueeze(0).to(self.device) / 255.0\n        return image_tensor\n\n    def process_inference_result(self, result):\n        # Process inference results\n        # This would typically publish results to other nodes\n        pass\n"})}),"\n",(0,r.jsx)(n.h4,{id:"hardware-interface-optimization",children:"Hardware Interface Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Optimized hardware interface\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, LaserScan\nfrom geometry_msgs.msg import Twist\nimport numpy as np\nimport threading\nfrom collections import deque\n\nclass OptimizedHardwareInterface(Node):\n    def __init__(self):\n        super().__init__('optimized_hardware_interface')\n\n        # Optimize data structures for real-time performance\n        self.imu_buffer = deque(maxlen=10)\n        self.scan_buffer = deque(maxlen=5)\n\n        # Hardware interfaces\n        self.imu_sub = self.create_subscription(Imu, '/imu/data',\n                                               self.imu_callback, 1)\n        self.scan_sub = self.create_subscription(LaserScan, '/scan',\n                                                self.scan_callback, 1)\n        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 1)\n\n        # Real-time timer for control loop\n        self.control_timer = self.create_timer(0.01, self.control_loop)  # 100Hz\n\n        # Lock for thread safety\n        self.data_lock = threading.Lock()\n\n        # Pre-allocated arrays for performance\n        self.processed_scan = np.empty(360, dtype=np.float32)\n\n    def imu_callback(self, msg):\n        with self.data_lock:\n            self.imu_buffer.append({\n                'orientation': [msg.orientation.x, msg.orientation.y,\n                               msg.orientation.z, msg.orientation.w],\n                'angular_velocity': [msg.angular_velocity.x,\n                                   msg.angular_velocity.y,\n                                   msg.angular_velocity.z],\n                'linear_acceleration': [msg.linear_acceleration.x,\n                                      msg.linear_acceleration.y,\n                                      msg.linear_acceleration.z]\n            })\n\n    def scan_callback(self, msg):\n        with self.data_lock:\n            # Efficiently copy scan data\n            self.processed_scan[:len(msg.ranges)] = msg.ranges\n            self.processed_scan[len(msg.ranges):] = np.inf\n\n    def control_loop(self):\n        with self.data_lock:\n            # Real-time control logic\n            cmd = self.compute_control_command()\n\n        if cmd is not None:\n            self.cmd_pub.publish(cmd)\n\n    def compute_control_command(self):\n        # Optimized control computation\n        # This would implement the actual control algorithm\n        cmd = Twist()\n        cmd.linear.x = 0.5  # Placeholder\n        cmd.angular.z = 0.1  # Placeholder\n        return cmd\n"})}),"\n",(0,r.jsx)(n.h2,{id:"performance-optimization-techniques",children:"Performance Optimization Techniques"}),"\n",(0,r.jsx)(n.h3,{id:"model-optimization",children:"Model Optimization"}),"\n",(0,r.jsx)(n.h4,{id:"tensorrt-optimization",children:"TensorRT Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nimport numpy as np\n\nclass TensorRTOptimizer:\n    def __init__(self):\n        self.logger = trt.Logger(trt.Logger.WARNING)\n        self.runtime = trt.Runtime(self.logger)\n\n    def optimize_model(self, onnx_model_path, output_path,\n                      input_shape, precision='fp16'):\n        \"\"\"\n        Optimize ONNX model for TensorRT deployment\n        \"\"\"\n        # Create builder and network\n        builder = trt.Builder(self.logger)\n        network = builder.create_network(\n            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n        )\n        parser = trt.OnnxParser(network, self.logger)\n\n        # Parse ONNX model\n        with open(onnx_model_path, 'rb') as model_file:\n            if not parser.parse(model_file.read()):\n                for error in range(parser.num_errors):\n                    print(parser.get_error(error))\n                return False\n\n        # Configure optimization\n        config = builder.create_builder_config()\n\n        # Set precision\n        if precision == 'fp16':\n            config.set_flag(trt.BuilderFlag.FP16)\n        elif precision == 'int8':\n            config.set_flag(trt.BuilderFlag.INT8)\n            # Add calibration for INT8\n\n        # Set memory limit\n        config.max_workspace_size = 1 << 30  # 1GB\n\n        # Build engine\n        serialized_engine = builder.build_serialized_network(network, config)\n\n        # Save optimized engine\n        with open(output_path, 'wb') as f:\n            f.write(serialized_engine)\n\n        return True\n\n    def create_inference_context(self, engine_path):\n        \"\"\"\n        Create inference context for optimized model\n        \"\"\"\n        with open(engine_path, 'rb') as f:\n            engine_data = f.read()\n\n        engine = self.runtime.deserialize_cuda_engine(engine_data)\n        context = engine.create_execution_context()\n\n        return engine, context\n"})}),"\n",(0,r.jsx)(n.h4,{id:"model-quantization",children:"Model Quantization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import torch\nimport torch.quantization as quantization\n\nclass ModelQuantizer:\n    def __init__(self):\n        self.quantized_model = None\n\n    def quantize_model(self, model, calibration_data_loader):\n        """\n        Quantize model for edge deployment\n        """\n        # Set model to evaluation mode\n        model.eval()\n\n        # Specify quantization configuration\n        model.qconfig = quantization.get_default_qconfig(\'fbgemm\')\n\n        # Prepare model for quantization\n        quantization.prepare(model, inplace=True)\n\n        # Calibrate the model\n        self.calibrate_model(model, calibration_data_loader)\n\n        # Convert to quantized model\n        quantized_model = quantization.convert(model, inplace=False)\n\n        return quantized_model\n\n    def calibrate_model(self, model, data_loader):\n        """\n        Calibrate model with sample data\n        """\n        model.eval()\n        with torch.no_grad():\n            for i, (data, _) in enumerate(data_loader):\n                if i >= 100:  # Use first 100 batches for calibration\n                    break\n                model(data)\n\n    def dynamic_quantization(self, model):\n        """\n        Apply dynamic quantization to model\n        """\n        quantized_model = quantization.quantize_dynamic(\n            model,\n            {torch.nn.Linear, torch.nn.LSTM},\n            dtype=torch.qint8\n        )\n        return quantized_model\n'})}),"\n",(0,r.jsx)(n.h3,{id:"memory-management",children:"Memory Management"}),"\n",(0,r.jsx)(n.h4,{id:"efficient-memory-allocation",children:"Efficient Memory Allocation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom numba import cuda\nimport threading\n\nclass MemoryManager:\n    def __init__(self, device_memory_size=1<<30):  # 1GB\n        self.device_memory_size = device_memory_size\n        self.memory_pool = {}\n        self.lock = threading.Lock()\n\n        # Initialize GPU context\n        self.gpu_initialized = cuda.is_available()\n\n    def allocate_buffer(self, shape, dtype, device=\'cpu\'):\n        """\n        Efficiently allocate memory buffer\n        """\n        key = (shape, dtype, device)\n\n        with self.lock:\n            if key in self.memory_pool:\n                # Reuse existing buffer\n                buffer = self.memory_pool[key].pop()\n                if len(self.memory_pool[key]) == 0:\n                    del self.memory_pool[key]\n                return buffer\n\n        # Create new buffer\n        if device == \'gpu\' and self.gpu_initialized:\n            return cuda.device_array(shape, dtype=dtype)\n        else:\n            return np.empty(shape, dtype=dtype)\n\n    def release_buffer(self, buffer, shape, dtype, device=\'cpu\'):\n        """\n        Release buffer back to pool\n        """\n        key = (shape, dtype, device)\n\n        with self.lock:\n            if key not in self.memory_pool:\n                self.memory_pool[key] = []\n            self.memory_pool[key].append(buffer)\n\n    def get_memory_stats(self):\n        """\n        Get memory usage statistics\n        """\n        stats = {\n            \'cpu_pools\': len(self.memory_pool),\n            \'gpu_available\': self.gpu_initialized\n        }\n        return stats\n'})}),"\n",(0,r.jsx)(n.h2,{id:"real-time-performance",children:"Real-Time Performance"}),"\n",(0,r.jsx)(n.h3,{id:"real-time-scheduling",children:"Real-Time Scheduling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile, DurabilityPolicy\nimport threading\nimport time\nimport os\n\nclass RealTimeNode(Node):\n    def __init__(self):\n        super().__init__(\'real_time_node\')\n\n        # Configure real-time parameters\n        self.configure_real_time()\n\n        # Create high-frequency timer\n        self.timer = self.create_timer(\n            0.005,  # 200Hz\n            self.real_time_callback,\n            clock=self.get_clock()\n        )\n\n        # Set up QoS for real-time performance\n        self.qos_profile = QoSProfile(\n            depth=1,\n            durability=DurabilityPolicy.VOLATILE\n        )\n\n    def configure_real_time(self):\n        """\n        Configure real-time scheduling\n        """\n        try:\n            # Set CPU affinity to dedicated core\n            os.sched_setaffinity(0, [0])  # Run on CPU 0\n\n            # Set real-time priority (requires root privileges)\n            import os\n            import ctypes\n            from ctypes import util\n\n            # Load libc\n            libc = ctypes.CDLL(util.find_library("c"))\n\n            # Set real-time scheduling (SCHED_FIFO)\n            param = ctypes.c_int(99)  # Max priority\n            result = libc.sched_setscheduler(\n                os.getpid(),\n                ctypes.c_int(1),  # SCHED_FIFO\n                ctypes.byref(param)\n            )\n\n            if result != 0:\n                self.get_logger().warn(\'Could not set real-time scheduling\')\n\n        except Exception as e:\n            self.get_logger().warn(f\'Real-time config failed: {e}\')\n\n    def real_time_callback(self):\n        """\n        Real-time critical callback\n        """\n        start_time = self.get_clock().now()\n\n        # Process time-critical tasks\n        self.process_critical_task()\n\n        # Calculate execution time\n        end_time = self.get_clock().now()\n        execution_time = (end_time - start_time).nanoseconds / 1e9\n\n        # Log timing for analysis\n        if execution_time > 0.004:  # 4ms threshold\n            self.get_logger().warn(\n                f\'Task exceeded timing budget: {execution_time:.3f}s\'\n            )\n'})}),"\n",(0,r.jsx)(n.h3,{id:"pipeline-optimization",children:"Pipeline Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport concurrent.futures\nfrom collections import deque\nimport threading\n\nclass OptimizedPipeline:\n    def __init__(self, num_threads=4):\n        self.num_threads = num_threads\n        self.thread_pool = concurrent.futures.ThreadPoolExecutor(\n            max_workers=num_threads\n        )\n        self.pipeline_stages = []\n        self.buffers = {}\n\n    def add_stage(self, func, buffer_size=10):\n        """\n        Add processing stage to pipeline\n        """\n        stage_id = len(self.pipeline_stages)\n        self.pipeline_stages.append({\n            \'function\': func,\n            \'buffer\': deque(maxlen=buffer_size),\n            \'stage_id\': stage_id\n        })\n\n    async def process_pipeline(self, input_data):\n        """\n        Process data through optimized pipeline\n        """\n        # Process through each stage\n        current_data = input_data\n\n        for stage in self.pipeline_stages:\n            # Submit to thread pool for parallel processing\n            future = self.thread_pool.submit(\n                stage[\'function\'],\n                current_data\n            )\n\n            # Wait for result\n            current_data = future.result()\n\n        return current_data\n\n    def start_streaming_pipeline(self, input_stream):\n        """\n        Start continuous streaming pipeline\n        """\n        async def stream_processor():\n            async for data in input_stream:\n                # Process asynchronously\n                result = await self.process_pipeline(data)\n\n                # Yield result\n                yield result\n\n        return stream_processor()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"power-and-thermal-management",children:"Power and Thermal Management"}),"\n",(0,r.jsx)(n.h3,{id:"power-optimization",children:"Power Optimization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import subprocess\nimport time\nimport threading\n\nclass PowerManager:\n    def __init__(self):\n        self.power_mode = \'balanced\'\n        self.monitoring = False\n        self.power_monitor_thread = None\n\n    def set_power_mode(self, mode):\n        """\n        Set Jetson power mode (nvpmodel)\n        """\n        modes = {\n            \'max_performance\': \'MAXN\',\n            \'balanced\': \'MODE_15W\',  # Or appropriate mode for device\n            \'power_efficient\': \'MODE_10W\'\n        }\n\n        if mode in modes:\n            try:\n                subprocess.run([\'nvpmodel\', \'-m\', modes[mode]], check=True)\n                self.power_mode = mode\n                return True\n            except subprocess.CalledProcessError:\n                return False\n        return False\n\n    def start_power_monitoring(self):\n        """\n        Start power consumption monitoring\n        """\n        self.monitoring = True\n        self.power_monitor_thread = threading.Thread(\n            target=self._monitor_power_consumption\n        )\n        self.power_monitor_thread.start()\n\n    def _monitor_power_consumption(self):\n        """\n        Monitor power consumption in background\n        """\n        while self.monitoring:\n            try:\n                # Read power consumption from Jetson files\n                with open(\'/sys/bus/i2c/devices/0-0040/hwmon/hwmon0/power1_input\', \'r\') as f:\n                    power_mw = int(f.read().strip())\n\n                # Log or take action based on power consumption\n                if power_mw > 15000:  # 15W threshold\n                    self.throttle_computation()\n\n            except Exception as e:\n                pass  # Handle gracefully\n\n            time.sleep(1)  # Monitor every second\n\n    def throttle_computation(self):\n        """\n        Reduce computation to save power\n        """\n        # Reduce processing frequency\n        # Lower model precision\n        # Skip non-critical processing\n        pass\n'})}),"\n",(0,r.jsx)(n.h3,{id:"thermal-management",children:"Thermal Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import subprocess\nimport threading\nimport time\n\nclass ThermalManager:\n    def __init__(self):\n        self.max_temp = 85.0  # Celsius\n        self.current_temp = 0.0\n        self.cooling_active = False\n\n    def get_jetson_temperature(self):\n        """\n        Get current Jetson temperature\n        """\n        try:\n            # Read from thermal zones\n            with open(\'/sys/class/thermal/thermal_zone0/temp\', \'r\') as f:\n                temp_mC = int(f.read().strip())  # Temperature in milli-Celsius\n                return temp_mC / 1000.0  # Convert to Celsius\n        except Exception:\n            return 0.0\n\n    def thermal_protection_loop(self):\n        """\n        Continuous thermal monitoring loop\n        """\n        while True:\n            current_temp = self.get_jetson_temperature()\n            self.current_temp = current_temp\n\n            if current_temp > self.max_temp:\n                self.trigger_thermal_protection()\n            elif current_temp < (self.max_temp - 5):  # Hysteresis\n                self.restore_normal_operation()\n\n            time.sleep(2)  # Check every 2 seconds\n\n    def trigger_thermal_protection(self):\n        """\n        Activate thermal protection measures\n        """\n        if not self.cooling_active:\n            self.cooling_active = True\n            # Reduce CPU/GPU frequency\n            # Throttle AI inference\n            # Reduce processing pipeline complexity\n            print("Thermal protection activated")\n\n    def restore_normal_operation(self):\n        """\n        Restore normal operation when temperature drops\n        """\n        if self.cooling_active:\n            self.cooling_active = False\n            # Restore normal frequencies\n            # Resume full processing\n            print("Normal operation restored")\n'})}),"\n",(0,r.jsx)(n.h2,{id:"deployment-validation",children:"Deployment Validation"}),"\n",(0,r.jsx)(n.h3,{id:"performance-testing",children:"Performance Testing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import time\nimport statistics\nfrom collections import deque\n\nclass PerformanceValidator:\n    def __init__(self, node):\n        self.node = node\n        self.metrics = {\n            'latency': deque(maxlen=1000),\n            'throughput': deque(maxlen=1000),\n            'memory_usage': deque(maxlen=1000),\n            'cpu_usage': deque(maxlen=1000)\n        }\n\n    def benchmark_inference(self, model, test_data, num_runs=100):\n        \"\"\"\n        Benchmark model inference performance\n        \"\"\"\n        latencies = []\n\n        for i in range(num_runs):\n            start_time = time.perf_counter()\n\n            # Run inference\n            with torch.no_grad():\n                result = model(test_data)\n\n            end_time = time.perf_counter()\n            latency = (end_time - start_time) * 1000  # ms\n            latencies.append(latency)\n\n        # Calculate statistics\n        stats = {\n            'mean_latency': statistics.mean(latencies),\n            'median_latency': statistics.median(latencies),\n            'std_latency': statistics.stdev(latencies) if len(latencies) > 1 else 0,\n            'min_latency': min(latencies),\n            'max_latency': max(latencies),\n            '95th_percentile': sorted(latencies)[int(0.95 * len(latencies))]\n        }\n\n        return stats\n\n    def validate_real_time_performance(self, duration=60):\n        \"\"\"\n        Validate real-time performance over extended period\n        \"\"\"\n        start_time = time.time()\n        cycle_times = []\n\n        while time.time() - start_time < duration:\n            cycle_start = time.perf_counter()\n\n            # Execute real-time task\n            self.execute_real_time_task()\n\n            cycle_end = time.perf_counter()\n            cycle_time = (cycle_end - cycle_start) * 1000  # ms\n            cycle_times.append(cycle_time)\n\n        # Analyze results\n        avg_cycle_time = statistics.mean(cycle_times)\n        max_cycle_time = max(cycle_times)\n        missed_deadlines = sum(1 for t in cycle_times if t > 10)  # >10ms deadline\n\n        return {\n            'avg_cycle_time': avg_cycle_time,\n            'max_cycle_time': max_cycle_time,\n            'missed_deadlines': missed_deadlines,\n            'deadline_miss_rate': missed_deadlines / len(cycle_times)\n        }\n\n    def execute_real_time_task(self):\n        \"\"\"\n        Execute a representative real-time task\n        \"\"\"\n        # Placeholder for actual task\n        time.sleep(0.001)  # Simulate 1ms task\n"})}),"\n",(0,r.jsx)(n.h3,{id:"resource-utilization-monitoring",children:"Resource Utilization Monitoring"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import psutil\nimport GPUtil\nimport threading\nimport time\nfrom collections import deque\n\nclass ResourceMonitor:\n    def __init__(self, interval=1.0):\n        self.interval = interval\n        self.monitoring = False\n        self.stats = {\n            'cpu_percent': deque(maxlen=300),  # 5 minutes at 1Hz\n            'memory_percent': deque(maxlen=300),\n            'gpu_percent': deque(maxlen=300),\n            'gpu_memory': deque(maxlen=300)\n        }\n\n    def start_monitoring(self):\n        \"\"\"\n        Start resource monitoring in background thread\n        \"\"\"\n        self.monitoring = True\n        self.monitor_thread = threading.Thread(target=self._monitor_loop)\n        self.monitor_thread.start()\n\n    def _monitor_loop(self):\n        \"\"\"\n        Background monitoring loop\n        \"\"\"\n        while self.monitoring:\n            # CPU usage\n            cpu_percent = psutil.cpu_percent(interval=0.1)\n\n            # Memory usage\n            memory_info = psutil.virtual_memory()\n            memory_percent = memory_info.percent\n\n            # GPU usage (if available)\n            gpus = GPUtil.getGPUs()\n            if gpus:\n                gpu = gpus[0]  # Primary GPU\n                gpu_percent = gpu.load * 100\n                gpu_memory = gpu.memoryUtil * 100\n            else:\n                gpu_percent = 0\n                gpu_memory = 0\n\n            # Store statistics\n            self.stats['cpu_percent'].append(cpu_percent)\n            self.stats['memory_percent'].append(memory_percent)\n            self.stats['gpu_percent'].append(gpu_percent)\n            self.stats['gpu_memory'].append(gpu_memory)\n\n            time.sleep(self.interval)\n\n    def get_current_stats(self):\n        \"\"\"\n        Get current resource utilization\n        \"\"\"\n        if not self.stats['cpu_percent']:\n            return {}\n\n        return {\n            'cpu_percent': self.stats['cpu_percent'][-1],\n            'memory_percent': self.stats['memory_percent'][-1],\n            'gpu_percent': self.stats['gpu_percent'][-1],\n            'gpu_memory': self.stats['gpu_memory'][-1],\n            'cpu_avg': sum(self.stats['cpu_percent']) / len(self.stats['cpu_percent']),\n            'memory_avg': sum(self.stats['memory_percent']) / len(self.stats['memory_percent'])\n        }\n"})}),"\n",(0,r.jsx)(n.h2,{id:"deployment-best-practices",children:"Deployment Best Practices"}),"\n",(0,r.jsx)(n.h3,{id:"configuration-management",children:"Configuration Management"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# deployment_config.yaml\ndeployment:\n  platform: "jetson-orin"\n  mode: "production"\n\noptimization:\n  precision: "fp16"\n  batch_size: 1\n  max_workspace_size: "1GB"\n\nreal_time:\n  control_frequency: 100\n  max_latency: 10  # ms\n  watchdog_timeout: 500  # ms\n\nresources:\n  cpu_affinity: [0, 1]\n  gpu_memory_fraction: 0.8\n  memory_limit: "2GB"\n\nmonitoring:\n  enabled: true\n  log_level: "INFO"\n  metrics_collection: true\n'})}),"\n",(0,r.jsx)(n.h3,{id:"deployment-scripts",children:"Deployment Scripts"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'#!/bin/bash\n# deploy_isaac_app.sh\n\nset -e  # Exit on error\n\necho "Starting Isaac application deployment..."\n\n# Check prerequisites\nif ! command -v docker &> /dev/null; then\n    echo "Docker is required but not installed"\n    exit 1\nfi\n\nif ! nvidia-smi &> /dev/null; then\n    echo "NVIDIA drivers not found"\n    exit 1\nfi\n\n# Set power mode for optimal performance\necho "Setting power mode..."\nsudo nvpmodel -m MAXN\n\n# Configure Jetson clocks\necho "Configuring clocks..."\nsudo jetson_clocks\n\n# Build Docker image\necho "Building Docker image..."\ndocker build -t isaac-robot-app:latest .\n\n# Run the application\necho "Starting application..."\ndocker run -it --rm \\\n    --gpus all \\\n    --privileged \\\n    --network host \\\n    --env NVIDIA_VISIBLE_DEVICES=all \\\n    --env NVIDIA_DRIVER_CAPABILITIES=compute,utility \\\n    --volume /tmp:/tmp \\\n    --device /dev:/dev \\\n    isaac-robot-app:latest\n\necho "Deployment completed!"\n'})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting-and-debugging",children:"Troubleshooting and Debugging"}),"\n",(0,r.jsx)(n.h3,{id:"common-deployment-issues",children:"Common Deployment Issues"}),"\n",(0,r.jsx)(n.h4,{id:"memory-issues",children:"Memory Issues"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Insufficient GPU memory"}),": Optimize model size or reduce batch size"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory fragmentation"}),": Restart application or use memory pools"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Memory leaks"}),": Monitor memory usage and fix resource management"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"High latency"}),": Profile code and optimize bottlenecks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Low throughput"}),": Increase parallelism or optimize algorithms"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Thermal throttling"}),": Improve cooling or reduce computational load"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"hardware-issues",children:"Hardware Issues"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor connectivity"}),": Check physical connections and drivers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Power delivery"}),": Verify adequate power supply for peak loads"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Thermal management"}),": Ensure proper cooling and ventilation"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Edge deployment optimization is crucial for successful robotic applications. The NVIDIA Jetson platform, combined with Isaac's optimization tools, provides the foundation for deploying sophisticated AI-powered robotic systems in real-world environments."}),"\n",(0,r.jsx)(n.p,{children:"Successful deployment requires careful attention to performance optimization, resource management, real-time constraints, and environmental factors. The techniques covered in this chapter provide a framework for deploying efficient, reliable robotic systems that can operate effectively in their intended environments."}),"\n",(0,r.jsx)(n.p,{children:"In the next chapters, we'll explore how these deployment strategies integrate with the broader robotic system architecture, including perception, planning, and control systems."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(m,{...e})}):m(e)}},8453(e,n,t){t.d(n,{R:()=>a,x:()=>s});var i=t(6540);const r={},o=i.createContext(r);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);