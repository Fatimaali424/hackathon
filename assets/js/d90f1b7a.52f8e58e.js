"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[2089],{7089(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4/assignment","title":"Module 4 Assignment: Vision-Language-Action System Implementation","description":"Overview","source":"@site/docs/module-4/assignment.md","sourceDirName":"module-4","slug":"/module-4/assignment","permalink":"/hackathon/docs/module-4/assignment","draft":false,"unlisted":false,"editUrl":"https://github.com/Fatimaali424/hackathon/edit/main/website/docs/module-4/assignment.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Lab 12: Complete VLA System Implementation","permalink":"/hackathon/docs/module-4/lab-12-vla-system"},"next":{"title":"Module 4: Hardware Specifications for Vision-Language-Action Systems","permalink":"/hackathon/docs/module-4/hardware"}}');var s=i(4848),a=i(8453);const o={sidebar_position:8},r="Module 4 Assignment: Vision-Language-Action System Implementation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Assignment Objectives",id:"assignment-objectives",level:2},{value:"Assignment Requirements",id:"assignment-requirements",level:2},{value:"Core Requirements",id:"core-requirements",level:3},{value:"Technical Specifications",id:"technical-specifications",level:3},{value:"System Architecture",id:"system-architecture",level:4},{value:"Performance Requirements",id:"performance-requirements",level:4},{value:"Hardware Targets",id:"hardware-targets",level:4},{value:"Implementation Guidelines",id:"implementation-guidelines",level:2},{value:"Phase 1: Vision Component Implementation",id:"phase-1-vision-component-implementation",level:3},{value:"Step 1: Visual Perception Pipeline",id:"step-1-visual-perception-pipeline",level:4},{value:"Step 2: Vision-Language Integration",id:"step-2-vision-language-integration",level:4},{value:"Phase 2: Language Component Implementation",id:"phase-2-language-component-implementation",level:3},{value:"Step 1: Natural Language Processing",id:"step-1-natural-language-processing",level:4},{value:"Phase 3: Action Component Implementation",id:"phase-3-action-component-implementation",level:3},{value:"Step 1: Action Generation and Planning",id:"step-1-action-generation-and-planning",level:4},{value:"Phase 4: Complete VLA System Integration",id:"phase-4-complete-vla-system-integration",level:3},{value:"Step 1: VLA System Architecture",id:"step-1-vla-system-architecture",level:4},{value:"Testing and Validation Plan",id:"testing-and-validation-plan",level:2},{value:"Simulation Testing",id:"simulation-testing",level:3},{value:"Hardware Validation",id:"hardware-validation",level:3},{value:"Documentation Requirements",id:"documentation-requirements",level:2},{value:"Technical Report (1500-2000 words)",id:"technical-report-1500-2000-words",level:3},{value:"Code Documentation",id:"code-documentation",level:3},{value:"Submission Requirements",id:"submission-requirements",level:2},{value:"Deliverables",id:"deliverables",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:3},{value:"Grading Rubric",id:"grading-rubric",level:3},{value:"Resources and References",id:"resources-and-references",level:2},{value:"Required Resources",id:"required-resources",level:3},{value:"Suggested Extensions",id:"suggested-extensions",level:3},{value:"Getting Started",id:"getting-started",level:2},{value:"Support and Questions",id:"support-and-questions",level:2},{value:"Deadline",id:"deadline",level:2},{value:"Academic Integrity",id:"academic-integrity",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"module-4-assignment-vision-language-action-system-implementation",children:"Module 4 Assignment: Vision-Language-Action System Implementation"})}),"\n",(0,s.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(e.p,{children:"This assignment challenges you to implement a complete Vision-Language-Action (VLA) system that integrates visual perception, natural language understanding, and robotic action execution. You will create a system capable of interpreting voice commands, processing visual information, and generating appropriate robot behaviors that combine perception and action in a unified framework."}),"\n",(0,s.jsx)(e.h2,{id:"assignment-objectives",children:"Assignment Objectives"}),"\n",(0,s.jsx)(e.p,{children:"By completing this assignment, you will demonstrate your ability to:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Design and implement a complete VLA system architecture"}),"\n",(0,s.jsx)(e.li,{children:"Integrate vision, language, and action components seamlessly"}),"\n",(0,s.jsx)(e.li,{children:"Optimize multimodal systems for real-time robotic applications"}),"\n",(0,s.jsx)(e.li,{children:"Evaluate and validate system performance across multiple modalities"}),"\n",(0,s.jsx)(e.li,{children:"Document and present technical implementations effectively"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"assignment-requirements",children:"Assignment Requirements"}),"\n",(0,s.jsx)(e.h3,{id:"core-requirements",children:"Core Requirements"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Vision Component (25 points)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Implement visual perception pipeline with object detection and scene understanding"}),"\n",(0,s.jsx)(e.li,{children:"Integrate with Isaac ROS vision nodes for GPU acceleration"}),"\n",(0,s.jsx)(e.li,{children:"Demonstrate real-time processing capabilities with appropriate latency"}),"\n",(0,s.jsx)(e.li,{children:"Include 3D scene understanding and spatial reasoning"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Language Component (25 points)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Implement natural language processing for command understanding"}),"\n",(0,s.jsx)(e.li,{children:"Integrate speech recognition and text processing capabilities"}),"\n",(0,s.jsx)(e.li,{children:"Demonstrate grounding of language in visual context"}),"\n",(0,s.jsx)(e.li,{children:"Handle command ambiguity and provide clarification when needed"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Action Component (25 points)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Implement action generation from multimodal inputs"}),"\n",(0,s.jsx)(e.li,{children:"Create planning and control systems for robot execution"}),"\n",(0,s.jsx)(e.li,{children:"Integrate with navigation and manipulation capabilities"}),"\n",(0,s.jsx)(e.li,{children:"Demonstrate safe and reliable action execution"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Integration and Coordination (25 points)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Create unified system architecture that combines all components"}),"\n",(0,s.jsx)(e.li,{children:"Implement cross-modal attention and fusion mechanisms"}),"\n",(0,s.jsx)(e.li,{children:"Demonstrate real-time coordination between modalities"}),"\n",(0,s.jsx)(e.li,{children:"Validate system performance in integrated scenarios"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"technical-specifications",children:"Technical Specifications"}),"\n",(0,s.jsx)(e.h4,{id:"system-architecture",children:"System Architecture"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Use ROS 2 Humble with Isaac ROS packages"}),"\n",(0,s.jsx)(e.li,{children:"Implement multimodal fusion using cross-attention mechanisms"}),"\n",(0,s.jsx)(e.li,{children:"Support both simulation and real hardware deployment"}),"\n",(0,s.jsx)(e.li,{children:"Include error handling and system recovery mechanisms"}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"performance-requirements",children:"Performance Requirements"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision Processing"}),": ",(0,s.jsx)(e.code,{children:"<50ms"})," latency for ",(0,s.jsx)(e.code,{children:"640x480"})," input"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language Processing"}),": ",(0,s.jsx)(e.code,{children:"<100ms"})," for command interpretation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action Planning"}),": ",(0,s.jsx)(e.code,{children:"<200ms"})," for action generation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"End-to-End Latency"}),": ",(0,s.jsx)(e.code,{children:"<500ms"})," for complete VLA pipeline"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Memory Usage"}),": ",(0,s.jsx)(e.code,{children:"<2GB"})," for main VLA application process"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"hardware-targets",children:"Hardware Targets"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Primary: NVIDIA Jetson Orin AGX"}),"\n",(0,s.jsx)(e.li,{children:"Secondary: NVIDIA Jetson Orin NX"}),"\n",(0,s.jsx)(e.li,{children:"Simulation: Isaac Sim or Gazebo for development and testing"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"implementation-guidelines",children:"Implementation Guidelines"}),"\n",(0,s.jsx)(e.h3,{id:"phase-1-vision-component-implementation",children:"Phase 1: Vision Component Implementation"}),"\n",(0,s.jsx)(e.h4,{id:"step-1-visual-perception-pipeline",children:"Step 1: Visual Perception Pipeline"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# vision_component.py\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom transformers import VisionEncoderDecoderModel\nimport numpy as np\n\nclass VisualPerceptionComponent:\n    def __init__(self, device='cuda'):\n        self.device = device\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n        # Initialize vision encoder (using pre-trained model)\n        self.vision_encoder = self._initialize_vision_encoder()\n        self.scene_understanding = SceneUnderstandingModule()\n        self.spatial_reasoning = SpatialReasoningModule()\n\n    def _initialize_vision_encoder(self):\n        \"\"\"Initialize vision encoder for feature extraction\"\"\"\n        import torchvision.models as models\n        encoder = models.resnet18(pretrained=True)\n        # Replace final layer for feature extraction\n        num_features = encoder.fc.in_features\n        encoder.fc = nn.Linear(num_features, 512)  # 512-dim features\n        return encoder.to(self.device)\n\n    def process_image(self, image):\n        \"\"\"Process image and extract visual features\"\"\"\n        # Transform image\n        transformed_image = self.transform(image).unsqueeze(0).to(self.device)\n\n        # Extract features\n        with torch.no_grad():\n            features = self.vision_encoder(transformed_image)\n\n        # Perform scene understanding\n        scene_analysis = self.scene_understanding.analyze(features, image)\n\n        # Perform spatial reasoning\n        spatial_context = self.spatial_reasoning.reason(scene_analysis)\n\n        return {\n            'features': features,\n            'scene_analysis': scene_analysis,\n            'spatial_context': spatial_context,\n            'objects': scene_analysis.get('objects', []),\n            'relations': scene_analysis.get('spatial_relations', [])\n        }\n\nclass SceneUnderstandingModule:\n    def analyze(self, features, image):\n        \"\"\"Analyze scene content and relationships\"\"\"\n        # This would typically use object detection and segmentation\n        # For this assignment, implement simplified analysis\n        return {\n            'objects': [{'name': 'object', 'bbox': [0.1, 0.1, 0.8, 0.8], 'confidence': 0.9}],\n            'spatial_relations': [{'relation': 'left_of', 'object1': 'object', 'object2': 'reference'}],\n            'scene_type': 'indoor',\n            'dominant_colors': ['red', 'blue']\n        }\n\nclass SpatialReasoningModule:\n    def reason(self, scene_analysis):\n        \"\"\"Perform spatial reasoning based on scene analysis\"\"\"\n        # Implement spatial relationship reasoning\n        spatial_context = {\n            'relative_positions': self._extract_relative_positions(scene_analysis),\n            'reachable_areas': self._identify_reachable_areas(scene_analysis),\n            'navigation_paths': self._plan_navigation_paths(scene_analysis)\n        }\n        return spatial_context\n\n    def _extract_relative_positions(self, scene_analysis):\n        \"\"\"Extract relative positions of objects\"\"\"\n        # Implementation for relative position extraction\n        return {'object_positions': {}}\n\n    def _identify_reachable_areas(self, scene_analysis):\n        \"\"\"Identify areas reachable by robot\"\"\"\n        # Implementation for reachability analysis\n        return {'reachable_areas': []}\n\n    def _plan_navigation_paths(self, scene_analysis):\n        \"\"\"Plan navigation paths based on scene\"\"\"\n        # Implementation for path planning\n        return {'paths': []}\n"})}),"\n",(0,s.jsx)(e.h4,{id:"step-2-vision-language-integration",children:"Step 2: Vision-Language Integration"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# vision_language_fusion.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass VisionLanguageFusion:\n    def __init__(self, vision_dim=512, language_dim=512, fusion_dim=512):\n        self.vision_dim = vision_dim\n        self.language_dim = language_dim\n        self.fusion_dim = fusion_dim\n\n        # Vision-language attention mechanism\n        self.vision_attention = CrossModalAttention(vision_dim, language_dim)\n        self.language_attention = CrossModalAttention(language_dim, vision_dim)\n\n        # Fusion network\n        self.fusion_network = nn.Sequential(\n            nn.Linear(vision_dim + language_dim, fusion_dim),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(fusion_dim, fusion_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(fusion_dim, fusion_dim)\n        )\n\n        # Output heads for different tasks\n        self.object_grounding_head = nn.Linear(fusion_dim, vision_dim)\n        self.action_prediction_head = nn.Linear(fusion_dim, 128)  # 128-dim action space\n\n    def forward(self, vision_features, language_features):\n        """Fuse vision and language features"""\n        # Apply cross-modal attention\n        attended_vision = self.vision_attention(vision_features, language_features)\n        attended_language = self.language_attention(language_features, vision_features)\n\n        # Concatenate and fuse\n        combined_features = torch.cat([attended_vision, attended_language], dim=-1)\n        fused_features = self.fusion_network(combined_features)\n\n        # Generate outputs\n        object_grounding = self.object_grounding_head(fused_features)\n        action_predictions = self.action_prediction_head(fused_features)\n\n        return {\n            \'fused_features\': fused_features,\n            \'object_grounding\': object_grounding,\n            \'action_predictions\': action_predictions,\n            \'attention_weights\': {\n                \'vision_to_language\': self.vision_attention.get_attention_weights(),\n                \'language_to_vision\': self.language_attention.get_attention_weights()\n            }\n        }\n\nclass CrossModalAttention(nn.Module):\n    def __init__(self, query_dim, key_value_dim):\n        super().__init__()\n        self.query_dim = query_dim\n        self.key_value_dim = key_value_dim\n\n        self.query_proj = nn.Linear(query_dim, query_dim)\n        self.key_proj = nn.Linear(key_value_dim, query_dim)\n        self.value_proj = nn.Linear(key_value_dim, query_dim)\n\n        self.scale = query_dim ** -0.5\n        self.attention_weights = None\n\n    def forward(self, query, key_value):\n        """Apply cross-modal attention"""\n        Q = self.query_proj(query)\n        K = self.key_proj(key_value)\n        V = self.value_proj(key_value)\n\n        # Calculate attention scores\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n        attention_weights = F.softmax(attention_scores, dim=-1)\n\n        # Apply attention\n        attended_output = torch.matmul(attention_weights, V)\n\n        # Store weights for later access\n        self.attention_weights = attention_weights\n\n        return attended_output\n\n    def get_attention_weights(self):\n        """Get the last computed attention weights"""\n        return self.attention_weights\n'})}),"\n",(0,s.jsx)(e.h3,{id:"phase-2-language-component-implementation",children:"Phase 2: Language Component Implementation"}),"\n",(0,s.jsx)(e.h4,{id:"step-1-natural-language-processing",children:"Step 1: Natural Language Processing"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# language_component.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\n\nclass LanguageProcessingComponent:\n    def __init__(self, model_name='bert-base-uncased', device='cuda'):\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.language_model = AutoModel.from_pretrained(model_name)\n        self.language_model.to(device)\n\n        # Intent classification head\n        self.intent_classifier = nn.Linear(self.language_model.config.hidden_size, 10)  # 10 intent classes\n\n        # Entity recognition head\n        self.entity_recognizer = nn.Linear(self.language_model.config.hidden_size, 50)  # 50 entity types\n\n    def process_command(self, command_text):\n        \"\"\"Process natural language command\"\"\"\n        # Tokenize input\n        inputs = self.tokenizer(\n            command_text,\n            return_tensors='pt',\n            padding=True,\n            truncation=True,\n            max_length=128\n        ).to(self.device)\n\n        # Get language features\n        with torch.no_grad():\n            outputs = self.language_model(**inputs)\n            sequence_output = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n            pooled_output = outputs.pooler_output  # [batch, hidden_size]\n\n        # Classify intent\n        intent_logits = self.intent_classifier(pooled_output)\n        intent_probs = F.softmax(intent_logits, dim=-1)\n        predicted_intent = torch.argmax(intent_logits, dim=-1)\n\n        # Recognize entities\n        entity_logits = self.entity_recognizer(sequence_output)\n        entity_probs = F.softmax(entity_logits, dim=-1)\n        predicted_entities = torch.argmax(entity_logits, dim=-2)\n\n        return {\n            'command_text': command_text,\n            'language_features': pooled_output,\n            'sequence_features': sequence_output,\n            'intent': predicted_intent.item(),\n            'intent_probabilities': intent_probs.squeeze().tolist(),\n            'entities': predicted_entities.squeeze().tolist(),\n            'entity_probabilities': entity_probs.squeeze().tolist(),\n            'tokenized_input': inputs\n        }\n\n    def ground_command_in_context(self, command_text, visual_context):\n        \"\"\"Ground language command in visual context\"\"\"\n        # Process command\n        lang_result = self.process_command(command_text)\n\n        # Extract relevant information based on visual context\n        grounded_result = self._ground_in_visual_context(lang_result, visual_context)\n\n        return grounded_result\n\n    def _ground_in_visual_context(self, language_result, visual_context):\n        \"\"\"Ground language in visual context\"\"\"\n        # This would implement grounding algorithms\n        # For this assignment, return a simplified grounding\n        return {\n            'command': language_result['command_text'],\n            'intent': language_result['intent'],\n            'relevant_objects': visual_context.get('objects', []),\n            'spatial_relations': visual_context.get('spatial_relations', []),\n            'grounded_entities': self._match_entities_to_objects(\n                language_result['entities'],\n                visual_context.get('objects', [])\n            )\n        }\n\n    def _match_entities_to_objects(self, entities, objects):\n        \"\"\"Match recognized entities to detected objects\"\"\"\n        # Simplified entity-object matching\n        matched = []\n        for entity_idx in entities[:5]:  # Limit to first 5 entities\n            if entity_idx < len(objects):\n                matched.append({\n                    'entity_type': f'entity_{entity_idx}',\n                    'matched_object': objects[entity_idx] if entity_idx < len(objects) else None\n                })\n        return matched\n"})}),"\n",(0,s.jsx)(e.h3,{id:"phase-3-action-component-implementation",children:"Phase 3: Action Component Implementation"}),"\n",(0,s.jsx)(e.h4,{id:"step-1-action-generation-and-planning",children:"Step 1: Action Generation and Planning"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# action_component.py\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Dict, List, Any\n\nclass ActionGenerationComponent:\n    def __init__(self, action_space_dim=6, device='cuda'):\n        self.device = device\n        self.action_space_dim = action_space_dim\n\n        # Action generator network\n        self.action_generator = nn.Sequential(\n            nn.Linear(1024, 512),  # Input: fused features from vision-language\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, action_space_dim)\n        )\n\n        # Action refinement network\n        self.action_refiner = ActionRefinementNetwork(action_space_dim)\n\n        # Safety checker\n        self.safety_checker = SafetyChecker()\n\n    def generate_action(self, fused_features, language_context, visual_context):\n        \"\"\"Generate action from multimodal inputs\"\"\"\n        # Generate raw action\n        raw_action = self.action_generator(fused_features)\n\n        # Refine action based on context\n        refined_action = self.action_refiner.refine(\n            raw_action, language_context, visual_context\n        )\n\n        # Check safety\n        is_safe, safety_report = self.safety_checker.check_action(\n            refined_action, visual_context\n        )\n\n        if not is_safe:\n            # Generate safe alternative action\n            safe_action = self._generate_safe_alternative(refined_action, visual_context)\n            refined_action = safe_action\n\n        return {\n            'raw_action': raw_action,\n            'refined_action': refined_action,\n            'is_safe': is_safe,\n            'safety_report': safety_report,\n            'confidence': self._calculate_action_confidence(refined_action)\n        }\n\n    def _generate_safe_alternative(self, action, visual_context):\n        \"\"\"Generate safe alternative action\"\"\"\n        # Implement safety fallback\n        safe_action = torch.zeros_like(action)\n        return safe_action\n\n    def _calculate_action_confidence(self, action):\n        \"\"\"Calculate confidence in action\"\"\"\n        # Simple confidence based on action magnitude\n        action_magnitude = torch.norm(action, p=2).item()\n        confidence = max(0.0, min(1.0, 1.0 - action_magnitude / 10.0))  # Normalize\n        return confidence\n\nclass ActionRefinementNetwork(nn.Module):\n    def __init__(self, action_dim):\n        super().__init__()\n        self.action_dim = action_dim\n\n        # Refinement network\n        self.refinement_net = nn.Sequential(\n            nn.Linear(action_dim + 64, 128),  # +64 for context features\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_dim)\n        )\n\n    def refine(self, raw_action, language_context, visual_context):\n        \"\"\"Refine action based on contextual information\"\"\"\n        # Extract context features\n        context_features = self._extract_context_features(\n            language_context, visual_context\n        )\n\n        # Combine raw action with context\n        combined_input = torch.cat([raw_action, context_features], dim=-1)\n\n        # Refine action\n        refinement_delta = self.refinement_net(combined_input)\n        refined_action = raw_action + refinement_delta\n\n        return refined_action\n\n    def _extract_context_features(self, language_context, visual_context):\n        \"\"\"Extract numerical features from context\"\"\"\n        # This would extract features from context dictionaries\n        # For this assignment, return simplified features\n        features = torch.zeros(64)\n        return features\n\nclass SafetyChecker:\n    def __init__(self):\n        self.safe_action_ranges = {\n            'linear_velocity': (-1.0, 1.0),\n            'angular_velocity': (-1.0, 1.0),\n            'gripper_position': (0.0, 1.0)\n        }\n\n    def check_action(self, action, visual_context):\n        \"\"\"Check if action is safe\"\"\"\n        action_np = action.detach().cpu().numpy()\n\n        # Check basic action ranges\n        is_safe = True\n        safety_report = {\n            'violations': [],\n            'warnings': [],\n            'action_magnitude': float(np.linalg.norm(action_np))\n        }\n\n        # Check for obvious violations\n        if np.any(np.isnan(action_np)) or np.any(np.isinf(action_np)):\n            is_safe = False\n            safety_report['violations'].append('Invalid action values (NaN/Inf)')\n\n        # Check for excessive magnitudes\n        if safety_report['action_magnitude'] > 5.0:\n            safety_report['warnings'].append('High action magnitude')\n\n        # Check individual components (simplified)\n        if len(action_np) >= 2:\n            if abs(action_np[0]) > 2.0:  # Linear velocity\n                safety_report['warnings'].append('High linear velocity')\n            if abs(action_np[1]) > 2.0:  # Angular velocity\n                safety_report['warnings'].append('High angular velocity')\n\n        # Check for obstacles in visual context\n        obstacles = visual_context.get('objects', [])\n        for obj in obstacles:\n            if obj.get('distance', float('inf')) < 0.5:  # Too close\n                safety_report['warnings'].append(f'Obstacle at {obj.get(\"distance\", 0):.2f}m')\n\n        return is_safe, safety_report\n"})}),"\n",(0,s.jsx)(e.h3,{id:"phase-4-complete-vla-system-integration",children:"Phase 4: Complete VLA System Integration"}),"\n",(0,s.jsx)(e.h4,{id:"step-1-vla-system-architecture",children:"Step 1: VLA System Architecture"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# vla_system.py\n\nimport torch\nimport time\nfrom typing import Dict, Any\nimport threading\nimport queue\n\nclass VLASystem:\n    def __init__(self, device='cuda'):\n        self.device = device\n        self.vision_component = VisualPerceptionComponent(device)\n        self.language_component = LanguageProcessingComponent(device=device)\n        self.action_component = ActionGenerationComponent(device=device)\n        self.vision_language_fusion = VisionLanguageFusion()\n\n        # System state\n        self.current_visual_context = None\n        self.current_language_context = None\n        self.system_ready = False\n\n        # Performance tracking\n        self.performance_stats = {\n            'vision_time': [],\n            'language_time': [],\n            'fusion_time': [],\n            'action_time': [],\n            'total_time': []\n        }\n\n    def process_vla_input(self, image, command_text) -> Dict[str, Any]:\n        \"\"\"Process complete VLA input and generate action\"\"\"\n        start_time = time.time()\n\n        # Process vision component\n        vision_start = time.time()\n        vision_result = self.vision_component.process_image(image)\n        vision_time = time.time() - vision_start\n\n        # Process language component\n        language_start = time.time()\n        language_result = self.language_component.ground_command_in_context(\n            command_text, vision_result\n        )\n        language_time = time.time() - language_start\n\n        # Fuse vision and language\n        fusion_start = time.time()\n        fused_features = self._fuse_vision_language(\n            vision_result['features'],\n            language_result['language_features']\n        )\n        fusion_time = time.time() - fusion_start\n\n        # Generate action\n        action_start = time.time()\n        action_result = self.action_component.generate_action(\n            fused_features['fused_features'],\n            language_result,\n            vision_result\n        )\n        action_time = time.time() - action_start\n\n        total_time = time.time() - start_time\n\n        # Update performance stats\n        self._update_performance_stats(\n            vision_time, language_time, fusion_time, action_time, total_time\n        )\n\n        return {\n            'vision_result': vision_result,\n            'language_result': language_result,\n            'fused_features': fused_features,\n            'action_result': action_result,\n            'processing_times': {\n                'vision': vision_time,\n                'language': language_time,\n                'fusion': fusion_time,\n                'action': action_time,\n                'total': total_time\n            },\n            'system_health': self._check_system_health()\n        }\n\n    def _fuse_vision_language(self, vision_features, language_features):\n        \"\"\"Fuse vision and language features\"\"\"\n        fusion_result = self.vision_language_fusion(\n            vision_features, language_features\n        )\n        return fusion_result\n\n    def _update_performance_stats(self, vision_t, lang_t, fusion_t, action_t, total_t):\n        \"\"\"Update performance statistics\"\"\"\n        self.performance_stats['vision_time'].append(vision_t)\n        self.performance_stats['language_time'].append(lang_t)\n        self.performance_stats['fusion_time'].append(fusion_t)\n        self.performance_stats['action_time'].append(action_t)\n        self.performance_stats['total_time'].append(total_t)\n\n        # Keep only recent stats (last 100 measurements)\n        max_len = 100\n        for key in self.performance_stats:\n            if len(self.performance_stats[key]) > max_len:\n                self.performance_stats[key] = self.performance_stats[key][-max_len:]\n\n    def _check_system_health(self):\n        \"\"\"Check system health and performance\"\"\"\n        if not self.performance_stats['total_time']:\n            return {'healthy': True, 'avg_latency_ms': 0}\n\n        avg_total_time = sum(self.performance_stats['total_time']) / len(self.performance_stats['total_time'])\n        avg_latency_ms = avg_total_time * 1000\n\n        healthy = avg_latency_ms < 500  # 500ms threshold\n\n        return {\n            'healthy': healthy,\n            'avg_latency_ms': avg_latency_ms,\n            'total_measurements': len(self.performance_stats['total_time'])\n        }\n\n    def get_performance_summary(self):\n        \"\"\"Get performance summary statistics\"\"\"\n        if not self.performance_stats['total_time']:\n            return \"No performance data collected\"\n\n        summary = {}\n        for component, times in self.performance_stats.items():\n            if times:\n                avg_time = sum(times) / len(times) * 1000  # Convert to ms\n                std_time = (sum((t - sum(times)/len(times))**2 for t in times) / len(times))**0.5 * 1000\n                summary[component] = {\n                    'avg_ms': round(avg_time, 2),\n                    'std_ms': round(std_time, 2),\n                    'count': len(times)\n                }\n\n        return summary\n"})}),"\n",(0,s.jsx)(e.h2,{id:"testing-and-validation-plan",children:"Testing and Validation Plan"}),"\n",(0,s.jsx)(e.h3,{id:"simulation-testing",children:"Simulation Testing"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Vision Component Testing:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Test object detection accuracy in various lighting conditions"}),"\n",(0,s.jsx)(e.li,{children:"Validate 3D scene understanding against ground truth"}),"\n",(0,s.jsx)(e.li,{children:"Measure processing latency and throughput"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Language Component Testing:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Test command understanding accuracy"}),"\n",(0,s.jsx)(e.li,{children:"Validate grounding in visual context"}),"\n",(0,s.jsx)(e.li,{children:"Measure parsing speed and accuracy"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Action Component Testing:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Test action generation from multimodal inputs"}),"\n",(0,s.jsx)(e.li,{children:"Validate safety checking mechanisms"}),"\n",(0,s.jsx)(e.li,{children:"Measure action refinement effectiveness"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Integration Testing:"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Test complete VLA pipeline end-to-end"}),"\n",(0,s.jsx)(e.li,{children:"Validate system behavior in complex scenarios"}),"\n",(0,s.jsx)(e.li,{children:"Measure overall system latency and performance"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"hardware-validation",children:"Hardware Validation"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Deploy to Jetson hardware and validate real-time performance"}),"\n",(0,s.jsx)(e.li,{children:"Test with actual robot hardware for real-world scenarios"}),"\n",(0,s.jsx)(e.li,{children:"Measure power consumption and thermal characteristics"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"documentation-requirements",children:"Documentation Requirements"}),"\n",(0,s.jsx)(e.h3,{id:"technical-report-1500-2000-words",children:"Technical Report (1500-2000 words)"}),"\n",(0,s.jsx)(e.p,{children:"Your report should include:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"System Architecture (300-400 words)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"High-level system design with multimodal integration"}),"\n",(0,s.jsx)(e.li,{children:"Component interactions and data flow"}),"\n",(0,s.jsx)(e.li,{children:"Technology choices and rationale"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Implementation Details (600-800 words)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Key algorithms and approaches used"}),"\n",(0,s.jsx)(e.li,{children:"Cross-modal attention and fusion mechanisms"}),"\n",(0,s.jsx)(e.li,{children:"Optimization techniques implemented"}),"\n",(0,s.jsx)(e.li,{children:"Challenges encountered and solutions"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Performance Evaluation (400-500 words)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Quantitative results with metrics"}),"\n",(0,s.jsx)(e.li,{children:"Comparison with baseline approaches"}),"\n",(0,s.jsx)(e.li,{children:"Analysis of bottlenecks and improvements"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Results and Analysis (200-300 words)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Key findings from testing"}),"\n",(0,s.jsx)(e.li,{children:"System limitations and future improvements"}),"\n",(0,s.jsx)(e.li,{children:"Recommendations for similar projects"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"code-documentation",children:"Code Documentation"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Include comprehensive code comments"}),"\n",(0,s.jsx)(e.li,{children:"Provide README files for each major component"}),"\n",(0,s.jsx)(e.li,{children:"Document API interfaces and usage examples"}),"\n",(0,s.jsx)(e.li,{children:"Include configuration files and deployment instructions"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"submission-requirements",children:"Submission Requirements"}),"\n",(0,s.jsx)(e.h3,{id:"deliverables",children:"Deliverables"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Source Code (ZIP file)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Complete ROS 2 package with all VLA components"}),"\n",(0,s.jsx)(e.li,{children:"Dockerfiles for containerization"}),"\n",(0,s.jsx)(e.li,{children:"Configuration files and launch scripts"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Technical Report (PDF)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"As described above"}),"\n",(0,s.jsx)(e.li,{children:"Include screenshots, diagrams, and performance graphs"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Video Demonstration (Optional but recommended)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"3-5 minute video showing system operation"}),"\n",(0,s.jsx)(e.li,{children:"Highlight key features and capabilities"}),"\n",(0,s.jsx)(e.li,{children:"Show both simulation and hardware results if available"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(e.li,{children:["\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Performance Results (CSV/JSON)"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Quantitative metrics from testing"}),"\n",(0,s.jsx)(e.li,{children:"Benchmark results for optimization"}),"\n",(0,s.jsx)(e.li,{children:"Resource utilization data"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,s.jsxs)(e.table,{children:[(0,s.jsx)(e.thead,{children:(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.th,{children:"Component"}),(0,s.jsx)(e.th,{children:"Points"}),(0,s.jsx)(e.th,{children:"Description"})]})}),(0,s.jsxs)(e.tbody,{children:[(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Vision Component"}),(0,s.jsx)(e.td,{children:"25"}),(0,s.jsx)(e.td,{children:"Object detection, scene understanding, real-time performance"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Language Component"}),(0,s.jsx)(e.td,{children:"25"}),(0,s.jsx)(e.td,{children:"Command understanding, grounding, processing accuracy"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Action Component"}),(0,s.jsx)(e.td,{children:"25"}),(0,s.jsx)(e.td,{children:"Action generation, planning, safety mechanisms"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:"Integration & Coordination"}),(0,s.jsx)(e.td,{children:"25"}),(0,s.jsx)(e.td,{children:"Cross-modal fusion, system architecture, validation"})]}),(0,s.jsxs)(e.tr,{children:[(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"Total"})}),(0,s.jsx)(e.td,{children:(0,s.jsx)(e.strong,{children:"100"})}),(0,s.jsx)(e.td,{})]})]})]}),"\n",(0,s.jsx)(e.h3,{id:"grading-rubric",children:"Grading Rubric"}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Excellent (90-100 points):"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"All requirements fully implemented with sophisticated approaches"}),"\n",(0,s.jsx)(e.li,{children:"Advanced multimodal fusion and attention mechanisms"}),"\n",(0,s.jsx)(e.li,{children:"Comprehensive testing and validation"}),"\n",(0,s.jsx)(e.li,{children:"Professional documentation and code quality"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Good (80-89 points):"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"All requirements implemented with solid approaches"}),"\n",(0,s.jsx)(e.li,{children:"Good multimodal integration with basic fusion"}),"\n",(0,s.jsx)(e.li,{children:"Adequate testing and validation"}),"\n",(0,s.jsx)(e.li,{children:"Good documentation and code quality"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Satisfactory (70-79 points):"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Core requirements implemented with basic approaches"}),"\n",(0,s.jsx)(e.li,{children:"Basic multimodal integration"}),"\n",(0,s.jsx)(e.li,{children:"Limited testing and validation"}),"\n",(0,s.jsx)(e.li,{children:"Adequate documentation"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:(0,s.jsx)(e.strong,{children:"Needs Improvement (Below 70 points):"})}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Missing significant requirements"}),"\n",(0,s.jsx)(e.li,{children:"Poor multimodal integration"}),"\n",(0,s.jsx)(e.li,{children:"Inadequate testing or documentation"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"resources-and-references",children:"Resources and References"}),"\n",(0,s.jsx)(e.h3,{id:"required-resources",children:"Required Resources"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"NVIDIA Isaac ROS documentation"}),"\n",(0,s.jsx)(e.li,{children:"ROS 2 Humble tutorials"}),"\n",(0,s.jsx)(e.li,{children:"Transformers library documentation"}),"\n",(0,s.jsx)(e.li,{children:"Jetson platform documentation"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"suggested-extensions",children:"Suggested Extensions"}),"\n",(0,s.jsx)(e.p,{children:"For advanced students, consider implementing:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Learning-based VLA components"}),"\n",(0,s.jsx)(e.li,{children:"Multi-robot coordination"}),"\n",(0,s.jsx)(e.li,{children:"Advanced optimization techniques"}),"\n",(0,s.jsx)(e.li,{children:"Edge-cloud hybrid architectures"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Set up development environment"})," with Isaac ROS and dependencies"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Create project structure"})," following ROS 2 conventions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Implement vision component"})," first, test in isolation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Add language processing"})," component with grounding"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Create action generation"})," system with safety checks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Integrate components"})," into complete VLA system"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Optimize and validate"})," system performance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Document and test"})," thoroughly before submission"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"support-and-questions",children:"Support and Questions"}),"\n",(0,s.jsx)(e.p,{children:"For technical questions about this assignment:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Refer to the Module 4 content and examples"}),"\n",(0,s.jsx)(e.li,{children:"Use the Isaac ROS and ROS 2 documentation"}),"\n",(0,s.jsx)(e.li,{children:"Consult with peers and instructors during lab sessions"}),"\n",(0,s.jsx)(e.li,{children:"Post questions in the course discussion forum"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"deadline",children:"Deadline"}),"\n",(0,s.jsx)(e.p,{children:"This assignment is due at the end of Week 12. Submit all components through the course management system by 11:59 PM on the due date."}),"\n",(0,s.jsx)(e.h2,{id:"academic-integrity",children:"Academic Integrity"}),"\n",(0,s.jsx)(e.p,{children:"This assignment must be completed individually. You may:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Use provided course materials and examples as references"}),"\n",(0,s.jsx)(e.li,{children:"Discuss concepts and approaches with classmates"}),"\n",(0,s.jsx)(e.li,{children:"Seek help from instructors and teaching assistants"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"You may NOT:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Share code directly with other students"}),"\n",(0,s.jsx)(e.li,{children:"Copy solutions from external sources"}),"\n",(0,s.jsx)(e.li,{children:"Submit work that is not your own"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Remember to cite any external resources or references used in your implementation."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>o,x:()=>r});var t=i(6540);const s={},a=t.createContext(s);function o(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);