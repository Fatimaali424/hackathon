"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[978],{5941(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-2/lab-5-sensor-simulation","title":"Lab 5: Advanced Sensor Simulation","description":"Overview","source":"@site/docs/module-2/lab-5-sensor-simulation.md","sourceDirName":"module-2","slug":"/module-2/lab-5-sensor-simulation","permalink":"/hackathon/docs/module-2/lab-5-sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/Fatimaali424/hackathon/edit/main/website/docs/module-2/lab-5-sensor-simulation.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"Lab 4: Basic Robot Model and Simulation","permalink":"/hackathon/docs/module-2/lab-4-robot-model"},"next":{"title":"Lab 6: Unity Integration with Simulation","permalink":"/hackathon/docs/module-2/lab-6-unity-integration"}}');var a=i(4848),o=i(8453);const t={sidebar_position:8},r="Lab 5: Advanced Sensor Simulation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Hardware/Software Requirements",id:"hardwaresoftware-requirements",level:2},{value:"Lab Duration",id:"lab-duration",level:2},{value:"Part 1: Advanced Sensor Configuration",id:"part-1-advanced-sensor-configuration",level:2},{value:"Step 1: Project Setup",id:"step-1-project-setup",level:3},{value:"Step 2: Advanced Gazebo Sensor Plugins",id:"step-2-advanced-gazebo-sensor-plugins",level:3},{value:"Part 2: Sensor Fusion Implementation",id:"part-2-sensor-fusion-implementation",level:2},{value:"Step 3: Robot Localization with Sensor Fusion",id:"step-3-robot-localization-with-sensor-fusion",level:3},{value:"Step 4: Sensor Fusion Node Implementation",id:"step-4-sensor-fusion-node-implementation",level:3},{value:"Part 3: Launch Files for Sensor Simulation",id:"part-3-launch-files-for-sensor-simulation",level:2},{value:"Step 5: Advanced Robot Launch File",id:"step-5-advanced-robot-launch-file",level:3},{value:"Part 4: Testing and Analysis",id:"part-4-testing-and-analysis",level:2},{value:"Step 6: Sensor Analysis Tools",id:"step-6-sensor-analysis-tools",level:3},{value:"Step 7: Building the Package",id:"step-7-building-the-package",level:3},{value:"Step 8: Building and Running the Simulation",id:"step-8-building-and-running-the-simulation",level:3},{value:"Part 5: Lab Exercises",id:"part-5-lab-exercises",level:2},{value:"Exercise 1: Sensor Noise Analysis",id:"exercise-1-sensor-noise-analysis",level:3},{value:"Exercise 2: Sensor Fusion Implementation",id:"exercise-2-sensor-fusion-implementation",level:3},{value:"Exercise 3: Multi-Sensor Mapping",id:"exercise-3-multi-sensor-mapping",level:3},{value:"Exercise 4: Sensor Failure Simulation",id:"exercise-4-sensor-failure-simulation",level:3},{value:"Lab Assignment",id:"lab-assignment",level:2},{value:"Task 1: Add a 3D LIDAR",id:"task-1-add-a-3d-lidar",level:3},{value:"Task 2: Implement Adaptive Sensor Fusion",id:"task-2-implement-adaptive-sensor-fusion",level:3},{value:"Task 3: Create a Custom Sensor Model",id:"task-3-create-a-custom-sensor-model",level:3},{value:"Summary",id:"summary",level:2},{value:"Additional Resources",id:"additional-resources",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"lab-5-advanced-sensor-simulation",children:"Lab 5: Advanced Sensor Simulation"})}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"This lab focuses on implementing and simulating advanced sensors for robotic systems in Gazebo. You will learn to configure various sensor types including LIDAR, IMU, GPS, and force/torque sensors, understand their simulation characteristics, and integrate them with ROS 2 for realistic perception and state estimation."}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"After completing this lab, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Configure and simulate various sensor types in Gazebo"}),"\n",(0,a.jsx)(e.li,{children:"Understand the characteristics and limitations of simulated sensors"}),"\n",(0,a.jsx)(e.li,{children:"Implement sensor fusion techniques for state estimation"}),"\n",(0,a.jsx)(e.li,{children:"Analyze sensor noise and error models in simulation"}),"\n",(0,a.jsx)(e.li,{children:"Integrate multiple sensors into a coherent perception system"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Completion of Lab 4: Basic Robot Model and Simulation"}),"\n",(0,a.jsx)(e.li,{children:"Understanding of ROS 2 concepts including message types and topics"}),"\n",(0,a.jsx)(e.li,{children:"Basic knowledge of coordinate frames and transformations"}),"\n",(0,a.jsx)(e.li,{children:"Familiarity with Gazebo simulation from Module 2"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"hardwaresoftware-requirements",children:"Hardware/Software Requirements"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Ubuntu 22.04 LTS"}),"\n",(0,a.jsx)(e.li,{children:"ROS 2 Humble Hawksbill"}),"\n",(0,a.jsx)(e.li,{children:"Gazebo Fortress (Ignition)"}),"\n",(0,a.jsx)(e.li,{children:"Basic text editor or IDE"}),"\n",(0,a.jsx)(e.li,{children:"Minimum 8GB RAM recommended"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"lab-duration",children:"Lab Duration"}),"\n",(0,a.jsx)(e.p,{children:"Estimated completion time: 3-4 hours"}),"\n",(0,a.jsx)(e.h2,{id:"part-1-advanced-sensor-configuration",children:"Part 1: Advanced Sensor Configuration"}),"\n",(0,a.jsx)(e.h3,{id:"step-1-project-setup",children:"Step 1: Project Setup"}),"\n",(0,a.jsx)(e.p,{children:"We'll extend the robot model from Lab 4 to include advanced sensors. First, let's create a new URDF file with additional sensors:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- File: ~/robotics_ws/src/robot_description/urdf/advanced_robot.xacro --\x3e\n<?xml version="1.0"?>\n<robot name="advanced_robot" xmlns:xacro="http://www.ros.org/wiki/xacro">\n\n  \x3c!-- Include the base robot model --\x3e\n  <xacro:include filename="$(find robot_description)/urdf/robot.xacro"/>\n\n  \x3c!-- Properties for sensors --\x3e\n  <xacro:property name="M_PI" value="3.1415926535897931" />\n\n  \x3c!-- IMU Sensor --\x3e\n  <link name="imu_link">\n    <visual>\n      <geometry>\n        <box size="0.02 0.02 0.02"/>\n      </geometry>\n      <material name="green">\n        <color rgba="0 0.8 0 1"/>\n      </material>\n    </visual>\n\n    <collision>\n      <geometry>\n        <box size="0.02 0.02 0.02"/>\n      </geometry>\n    </collision>\n\n    <inertial>\n      <mass value="0.01"/>\n      <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n    </inertial>\n  </link>\n\n  <joint name="imu_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="imu_link"/>\n    <origin xyz="0 0 0.05" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- GPS Sensor --\x3e\n  <link name="gps_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.01" length="0.01"/>\n      </geometry>\n      <material name="yellow">\n        <color rgba="0.8 0.8 0 1"/>\n      </material>\n    </visual>\n\n    <collision>\n      <geometry>\n        <cylinder radius="0.01" length="0.01"/>\n      </geometry>\n    </collision>\n\n    <inertial>\n      <mass value="0.01"/>\n      <inertia ixx="0.000001" ixy="0" ixz="0" iyy="0.000001" iyz="0" izz="0.000001"/>\n    </inertial>\n  </link>\n\n  <joint name="gps_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="gps_link"/>\n    <origin xyz="0.1 0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- 360 LIDAR Sensor --\x3e\n  <link name="lidar_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.02" length="0.03"/>\n      </geometry>\n      <material name="red">\n        <color rgba="0.8 0 0 1"/>\n      </material>\n    </visual>\n\n    <collision>\n      <geometry>\n        <cylinder radius="0.02" length="0.03"/>\n      </geometry>\n    </collision>\n\n    <inertial>\n      <mass value="0.1"/>\n      <inertia ixx="0.00005" ixy="0" ixz="0" iyy="0.00005" iyz="0" izz="0.00008"/>\n    </inertial>\n  </link>\n\n  <joint name="lidar_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="lidar_link"/>\n    <origin xyz="0 0 0.15" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Force/Torque Sensor --\x3e\n  <link name="ft_sensor_link">\n    <visual>\n      <geometry>\n        <box size="0.03 0.03 0.01"/>\n      </geometry>\n      <material name="purple">\n        <color rgba="0.8 0 0.8 1"/>\n      </material>\n    </visual>\n\n    <collision>\n      <geometry>\n        <box size="0.03 0.03 0.01"/>\n      </geometry>\n    </collision>\n\n    <inertial>\n      <mass value="0.05"/>\n      <inertia ixx="0.00001" ixy="0" ixz="0" iyy="0.00001" iyz="0" izz="0.00001"/>\n    </inertial>\n  </link>\n\n  <joint name="ft_sensor_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="ft_sensor_link"/>\n    <origin xyz="-0.2 0 0.05" rpy="0 0 3.14159"/>\n  </joint>\n\n  \x3c!-- Include advanced Gazebo configurations --\x3e\n  <xacro:include filename="$(find robot_description)/urdf/advanced_robot.gazebo.xacro"/>\n\n</robot>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-2-advanced-gazebo-sensor-plugins",children:"Step 2: Advanced Gazebo Sensor Plugins"}),"\n",(0,a.jsx)(e.p,{children:"Create the Gazebo-specific configuration file for the advanced sensors:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'\x3c!-- File: ~/robotics_ws/src/robot_description/urdf/advanced_robot.gazebo.xacro --\x3e\n<?xml version="1.0"?>\n<robot xmlns:xacro="http://www.ros.org/wiki/xacro">\n\n  \x3c!-- Include the base Gazebo configuration --\x3e\n  <xacro:include filename="$(find robot_description)/urdf/robot.gazebo.xacro"/>\n\n  \x3c!-- IMU Sensor Plugin --\x3e\n  <gazebo reference="imu_link">\n    <sensor name="imu_sensor" type="imu">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <imu>\n        <angular_velocity>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n              <bias_mean>0.0000075</bias_mean>\n              <bias_stddev>0.0000008</bias_stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n              <bias_mean>0.0000075</bias_mean>\n              <bias_stddev>0.0000008</bias_stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>2e-4</stddev>\n              <bias_mean>0.0000075</bias_mean>\n              <bias_stddev>0.0000008</bias_stddev>\n            </noise>\n          </z>\n        </angular_velocity>\n        <linear_acceleration>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n              <bias_mean>0.01</bias_mean>\n              <bias_stddev>0.001</bias_stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n              <bias_mean>0.01</bias_mean>\n              <bias_stddev>0.001</bias_stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n              <bias_mean>0.01</bias_mean>\n              <bias_stddev>0.001</bias_stddev>\n            </noise>\n          </z>\n        </linear_acceleration>\n      </imu>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- GPS Sensor Plugin --\x3e\n  <gazebo reference="gps_link">\n    <sensor name="gps_sensor" type="navsat">\n      <always_on>true</always_on>\n      <update_rate>10</update_rate>\n      <navsat>\n        <position_sensing>\n          <horizontal>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.01</stddev>\n            </noise>\n          </horizontal>\n          <vertical>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.02</stddev>\n            </noise>\n          </vertical>\n        </position_sensing>\n      </navsat>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- 360 LIDAR Sensor Plugin --\x3e\n  <gazebo reference="lidar_link">\n    <sensor name="lidar_sensor" type="ray">\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>360</samples>\n            <resolution>1</resolution>\n            <min_angle>-3.14159</min_angle>\n            <max_angle>3.14159</max_angle>\n          </horizontal>\n        </scan>\n        <range>\n          <min>0.1</min>\n          <max>30.0</max>\n          <resolution>0.01</resolution>\n        </range>\n      </ray>\n      <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n        <ros>\n          <namespace>lidar</namespace>\n          <remapping>~/out:=scan</remapping>\n        </ros>\n        <output_type>sensor_msgs/LaserScan</output_type>\n        <frame_name>lidar_link</frame_name>\n      </plugin>\n      <always_on>true</always_on>\n      <update_rate>10</update_rate>\n      <visualize>true</visualize>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Force/Torque Sensor Plugin --\x3e\n  <gazebo reference="ft_sensor_link">\n    <sensor name="ft_sensor" type="force_torque">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <force_torque>\n        <frame>sensor</frame>\n        <measure_direction>child_to_parent</measure_direction>\n      </force_torque>\n      <plugin name="ft_sensor_controller" filename="libgazebo_ros_ft_sensor.so">\n        <ros>\n          <namespace>ft_sensor</namespace>\n          <remapping>~/wrench:=wrench</remapping>\n        </ros>\n        <frame_name>ft_sensor_link</frame_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Color materials for sensors --\x3e\n  <gazebo reference="imu_link">\n    <material>Gazebo/Green</material>\n  </gazebo>\n\n  <gazebo reference="gps_link">\n    <material>Gazebo/Yellow</material>\n  </gazebo>\n\n  <gazebo reference="lidar_link">\n    <material>Gazebo/Red</material>\n  </gazebo>\n\n  <gazebo reference="ft_sensor_link">\n    <material>Gazebo/Purple</material>\n  </gazebo>\n\n</robot>\n'})}),"\n",(0,a.jsx)(e.h2,{id:"part-2-sensor-fusion-implementation",children:"Part 2: Sensor Fusion Implementation"}),"\n",(0,a.jsx)(e.h3,{id:"step-3-robot-localization-with-sensor-fusion",children:"Step 3: Robot Localization with Sensor Fusion"}),"\n",(0,a.jsx)(e.p,{children:"Create a configuration file for the robot_localization package to fuse IMU and odometry data:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-yaml",children:"# File: ~/robotics_ws/src/robot_description/config/robot_localization.yaml\n# The frequency, in Hz, at which the filter will output a position estimate\nfrequency: 50\n\n# The period, in seconds, after which we consider a sensor to have timed out\nsensor_timeout: 0.1\n\n# Whether to two-dimensionalize the robot's pose\ntwo_d_mode: true\n\n# Whether to print diagnostic messages\nprint_diagnostics: true\n\n# Settings for each IMU input\nimu0: /imu/data\nimu0_config: [false, false, false,  # x, y, z\n              false, false, false,  # roll, pitch, yaw\n              true, true, true,    # x_vel, y_vel, z_vel\n              false, false, false,  # roll_vel, pitch_vel, yaw_vel\n              true, true, true]    # x_acc, y_acc, z_acc\nimu0_differential: false\nimu0_relative: true\nimu0_queue_size: 10\n\n# Settings for odometry input\nodom0: /odom\nodom0_config: [true, true, false,  # x, y, z\n               false, false, true,  # roll, pitch, yaw\n               false, false, false,  # x_vel, y_vel, z_vel\n               false, false, false,  # roll_vel, pitch_vel, yaw_vel\n               false, false, false]  # x_acc, y_acc, z_acc\nodom0_differential: false\nodom0_relative: true\nodom0_queue_size: 10\n\n# Process noise for the filter\nprocess_noise_covariance: [0.05, 0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                          0,    0.05, 0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                          0,    0,    0.06, 0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                          0,    0,    0,    0.03, 0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                          0,    0,    0,    0,    0.03, 0,    0,     0,     0,    0,    0,    0,    0,    0,    0,\n                          0,    0,    0,    0,    0,    0.06, 0,     0,     0,    0,    0,    0,    0,    0,    0,\n                          0,    0,    0,    0,    0,    0,    0.025, 0,     0,    0,    0,    0,    0,    0,    0,\n                          0,    0,    0,    0,    0,    0,    0,     0.025, 0,    0,    0,    0,    0,    0,    0,\n                          0,    0,    0,    0,    0,    0,    0,     0,     0.04, 0,    0,    0,    0,    0,    0,\n                          0,    0,    0,    0,    0,    0,    0,     0,     0,    0.01, 0,    0,    0,    0,    0,\n                          0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0.01, 0,    0,    0,    0,\n                          0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0.02, 0,    0,    0,\n                          0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0.01, 0,    0,\n                          0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0.01, 0,\n                          0,    0,    0,    0,    0,    0,    0,     0,     0,    0,    0,    0,    0,    0,    0.015]\n\n# Initial estimate error covariance\ninitial_estimate_covariance: [1e-9, 0,    0,    0,    0,    0,    0,    0,    0,    0,     0,     0,     0,    0,    0,\n                             0,    1e-9, 0,    0,    0,    0,    0,    0,    0,    0,     0,     0,     0,    0,    0,\n                             0,    0,    1e-9, 0,    0,    0,    0,    0,    0,    0,     0,     0,     0,    0,    0,\n                             0,    0,    0,    1e-9, 0,    0,    0,    0,    0,    0,     0,     0,     0,    0,    0,\n                             0,    0,    0,    0,    1e-9, 0,    0,    0,    0,    0,     0,     0,     0,    0,    0,\n                             0,    0,    0,    0,    0,    1e-9, 0,    0,    0,    0,     0,     0,     0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    1e-9, 0,    0,    0,     0,     0,     0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0,    1e-9, 0,    0,     0,     0,     0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0,    0,    1e-9, 0,     0,     0,     0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0,    0,    0,    1e-9,  0,     0,     0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,     1e-9,  0,     0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,     0,     1e-9,  0,    0,    0,\n                             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,     0,     0,     1e-9, 0,    0,\n                             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,     0,     0,     0,    1e-9, 0,\n                             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,     0,     0,     0,    0,    1e-9]\n"})}),"\n",(0,a.jsx)(e.h3,{id:"step-4-sensor-fusion-node-implementation",children:"Step 4: Sensor Fusion Node Implementation"}),"\n",(0,a.jsx)(e.p,{children:"Create a Python node to demonstrate sensor fusion techniques:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: ~/robotics_ws/src/robot_description/sensor_fusion/sensor_fusion_node.py\n#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, LaserScan, NavSatFix\nfrom geometry_msgs.msg import Twist, Vector3\nfrom std_msgs.msg import Float64\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport math\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__('sensor_fusion_node')\n\n        # Initialize state variables\n        self.position = np.array([0.0, 0.0, 0.0])\n        self.velocity = np.array([0.0, 0.0, 0.0])\n        self.orientation = np.array([0.0, 0.0, 0.0, 1.0])  # quaternion\n        self.linear_acceleration = np.array([0.0, 0.0, 0.0])\n        self.angular_velocity = np.array([0.0, 0.0, 0.0])\n\n        # Time tracking\n        self.last_time = self.get_clock().now()\n\n        # Subscribers\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n        self.lidar_sub = self.create_subscription(\n            LaserScan, '/scan', self.lidar_callback, 10)\n        self.gps_sub = self.create_subscription(\n            NavSatFix, '/gps/fix', self.gps_callback, 10)\n\n        # Publishers\n        self.position_pub = self.create_publisher(\n            Vector3, '/estimated_position', 10)\n        self.velocity_pub = self.create_publisher(\n            Vector3, '/estimated_velocity', 10)\n        self.status_pub = self.create_publisher(\n            Float64, '/sensor_fusion_status', 10)\n\n        # Timer for publishing estimated state\n        self.timer = self.create_timer(0.02, self.publish_state)  # 50 Hz\n\n        self.get_logger().info('Sensor Fusion Node initialized')\n\n    def imu_callback(self, msg):\n        \"\"\"Process IMU data for state estimation\"\"\"\n        # Extract linear acceleration (remove gravity)\n        linear_acc = np.array([\n            msg.linear_acceleration.x,\n            msg.linear_acceleration.y,\n            msg.linear_acceleration.z\n        ])\n\n        # Extract angular velocity\n        ang_vel = np.array([\n            msg.angular_velocity.x,\n            msg.angular_velocity.y,\n            msg.angular_velocity.z\n        ])\n\n        # Extract orientation\n        orientation = np.array([\n            msg.orientation.x,\n            msg.orientation.y,\n            msg.orientation.z,\n            msg.orientation.w\n        ])\n\n        # Update internal state\n        self.linear_acceleration = linear_acc\n        self.angular_velocity = ang_vel\n        self.orientation = orientation\n\n        # Integrate acceleration to get velocity (simple integration)\n        current_time = self.get_clock().now()\n        dt = (current_time - self.last_time).nanoseconds / 1e9\n        self.last_time = current_time\n\n        if dt > 0:\n            self.velocity += linear_acc * dt\n            # Update position based on velocity\n            self.position += self.velocity * dt\n\n    def lidar_callback(self, msg):\n        \"\"\"Process LIDAR data for obstacle detection and mapping\"\"\"\n        # Calculate distances to nearest obstacles\n        ranges = np.array(msg.ranges)\n        # Remove invalid measurements (inf, nan)\n        valid_ranges = ranges[np.isfinite(ranges)]\n\n        if len(valid_ranges) > 0:\n            min_distance = np.min(valid_ranges)\n            self.get_logger().debug(f'Min obstacle distance: {min_distance:.2f}m')\n\n        # Calculate front, left, right distances\n        if len(ranges) >= 360:\n            front_idx = 0\n            left_idx = 90\n            right_idx = 270\n\n            front_distance = ranges[front_idx] if np.isfinite(ranges[front_idx]) else float('inf')\n            left_distance = ranges[left_idx] if np.isfinite(ranges[left_idx]) else float('inf')\n            right_distance = ranges[right_idx] if np.isfinite(ranges[right_idx]) else float('inf')\n\n            self.get_logger().debug(f'Front: {front_distance:.2f}, Left: {left_distance:.2f}, Right: {right_distance:.2f}')\n\n    def gps_callback(self, msg):\n        \"\"\"Process GPS data for absolute position\"\"\"\n        # For simulation, we'll use GPS as ground truth for position\n        # In real applications, we would fuse GPS with other sensors\n        self.position[0] = msg.longitude  # X position in local frame\n        self.position[1] = msg.latitude   # Y position in local frame\n        self.position[2] = msg.altitude   # Z position in local frame\n\n    def publish_state(self):\n        \"\"\"Publish the estimated state\"\"\"\n        # Create and publish position\n        pos_msg = Vector3()\n        pos_msg.x = float(self.position[0])\n        pos_msg.y = float(self.position[1])\n        pos_msg.z = float(self.position[2])\n        self.position_pub.publish(pos_msg)\n\n        # Create and publish velocity\n        vel_msg = Vector3()\n        vel_msg.x = float(self.velocity[0])\n        vel_msg.y = float(self.velocity[1])\n        vel_msg.z = float(self.velocity[2])\n        self.velocity_pub.publish(vel_msg)\n\n        # Publish fusion status (simple indicator)\n        status_msg = Float64()\n        status_msg.data = 1.0  # Indicate fusion is active\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sensor_fusion_node = SensorFusionNode()\n\n    try:\n        rclpy.spin(sensor_fusion_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        sensor_fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"part-3-launch-files-for-sensor-simulation",children:"Part 3: Launch Files for Sensor Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"step-5-advanced-robot-launch-file",children:"Step 5: Advanced Robot Launch File"}),"\n",(0,a.jsx)(e.p,{children:"Create a launch file for the advanced robot with all sensors:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: ~/robotics_ws/src/robot_description/launch/advanced_robot.launch.py\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription, DeclareLaunchArgument\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\nimport os\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n    world = LaunchConfiguration('world', default='empty.sdf')\n\n    # Paths\n    pkg_gazebo_ros = FindPackageShare('ros_gz_sim')\n    robot_description_path = PathJoinSubstitution([\n        FindPackageShare('robot_description'),\n        'urdf',\n        'advanced_robot.xacro'\n    ])\n\n    # Launch Gazebo\n    gazebo = IncludeLaunchDescription(\n        PythonLaunchDescriptionSource([\n            PathJoinSubstitution([\n                FindPackageShare('ros_gz_sim'),\n                'launch',\n                'gz_sim.launch.py'\n            ])\n        ]),\n        launch_arguments={\n            'gz_args': ['-r', world]\n        }.items()\n    )\n\n    # Read robot description\n    with open(os.path.join(\n        FindPackageShare('robot_description').perform({}),\n        'urdf',\n        'advanced_robot.xacro'\n    ), 'r') as infp:\n        robot_description = infp.read()\n\n    # Robot state publisher\n    robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        name='robot_state_publisher',\n        output='screen',\n        parameters=[{\n            'use_sim_time': use_sim_time,\n            'robot_description': robot_description\n        }]\n    )\n\n    # Spawn entity\n    spawn_entity = Node(\n        package='ros_gz_sim',\n        executable='spawn_entity.py',\n        arguments=[\n            '-topic', 'robot_description',\n            '-entity', 'advanced_robot',\n            '-x', '0.0',\n            '-y', '0.0',\n            '-z', '0.5'\n        ],\n        output='screen'\n    )\n\n    # Robot localization node\n    robot_localization = Node(\n        package='robot_localization',\n        executable='ekf_node',\n        name='ekf_filter_node',\n        output='screen',\n        parameters=[\n            PathJoinSubstitution([\n                FindPackageShare('robot_description'),\n                'config',\n                'robot_localization.yaml'\n            ])\n        ]\n    )\n\n    # Sensor fusion node\n    sensor_fusion = Node(\n        package='robot_description',\n        executable='sensor_fusion_node',\n        name='sensor_fusion_node',\n        output='screen'\n    )\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation (Gazebo) clock if true'),\n        DeclareLaunchArgument(\n            'world',\n            default_value='empty.sdf',\n            description='Choose one of the world files from `/usr/share/gazebo/worlds`'),\n        gazebo,\n        robot_state_publisher,\n        spawn_entity,\n        robot_localization,\n        sensor_fusion\n    ])\n"})}),"\n",(0,a.jsx)(e.h2,{id:"part-4-testing-and-analysis",children:"Part 4: Testing and Analysis"}),"\n",(0,a.jsx)(e.h3,{id:"step-6-sensor-analysis-tools",children:"Step 6: Sensor Analysis Tools"}),"\n",(0,a.jsx)(e.p,{children:"Create a script to analyze sensor data quality:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# File: ~/robotics_ws/src/robot_description/scripts/sensor_analysis.py\n#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, LaserScan, NavSatFix\nfrom std_msgs.msg import Float64MultiArray\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import deque\n\nclass SensorAnalysisNode(Node):\n    def __init__(self):\n        super().__init__('sensor_analysis_node')\n\n        # Data storage for analysis\n        self.imu_linear_acc_x = deque(maxlen=1000)\n        self.imu_linear_acc_y = deque(maxlen=1000)\n        self.imu_linear_acc_z = deque(maxlen=1000)\n\n        self.lidar_ranges = deque(maxlen=100)\n\n        # Subscribers\n        self.imu_sub = self.create_subscription(\n            Imu, '/imu/data', self.imu_callback, 10)\n        self.lidar_sub = self.create_subscription(\n            LaserScan, '/scan', self.lidar_callback, 10)\n\n        # Publishers for analysis results\n        self.noise_pub = self.create_publisher(\n            Float64MultiArray, '/sensor_noise_analysis', 10)\n\n        # Timer for analysis\n        self.timer = self.create_timer(1.0, self.analyze_data)\n\n        self.get_logger().info('Sensor Analysis Node initialized')\n\n    def imu_callback(self, msg):\n        \"\"\"Store IMU data for analysis\"\"\"\n        self.imu_linear_acc_x.append(msg.linear_acceleration.x)\n        self.imu_linear_acc_y.append(msg.linear_acceleration.y)\n        self.imu_linear_acc_z.append(msg.linear_acceleration.z)\n\n    def lidar_callback(self, msg):\n        \"\"\"Store LIDAR data for analysis\"\"\"\n        self.lidar_ranges.append(np.array(msg.ranges))\n\n    def analyze_data(self):\n        \"\"\"Analyze collected sensor data\"\"\"\n        if len(self.imu_linear_acc_x) > 10:\n            # Calculate statistics for IMU data\n            acc_x_mean = np.mean(self.imu_linear_acc_x)\n            acc_x_std = np.std(self.imu_linear_acc_x)\n            acc_y_mean = np.mean(self.imu_linear_acc_y)\n            acc_y_std = np.std(self.imu_linear_acc_y)\n            acc_z_mean = np.mean(self.imu_linear_acc_z)\n            acc_z_std = np.std(self.imu_linear_acc_z)\n\n            # Publish analysis results\n            analysis_msg = Float64MultiArray()\n            analysis_msg.data = [\n                acc_x_mean, acc_x_std,  # X acceleration mean and std\n                acc_y_mean, acc_y_std,  # Y acceleration mean and std\n                acc_z_mean, acc_z_std   # Z acceleration mean and std\n            ]\n            self.noise_pub.publish(analysis_msg)\n\n            self.get_logger().info(\n                f'IMU Noise Analysis - X: \u03bc={acc_x_mean:.4f}, \u03c3={acc_x_std:.4f} | '\n                f'Y: \u03bc={acc_y_mean:.4f}, \u03c3={acc_y_std:.4f} | '\n                f'Z: \u03bc={acc_z_mean:.4f}, \u03c3={acc_z_std:.4f}'\n            )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sensor_analysis_node = SensorAnalysisNode()\n\n    try:\n        rclpy.spin(sensor_analysis_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        sensor_analysis_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h3,{id:"step-7-building-the-package",children:"Step 7: Building the Package"}),"\n",(0,a.jsx)(e.p,{children:"Update the CMakeLists.txt to include the new Python nodes:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-cmake",children:'cmake_minimum_required(VERSION 3.8)\nproject(robot_description)\n\nif(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES "Clang")\n  add_compile_options(-Wall -Wextra -Wpedantic)\nendif()\n\n# Find dependencies\nfind_package(ament_cmake REQUIRED)\nfind_package(urdf REQUIRED)\nfind_package(xacro REQUIRED)\n\n# Install launch files\ninstall(DIRECTORY\n  launch\n  urdf\n  config\n  DESTINATION share/${PROJECT_NAME}\n)\n\n# Install scripts\ninstall(PROGRAMS\n  scripts/sensor_analysis.py\n  sensor_fusion/sensor_fusion_node.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n\nif(BUILD_TESTING)\n  find_package(ament_lint_auto REQUIRED)\n  set(ament_cmake_copyright_FOUND TRUE)\n  set(ament_cmake_flake8_FOUND TRUE)\n  set(ament_cmake_pep257_FOUND TRUE)\n  ament_lint_auto_find_test_dependencies()\nendif()\n\nament_package()\n'})}),"\n",(0,a.jsx)(e.p,{children:"And update package.xml to include new dependencies:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>robot_description</name>\n  <version>0.0.0</version>\n  <description>Robot description package for simple robot model</description>\n  <maintainer email="user@todo.todo">user</maintainer>\n  <license>Apache-2.0</license>\n\n  <buildtool_depend>ament_cmake</buildtool_depend>\n\n  <depend>urdf</depend>\n  <depend>xacro</depend>\n  <depend>robot_state_publisher</depend>\n  <depend>ros_gz_sim</depend>\n  <depend>robot_localization</depend>\n  <depend>sensor_msgs</depend>\n  <depend>geometry_msgs</depend>\n  <depend>std_msgs</depend>\n\n  <exec_depend>launch</exec_depend>\n  <exec_depend>launch_ros</exec_depend>\n\n  <test_depend>ament_lint_auto</test_depend>\n  <test_depend>ament_lint_common</test_depend>\n\n  <export>\n    <build_type>ament_cmake</build_type>\n  </export>\n</package>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-8-building-and-running-the-simulation",children:"Step 8: Building and Running the Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Build the updated package:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"cd ~/robotics_ws\ncolcon build --packages-select robot_description\nsource install/setup.bash\n"})}),"\n",(0,a.jsx)(e.p,{children:"Launch the advanced robot simulation:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Source the workspace\nsource ~/robotics_ws/install/setup.bash\n\n# Launch the robot with all sensors\nros2 launch robot_description advanced_robot.launch.py\n"})}),"\n",(0,a.jsx)(e.p,{children:"In another terminal, check the sensor topics:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Source the workspace\nsource ~/robotics_ws/install/setup.bash\n\n# List all topics\nros2 topic list\n\n# Echo IMU data\nros2 topic echo /imu/data\n\n# Echo LIDAR data\nros2 topic echo /scan\n\n# Echo GPS data\nros2 topic echo /gps/fix\n"})}),"\n",(0,a.jsx)(e.h2,{id:"part-5-lab-exercises",children:"Part 5: Lab Exercises"}),"\n",(0,a.jsx)(e.h3,{id:"exercise-1-sensor-noise-analysis",children:"Exercise 1: Sensor Noise Analysis"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Run the sensor analysis node to observe noise characteristics"}),"\n",(0,a.jsx)(e.li,{children:"Compare the noise levels with the values defined in the URDF"}),"\n",(0,a.jsx)(e.li,{children:"Document the differences and explain potential causes"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"exercise-2-sensor-fusion-implementation",children:"Exercise 2: Sensor Fusion Implementation"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Implement a simple Kalman filter to fuse IMU and odometry data"}),"\n",(0,a.jsx)(e.li,{children:"Compare the fused position estimate with individual sensor estimates"}),"\n",(0,a.jsx)(e.li,{children:"Analyze the improvement in accuracy"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"exercise-3-multi-sensor-mapping",children:"Exercise 3: Multi-Sensor Mapping"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Create a simple occupancy grid using LIDAR data"}),"\n",(0,a.jsx)(e.li,{children:"Overlay the robot's position (from fusion) on the grid"}),"\n",(0,a.jsx)(e.li,{children:"Visualize the resulting map in RViz"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"exercise-4-sensor-failure-simulation",children:"Exercise 4: Sensor Failure Simulation"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Modify the URDF to simulate sensor failures (e.g., set update_rate to 0)"}),"\n",(0,a.jsx)(e.li,{children:"Observe how the sensor fusion algorithm adapts"}),"\n",(0,a.jsx)(e.li,{children:"Implement a basic sensor health monitoring system"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"lab-assignment",children:"Lab Assignment"}),"\n",(0,a.jsx)(e.h3,{id:"task-1-add-a-3d-lidar",children:"Task 1: Add a 3D LIDAR"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Add a 3D LIDAR sensor (Velodyne-style) to the robot"}),"\n",(0,a.jsx)(e.li,{children:"Configure appropriate parameters for realistic simulation"}),"\n",(0,a.jsx)(e.li,{children:"Visualize the 3D point cloud in RViz"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"task-2-implement-adaptive-sensor-fusion",children:"Task 2: Implement Adaptive Sensor Fusion"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Create a sensor fusion algorithm that adapts based on sensor reliability"}),"\n",(0,a.jsx)(e.li,{children:"Implement a method to detect when individual sensors become unreliable"}),"\n",(0,a.jsx)(e.li,{children:"Adjust the fusion algorithm to give less weight to unreliable sensors"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"task-3-create-a-custom-sensor-model",children:"Task 3: Create a Custom Sensor Model"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Design a custom sensor model (e.g., thermal camera, ultrasonic array)"}),"\n",(0,a.jsx)(e.li,{children:"Implement the sensor plugin for Gazebo"}),"\n",(0,a.jsx)(e.li,{children:"Integrate the sensor into the robot model and simulation"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"In this lab, you've learned to configure and simulate various types of advanced sensors in Gazebo, implemented sensor fusion techniques for state estimation, and analyzed sensor noise characteristics. You've gained experience with IMU, GPS, LIDAR, and force/torque sensors, and understand how to integrate them into a coherent perception system."}),"\n",(0,a.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"http://gazebosim.org/tutorials?tut=ros_gz_sensors",children:"Gazebo Sensor Documentation"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"http://docs.ros.org/en/noetic/api/robot_localization/html/",children:"Robot Localization Package"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://docs.ros.org/en/rolling/Releases/Release-Galactic-Geochelone.html",children:"Sensor Messages in ROS 2"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://navigation.ros.org/setup_guides/index.html",children:"ROS 2 Sensor Integration Guide"})}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsx)(e.p,{children:"After completing this lab, you should:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand the characteristics of various robot sensors"}),"\n",(0,a.jsx)(e.li,{children:"Be able to configure and simulate advanced sensors in Gazebo"}),"\n",(0,a.jsx)(e.li,{children:"Have experience with sensor fusion techniques"}),"\n",(0,a.jsx)(e.li,{children:"Be ready to proceed to Lab 6: Unity Integration with Simulation"}),"\n"]})]})}function u(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>t,x:()=>r});var s=i(6540);const a={},o=s.createContext(a);function t(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:t(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);