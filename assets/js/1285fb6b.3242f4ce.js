"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[1883],{7225(n,e,t){t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4/lab-11-voice-command","title":"Lab 11: Voice Command Processing","description":"Overview","source":"@site/docs/module-4/lab-11-voice-command.md","sourceDirName":"module-4","slug":"/module-4/lab-11-voice-command","permalink":"/hackathon/docs/module-4/lab-11-voice-command","draft":false,"unlisted":false,"editUrl":"https://github.com/Fatimaali424/hackathon/edit/main/website/docs/module-4/lab-11-voice-command.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Lab 10: Basic Vision-Language Integration","permalink":"/hackathon/docs/module-4/lab-10-vision-language"},"next":{"title":"Lab 12: Complete VLA System Implementation","permalink":"/hackathon/docs/module-4/lab-12-vla-system"}}');var o=t(4848),s=t(8453);const r={sidebar_position:6},a="Lab 11: Voice Command Processing",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Hardware and Software Requirements",id:"hardware-and-software-requirements",level:2},{value:"Required Hardware",id:"required-hardware",level:3},{value:"Required Software",id:"required-software",level:3},{value:"Lab Setup",id:"lab-setup",level:2},{value:"Environment Configuration",id:"environment-configuration",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Basic Voice Command Recognition",id:"step-1-basic-voice-command-recognition",level:3},{value:"Step 2: Command Parser and Intent Recognition",id:"step-2-command-parser-and-intent-recognition",level:3},{value:"Step 3: Speech-to-Intent Processing Pipeline",id:"step-3-speech-to-intent-processing-pipeline",level:3},{value:"Step 4: Advanced Voice Processing with Machine Learning",id:"step-4-advanced-voice-processing-with-machine-learning",level:3},{value:"Step 5: Voice Command System Integration",id:"step-5-voice-command-system-integration",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Basic Testing Script",id:"basic-testing-script",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:3},{value:"Lab Deliverables",id:"lab-deliverables",level:2},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Extensions (Optional)",id:"extensions-optional",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"lab-11-voice-command-processing",children:"Lab 11: Voice Command Processing"})}),"\n",(0,o.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(e.p,{children:"This lab focuses on implementing voice command processing systems for robotics applications. You will learn to capture, process, and understand spoken commands, integrating speech recognition with natural language understanding and robotic action execution. This is crucial for creating intuitive human-robot interaction systems."}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"After completing this lab, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Implement real-time speech recognition systems"}),"\n",(0,o.jsx)(e.li,{children:"Process and understand voice commands in context"}),"\n",(0,o.jsx)(e.li,{children:"Integrate speech recognition with natural language processing"}),"\n",(0,o.jsx)(e.li,{children:"Handle voice command ambiguity and errors"}),"\n",(0,o.jsx)(e.li,{children:"Create robust voice command interfaces for robots"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Completion of Module 1 (ROS 2 fundamentals)"}),"\n",(0,o.jsx)(e.li,{children:"Completion of Module 2 (Simulation concepts)"}),"\n",(0,o.jsx)(e.li,{children:"Completion of Module 3 (Isaac perception)"}),"\n",(0,o.jsx)(e.li,{children:"Basic understanding of signal processing"}),"\n",(0,o.jsx)(e.li,{children:"Familiarity with Python audio processing libraries"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"hardware-and-software-requirements",children:"Hardware and Software Requirements"}),"\n",(0,o.jsx)(e.h3,{id:"required-hardware",children:"Required Hardware"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Microphone for audio input (USB or built-in)"}),"\n",(0,o.jsx)(e.li,{children:"Speakers for audio feedback (optional)"}),"\n",(0,o.jsx)(e.li,{children:"System with sufficient processing power for real-time audio processing"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"required-software",children:"Required Software"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Python 3.8+ with required libraries"}),"\n",(0,o.jsx)(e.li,{children:"Speech recognition libraries (speech_recognition, pyaudio)"}),"\n",(0,o.jsx)(e.li,{children:"Audio processing libraries (librosa, scipy)"}),"\n",(0,o.jsx)(e.li,{children:"Text-to-speech libraries (pyttsx3, espeak)"}),"\n",(0,o.jsx)(e.li,{children:"ROS 2 Humble with audio processing packages"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"lab-setup",children:"Lab Setup"}),"\n",(0,o.jsx)(e.h3,{id:"environment-configuration",children:"Environment Configuration"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Install required packages:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"pip install SpeechRecognition pyaudio\npip install librosa scipy numpy\npip install pyttsx3\npip install torch torchaudio\npip install transformers\n"})}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Test audio input:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import pyaudio\n\n# Test audio input device\np = pyaudio.PyAudio()\nprint(f\"Number of audio input devices: {p.get_device_count()}\")\n\nfor i in range(p.get_device_count()):\n    info = p.get_device_info_by_index(i)\n    if info['maxInputChannels'] > 0:\n        print(f\"Device {i}: {info['name']}\")\n"})}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Verify speech recognition:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import speech_recognition as sr\n\n# Test microphone access\nr = sr.Recognizer()\nwith sr.Microphone() as source:\n    print("Microphone test - say \'hello\':")\n    audio = r.listen(source, timeout=5)\n    try:\n        text = r.recognize_google(audio)\n        print(f"Recognized: {text}")\n    except sr.UnknownValueError:\n        print("Could not understand audio")\n    except sr.RequestError as e:\n        print(f"Error: {e}")\n'})}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,o.jsx)(e.h3,{id:"step-1-basic-voice-command-recognition",children:"Step 1: Basic Voice Command Recognition"}),"\n",(0,o.jsx)(e.p,{children:"Create a foundational voice command recognition system:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# voice_recognition.py\n\nimport speech_recognition as sr\nimport pyaudio\nimport threading\nimport queue\nimport time\nimport logging\nfrom typing import Callable, Optional\n\nclass VoiceCommandRecognizer:\n    def __init__(self, callback: Optional[Callable] = None):\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.callback = callback\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        # Configuration\n        self.recognizer.energy_threshold = 300  # Adjust based on environment\n        self.recognizer.dynamic_energy_threshold = True\n\n        # Threading and queues for continuous recognition\n        self.audio_queue = queue.Queue()\n        self.listening = False\n        self.continuous_thread = None\n\n        # Logging\n        logging.basicConfig(level=logging.INFO)\n        self.logger = logging.getLogger(__name__)\n\n    def set_callback(self, callback: Callable):\n        """Set callback function for recognized commands"""\n        self.callback = callback\n\n    def recognize_from_audio(self, audio_data):\n        """Recognize speech from audio data"""\n        try:\n            # Use Google Web Speech API (requires internet)\n            text = self.recognizer.recognize_google(audio_data)\n            self.logger.info(f"Recognized: {text}")\n            return text\n        except sr.UnknownValueError:\n            self.logger.warning("Could not understand audio")\n            return None\n        except sr.RequestError as e:\n            self.logger.error(f"Error with speech recognition service: {e}")\n            return None\n\n    def listen_once(self):\n        """Listen for a single voice command"""\n        self.logger.info("Listening for command...")\n\n        with self.microphone as source:\n            try:\n                audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=5)\n                return self.recognize_from_audio(audio)\n            except sr.WaitTimeoutError:\n                self.logger.warning("No speech detected within timeout")\n                return None\n\n    def start_continuous_listening(self):\n        """Start continuous voice command listening"""\n        if self.listening:\n            self.logger.warning("Already listening continuously")\n            return\n\n        self.listening = True\n        self.continuous_thread = threading.Thread(target=self._continuous_listening_worker)\n        self.continuous_thread.daemon = True\n        self.continuous_thread.start()\n\n    def stop_continuous_listening(self):\n        """Stop continuous voice command listening"""\n        self.listening = False\n        if self.continuous_thread:\n            self.continuous_thread.join(timeout=2)\n\n    def _continuous_listening_worker(self):\n        """Worker thread for continuous listening"""\n        with self.microphone as source:\n            while self.listening:\n                try:\n                    # Listen with timeout to allow for stopping\n                    audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=5)\n\n                    # Recognize and process\n                    text = self.recognize_from_audio(audio)\n                    if text and self.callback:\n                        self.callback(text)\n\n                except sr.WaitTimeoutError:\n                    # This is normal - just continue listening\n                    continue\n                except Exception as e:\n                    self.logger.error(f"Error in continuous listening: {e}")\n                    time.sleep(0.1)  # Brief pause before continuing\n\n    def calibrate_microphone(self):\n        """Calibrate microphone for ambient noise"""\n        with self.microphone as source:\n            self.logger.info("Calibrating microphone...")\n            self.recognizer.adjust_for_ambient_noise(source, duration=2)\n            self.logger.info("Calibration complete")\n'})}),"\n",(0,o.jsx)(e.h3,{id:"step-2-command-parser-and-intent-recognition",children:"Step 2: Command Parser and Intent Recognition"}),"\n",(0,o.jsx)(e.p,{children:"Create a system to parse recognized speech and identify intents:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# command_parser.py\n\nimport re\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass ParsedCommand:\n    intent: str\n    entities: Dict[str, str]\n    confidence: float\n    original_text: str\n\nclass VoiceCommandParser:\n    def __init__(self):\n        # Define command patterns and intents\n        self.command_patterns = {\n            'navigation': [\n                r'go to (the )?(?P<location>\\w+)',\n                r'move to (the )?(?P<location>\\w+)',\n                r'go to (the )?(?P<location>[\\w\\s]+?)\\s*(?:room|area|spot|place)?',\n                r'navigate to (the )?(?P<location>\\w+)',\n                r'bring me to (the )?(?P<location>\\w+)'\n            ],\n            'object_interaction': [\n                r'pick up (the )?(?P<object>[\\w\\s]+)',\n                r'grab (the )?(?P<object>[\\w\\s]+)',\n                r'take (the )?(?P<object>[\\w\\s]+)',\n                r'get (the )?(?P<object>[\\w\\s]+)',\n                r'bring me (the )?(?P<object>[\\w\\s]+)'\n            ],\n            'action': [\n                r'follow (me|him|her)',\n                r'stop',\n                r'start',\n                r'wait',\n                r'help',\n                r'what can you do'\n            ],\n            'question': [\n                r'what is (this|that)',\n                r'where is (the )?(?P<object>[\\w\\s]+)',\n                r'how (many|much|long|big|tall|far) (is|are|does|do)',\n                r'what time is it',\n                r'what day is it'\n            ]\n        }\n\n        # Location mappings\n        self.location_synonyms = {\n            'kitchen': ['kitchen', 'cooking area', 'cooking room'],\n            'bedroom': ['bedroom', 'sleeping room', 'bed room'],\n            'living room': ['living room', 'living area', 'sitting room', 'lounge'],\n            'office': ['office', 'study', 'work room'],\n            'bathroom': ['bathroom', 'restroom', 'toilet', 'bath'],\n            'dining room': ['dining room', 'dining area', 'dining hall']\n        }\n\n        # Object mappings\n        self.object_synonyms = {\n            'water': ['water', 'bottle of water', 'water bottle'],\n            'book': ['book', 'reading book', 'textbook'],\n            'cup': ['cup', 'coffee cup', 'mug', 'glass'],\n            'phone': ['phone', 'mobile', 'cell phone', 'smartphone'],\n            'keys': ['keys', 'key', 'house keys', 'car keys']\n        }\n\n    def parse_command(self, text: str) -> Optional[ParsedCommand]:\n        \"\"\"Parse voice command and extract intent and entities\"\"\"\n        text = text.lower().strip()\n\n        # Check each intent type\n        for intent, patterns in self.command_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, text)\n                if match:\n                    entities = match.groupdict()\n\n                    # Normalize entities using synonyms\n                    normalized_entities = {}\n                    for key, value in entities.items():\n                        normalized_value = self._normalize_entity(key, value.strip())\n                        normalized_entities[key] = normalized_value\n\n                    # Calculate confidence based on pattern match strength\n                    confidence = self._calculate_confidence(text, pattern, match)\n\n                    return ParsedCommand(\n                        intent=intent,\n                        entities=normalized_entities,\n                        confidence=confidence,\n                        original_text=text\n                    )\n\n        # If no pattern matches, try to classify as general command\n        return self._classify_general_command(text)\n\n    def _normalize_entity(self, entity_type: str, value: str) -> str:\n        \"\"\"Normalize entity values using synonym mappings\"\"\"\n        if entity_type == 'location':\n            for canonical, synonyms in self.location_synonyms.items():\n                if value in synonyms:\n                    return canonical\n        elif entity_type == 'object':\n            for canonical, synonyms in self.object_synonyms.items():\n                if value in synonyms:\n                    return canonical\n        return value\n\n    def _calculate_confidence(self, text: str, pattern: str, match) -> float:\n        \"\"\"Calculate confidence score for the match\"\"\"\n        # Simple confidence based on text coverage\n        matched_text = match.group(0)\n        confidence = len(matched_text) / len(text)\n\n        # Boost confidence if it's a strong match\n        if len(match.groups()) > 0:  # Has captured groups (entities)\n            confidence *= 1.2\n\n        return min(confidence, 1.0)\n\n    def _classify_general_command(self, text: str) -> Optional[ParsedCommand]:\n        \"\"\"Classify commands that don't match specific patterns\"\"\"\n        if any(word in text for word in ['hello', 'hi', 'hey', 'greetings']):\n            return ParsedCommand(\n                intent='greeting',\n                entities={},\n                confidence=0.8,\n                original_text=text\n            )\n        elif any(word in text for word in ['thank', 'thanks', 'thank you']):\n            return ParsedCommand(\n                intent='acknowledgment',\n                entities={},\n                confidence=0.8,\n                original_text=text\n            )\n        else:\n            return ParsedCommand(\n                intent='unknown',\n                entities={},\n                confidence=0.3,\n                original_text=text\n            )\n\n    def get_suggested_commands(self) -> List[str]:\n        \"\"\"Get list of suggested commands for user guidance\"\"\"\n        suggestions = [\n            \"Go to the kitchen\",\n            \"Pick up the red cup\",\n            \"What time is it?\",\n            \"Where is my phone?\",\n            \"Follow me\",\n            \"Stop\",\n            \"Help\"\n        ]\n        return suggestions\n"})}),"\n",(0,o.jsx)(e.h3,{id:"step-3-speech-to-intent-processing-pipeline",children:"Step 3: Speech-to-Intent Processing Pipeline"}),"\n",(0,o.jsx)(e.p,{children:"Create a complete pipeline that processes speech to actionable robot commands:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# speech_intent_pipeline.py\n\nimport asyncio\nimport threading\nfrom typing import Dict, Any, Callable, Optional\nfrom voice_recognition import VoiceCommandRecognizer\nfrom command_parser import VoiceCommandParser, ParsedCommand\n\nclass SpeechIntentPipeline:\n    def __init__(self, robot_controller_callback: Optional[Callable] = None):\n        self.recognizer = VoiceCommandRecognizer()\n        self.parser = VoiceCommandParser()\n        self.robot_controller = robot_controller_callback\n\n        # Callbacks\n        self.command_callbacks = {\n            \'navigation\': self._handle_navigation,\n            \'object_interaction\': self._handle_object_interaction,\n            \'action\': self._handle_action,\n            \'question\': self._handle_question,\n            \'greeting\': self._handle_greeting,\n            \'acknowledgment\': self._handle_acknowledgment\n        }\n\n        # State management\n        self.is_listening = False\n        self.last_command = None\n        self.command_history = []\n\n    def start_listening(self):\n        """Start the speech-to-intent pipeline"""\n        if not self.is_listening:\n            self.recognizer.set_callback(self._process_recognized_text)\n            self.recognizer.start_continuous_listening()\n            self.is_listening = True\n            print("Voice command system started")\n\n    def stop_listening(self):\n        """Stop the speech-to-intent pipeline"""\n        if self.is_listening:\n            self.recognizer.stop_continuous_listening()\n            self.is_listening = False\n            print("Voice command system stopped")\n\n    def _process_recognized_text(self, text: str):\n        """Process recognized text through the pipeline"""\n        print(f"Recognized: {text}")\n\n        # Parse the command\n        parsed_command = self.parser.parse_command(text)\n\n        if parsed_command:\n            print(f"Parsed - Intent: {parsed_command.intent}, "\n                  f"Entities: {parsed_command.entities}, "\n                  f"Confidence: {parsed_command.confidence:.2f}")\n\n            # Store in history\n            self.command_history.append(parsed_command)\n            self.last_command = parsed_command\n\n            # Handle the command based on intent\n            self._handle_command(parsed_command)\n\n    def _handle_command(self, parsed_command: ParsedCommand):\n        """Handle parsed command based on intent"""\n        if parsed_command.confidence < 0.5:\n            print("Low confidence command - requesting clarification")\n            self._request_clarification(parsed_command)\n            return\n\n        handler = self.command_callbacks.get(parsed_command.intent)\n        if handler:\n            try:\n                handler(parsed_command)\n            except Exception as e:\n                print(f"Error handling command: {e}")\n                self._handle_error(parsed_command, e)\n        else:\n            print(f"Unknown intent: {parsed_command.intent}")\n            self._handle_unknown_command(parsed_command)\n\n    def _handle_navigation(self, command: ParsedCommand):\n        """Handle navigation commands"""\n        location = command.entities.get(\'location\', \'unknown\')\n        print(f"Navigating to {location}")\n\n        if self.robot_controller:\n            self.robot_controller(\'navigate\', {\'location\': location})\n\n    def _handle_object_interaction(self, command: ParsedCommand):\n        """Handle object interaction commands"""\n        obj = command.entities.get(\'object\', \'unknown\')\n        print(f"Interacting with {obj}")\n\n        if self.robot_controller:\n            self.robot_controller(\'manipulate\', {\'object\': obj, \'action\': \'pick_up\'})\n\n    def _handle_action(self, command: ParsedCommand):\n        """Handle action commands"""\n        action = command.original_text\n        print(f"Performing action: {action}")\n\n        if self.robot_controller:\n            self.robot_controller(\'action\', {\'command\': action})\n\n    def _handle_question(self, command: ParsedCommand):\n        """Handle question commands"""\n        question = command.original_text\n        print(f"Processing question: {question}")\n\n        if self.robot_controller:\n            self.robot_controller(\'question\', {\'question\': question})\n\n    def _handle_greeting(self, command: ParsedCommand):\n        """Handle greeting commands"""\n        print("Hello! How can I help you?")\n\n        if self.robot_controller:\n            self.robot_controller(\'greeting\', {\'response\': \'Hello! How can I help you?\'})\n\n    def _handle_acknowledgment(self, command: ParsedCommand):\n        """Handle acknowledgment commands"""\n        print("You\'re welcome!")\n\n        if self.robot_controller:\n            self.robot_controller(\'acknowledgment\', {\'response\': "You\'re welcome!"})\n\n    def _request_clarification(self, command: ParsedCommand):\n        """Request clarification for low-confidence commands"""\n        print(f"I\'m not sure I understood. Did you mean: \'{command.original_text}\'?")\n\n        if self.robot_controller:\n            self.robot_controller(\'request_clarification\', {\n                \'original_command\': command.original_text,\n                \'suggested_commands\': self.parser.get_suggested_commands()\n            })\n\n    def _handle_unknown_command(self, command: ParsedCommand):\n        """Handle commands that couldn\'t be parsed"""\n        print(f"I don\'t know how to handle: \'{command.original_text}\'")\n        print("Try commands like:", ", ".join(self.parser.get_suggested_commands()))\n\n        if self.robot_controller:\n            self.robot_controller(\'unknown_command\', {\n                \'command\': command.original_text,\n                \'suggestions\': self.parser.get_suggested_commands()\n            })\n\n    def _handle_error(self, command: ParsedCommand, error: Exception):\n        """Handle errors during command processing"""\n        print(f"Error processing command \'{command.original_text}\': {error}")\n\n        if self.robot_controller:\n            self.robot_controller(\'error\', {\n                \'command\': command.original_text,\n                \'error\': str(error)\n            })\n\n    def calibrate_microphone(self):\n        """Calibrate microphone for ambient noise"""\n        self.recognizer.calibrate_microphone()\n\n    def get_status(self) -> Dict[str, Any]:\n        """Get current status of the pipeline"""\n        return {\n            \'is_listening\': self.is_listening,\n            \'command_history_length\': len(self.command_history),\n            \'last_command\': self.last_command.original_text if self.last_command else None,\n            \'suggested_commands\': self.parser.get_suggested_commands()\n        }\n'})}),"\n",(0,o.jsx)(e.h3,{id:"step-4-advanced-voice-processing-with-machine-learning",children:"Step 4: Advanced Voice Processing with Machine Learning"}),"\n",(0,o.jsx)(e.p,{children:"Implement more sophisticated voice command processing using ML models:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# advanced_voice_processing.py\n\nimport torch\nimport torch.nn as nn\nimport torchaudio\nimport numpy as np\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\nfrom typing import Dict, List, Tuple\n\nclass AdvancedVoiceProcessor:\n    def __init__(self):\n        # Initialize pre-trained speech recognition model\n        try:\n            self.processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")\n            self.model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")\n            self.use_transformers = True\n        except Exception as e:\n            print(f"Could not load pre-trained model: {e}")\n            print("Falling back to basic processing")\n            self.use_transformers = False\n\n    def preprocess_audio(self, audio_data: np.ndarray, sample_rate: int = 16000) -> torch.Tensor:\n        """Preprocess audio data for model input"""\n        # Resample if needed\n        if sample_rate != 16000:\n            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n            audio_data = resampler(torch.from_numpy(audio_data)).numpy()\n\n        return torch.tensor(audio_data).float()\n\n    def recognize_speech_transformers(self, audio_data: np.ndarray) -> str:\n        """Use pre-trained transformer model for speech recognition"""\n        if not self.use_transformers:\n            return None\n\n        try:\n            # Process audio\n            input_values = self.processor(\n                audio_data,\n                sampling_rate=16000,\n                return_tensors="pt"\n            ).input_values\n\n            # Recognize\n            with torch.no_grad():\n                logits = self.model(input_values).logits\n\n            # Decode\n            predicted_ids = torch.argmax(logits, dim=-1)\n            transcription = self.processor.batch_decode(predicted_ids)[0]\n\n            return transcription\n        except Exception as e:\n            print(f"Error in transformer-based recognition: {e}")\n            return None\n\n    def extract_audio_features(self, audio_data: np.ndarray, sample_rate: int = 16000) -> Dict[str, np.ndarray]:\n        """Extract various audio features for command classification"""\n        import librosa\n\n        features = {}\n\n        # MFCC features\n        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n        features[\'mfccs\'] = np.mean(mfccs, axis=1)\n\n        # Spectral features\n        spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)[0]\n        features[\'spectral_centroids\'] = np.mean(spectral_centroids)\n\n        # Zero crossing rate\n        zcr = librosa.feature.zero_crossing_rate(y=audio_data)[0]\n        features[\'zcr\'] = np.mean(zcr)\n\n        # Chroma features\n        chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)\n        features[\'chroma\'] = np.mean(chroma, axis=1)\n\n        return features\n\n    def classify_command_type(self, audio_features: Dict[str, np.ndarray]) -> str:\n        """Classify the type of command based on audio features"""\n        # This would typically use a trained classifier\n        # For this example, we\'ll use simple heuristics\n\n        # Features that might indicate different command types\n        avg_mfcc = np.mean(audio_features[\'mfccs\'])\n        avg_zcr = audio_features[\'zcr\']\n        avg_spectral = audio_features[\'spectral_centroids\']\n\n        # Simple rule-based classification\n        if avg_zcr > 0.02:  # Higher zero crossing rate might indicate short commands\n            return "short_command"\n        elif avg_mfcc > -100:  # Average MFCC value\n            return "long_command"\n        else:\n            return "unknown"\n\nclass VoiceCommandClassifier(nn.Module):\n    """Neural network for classifying voice command types"""\n    def __init__(self, input_dim: int, num_classes: int):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, num_classes)\n        )\n\n    def forward(self, x):\n        return self.classifier(x)\n\ndef create_voice_command_dataset():\n    """Create a sample dataset for training voice command classifiers"""\n    # This would typically load real audio data\n    # For this example, we\'ll create synthetic data\n    import numpy as np\n\n    # Simulate audio features for different command types\n    num_samples = 1000\n    num_features = 20  # Number of audio features\n\n    # Generate synthetic features for different command types\n    navigation_features = np.random.normal(0.5, 0.2, (num_samples//4, num_features))\n    object_features = np.random.normal(0.7, 0.2, (num_samples//4, num_features))\n    action_features = np.random.normal(0.3, 0.2, (num_samples//4, num_features))\n    question_features = np.random.normal(0.6, 0.2, (num_samples//4, num_features))\n\n    X = np.vstack([navigation_features, object_features, action_features, question_features])\n    y = np.hstack([\n        np.zeros(num_samples//4),      # navigation\n        np.ones(num_samples//4),       # object interaction\n        np.full(num_samples//4, 2),    # action\n        np.full(num_samples//4, 3)     # question\n    ])\n\n    return X, y\n'})}),"\n",(0,o.jsx)(e.h3,{id:"step-5-voice-command-system-integration",children:"Step 5: Voice Command System Integration"}),"\n",(0,o.jsx)(e.p,{children:"Create the complete voice command system with ROS 2 integration:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n# voice_command_system.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom sensor_msgs.msg import AudioData\nfrom voice_recognition import VoiceCommandRecognizer\nfrom command_parser import VoiceCommandParser, ParsedCommand\nfrom speech_intent_pipeline import SpeechIntentPipeline\nimport threading\nimport time\n\nclass VoiceCommandSystem(Node):\n    def __init__(self):\n        super().__init__('voice_command_system')\n\n        # Initialize voice processing components\n        self.voice_pipeline = SpeechIntentPipeline(self.robot_command_callback)\n\n        # Publishers\n        self.command_pub = self.create_publisher(String, '/robot_commands', 10)\n        self.navigation_pub = self.create_publisher(Pose, '/navigation_goal', 10)\n        self.response_pub = self.create_publisher(String, '/voice_response', 10)\n\n        # Subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, '/audio_input', self.audio_callback, 10\n        )\n        self.manual_command_sub = self.create_subscription(\n            String, '/manual_voice_input', self.manual_command_callback, 10\n        )\n\n        # Timer for status updates\n        self.status_timer = self.create_timer(5.0, self.publish_status)\n\n        # State management\n        self.system_active = True\n        self.command_queue = []\n        self.response_queue = []\n\n        # Start voice recognition\n        self.voice_pipeline.start_listening()\n\n        self.get_logger().info('Voice Command System initialized')\n\n    def robot_command_callback(self, intent: str, params: dict):\n        \"\"\"Callback for processed voice commands\"\"\"\n        command_msg = String()\n\n        if intent == 'navigate':\n            # Publish navigation goal\n            nav_pose = Pose()\n            # In a real system, you'd convert location to coordinates\n            self.navigation_pub.publish(nav_pose)\n            command_msg.data = f\"navigating_to_{params.get('location', 'unknown')}\"\n\n        elif intent == 'manipulate':\n            command_msg.data = f\"manipulating_{params.get('object', 'unknown')}\"\n\n        elif intent == 'action':\n            command_msg.data = params.get('command', 'unknown_action')\n\n        elif intent == 'question':\n            command_msg.data = f\"answering_{params.get('question', 'unknown_question')}\"\n\n        elif intent == 'greeting':\n            self.speak_response(params.get('response', 'Hello!'))\n            return\n\n        elif intent == 'acknowledgment':\n            self.speak_response(params.get('response', 'You are welcome!'))\n            return\n\n        elif intent == 'request_clarification':\n            response = f\"I didn't understand. Did you mean: {params.get('original_command', 'command')}?\"\n            self.speak_response(response)\n            return\n\n        elif intent == 'unknown_command':\n            suggestions = ', '.join(params.get('suggestions', []))\n            response = f\"I don't know that command. Try: {suggestions}\"\n            self.speak_response(response)\n            return\n\n        self.command_pub.publish(command_msg)\n\n    def audio_callback(self, msg):\n        \"\"\"Handle audio data from ROS topic\"\"\"\n        # Convert audio message to format expected by speech recognizer\n        # This is a simplified version - actual implementation would depend on audio format\n        pass\n\n    def manual_command_callback(self, msg):\n        \"\"\"Handle manually entered voice commands (for testing)\"\"\"\n        # Process the text as if it were recognized speech\n        self.get_logger().info(f\"Manual command received: {msg.data}\")\n\n        # Parse and handle the command\n        parsed = self.voice_pipeline.parser.parse_command(msg.data)\n        if parsed:\n            self.get_logger().info(f\"Parsed command: {parsed.intent} with entities {parsed.entities}\")\n            self.voice_pipeline._handle_command(parsed)\n\n    def speak_response(self, text: str):\n        \"\"\"Generate speech response\"\"\"\n        # This would use text-to-speech in a real implementation\n        self.get_logger().info(f\"Speaking: {text}\")\n\n        response_msg = String()\n        response_msg.data = text\n        self.response_pub.publish(response_msg)\n\n    def publish_status(self):\n        \"\"\"Publish system status periodically\"\"\"\n        status = self.voice_pipeline.get_status()\n        status_str = f\"Listening: {status['is_listening']}, Commands: {status['command_history_length']}\"\n\n        status_msg = String()\n        status_msg.data = f\"STATUS: {status_str}\"\n        self.response_pub.publish(status_msg)\n\n    def destroy_node(self):\n        \"\"\"Clean up before shutdown\"\"\"\n        self.voice_pipeline.stop_listening()\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    voice_system = VoiceCommandSystem()\n\n    try:\n        rclpy.spin(voice_system)\n    except KeyboardInterrupt:\n        voice_system.get_logger().info('Shutting down Voice Command System')\n    finally:\n        voice_system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,o.jsx)(e.h3,{id:"basic-testing-script",children:"Basic Testing Script"}),"\n",(0,o.jsx)(e.p,{children:"Create a test script to validate your voice command system:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# test_voice_command.py\n\nimport time\nimport threading\nfrom speech_intent_pipeline import SpeechIntentPipeline\n\ndef mock_robot_controller(intent: str, params: dict):\n    """Mock robot controller for testing"""\n    print(f"Robot controller received: {intent} with params {params}")\n\ndef test_voice_pipeline():\n    """Test the voice command pipeline"""\n    print("Testing Voice Command Pipeline...")\n\n    # Create pipeline with mock controller\n    pipeline = SpeechIntentPipeline(mock_robot_controller)\n\n    # Test different types of commands\n    test_commands = [\n        "go to the kitchen",\n        "pick up the red cup",\n        "what time is it",\n        "follow me",\n        "stop",\n        "hello robot",\n        "thank you"\n    ]\n\n    print("\\nTesting command parsing:")\n    for cmd in test_commands:\n        print(f"\\nTesting: \'{cmd}\'")\n        parsed = pipeline.parser.parse_command(cmd)\n        if parsed:\n            print(f"  Intent: {parsed.intent}")\n            print(f"  Entities: {parsed.entities}")\n            print(f"  Confidence: {parsed.confidence:.2f}")\n        else:\n            print("  No parse result")\n\n    # Test continuous listening (in a separate thread)\n    print("\\nStarting continuous listening test (5 seconds)...")\n    pipeline.start_listening()\n\n    # Wait for a few seconds\n    time.sleep(5)\n\n    # Stop listening\n    pipeline.stop_listening()\n\n    # Print status\n    status = pipeline.get_status()\n    print(f"\\nFinal status: {status}")\n\n    print("\\nVoice pipeline test completed!")\n\ndef test_microphone_calibration():\n    """Test microphone calibration"""\n    from voice_recognition import VoiceCommandRecognizer\n\n    print("Testing microphone calibration...")\n    recognizer = VoiceCommandRecognizer()\n    recognizer.calibrate_microphone()\n    print("Calibration completed!")\n\nif __name__ == "__main__":\n    test_microphone_calibration()\n    test_voice_pipeline()\n'})}),"\n",(0,o.jsx)(e.h3,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,o.jsx)(e.p,{children:"Create evaluation metrics for voice command processing:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# evaluate_voice_system.py\n\nimport time\nimport statistics\nfrom typing import List, Dict\nfrom dataclasses import dataclass\n\n@dataclass\nclass VoiceSystemMetrics:\n    recognition_accuracy: float\n    response_time_avg: float\n    response_time_std: float\n    command_success_rate: float\n    false_positive_rate: float\n    false_negative_rate: float\n\nclass VoiceSystemEvaluator:\n    def __init__(self):\n        self.metrics = []\n        self.response_times = []\n        self.correct_recognitions = 0\n        self.total_commands = 0\n        self.false_positives = 0\n        self.false_negatives = 0\n\n    def evaluate_recognition_performance(self, test_commands: List[Dict[str, str]]) -> VoiceSystemMetrics:\n        \"\"\"\n        Evaluate recognition performance with known commands\n        test_commands: List of {'audio_file': path, 'expected_text': text}\n        \"\"\"\n        recognition_times = []\n        correct_recognitions = 0\n\n        for test_case in test_commands:\n            start_time = time.time()\n\n            # Simulate recognition (in real system, this would process audio file)\n            recognized_text = self._simulate_recognition(test_case['audio_file'])\n            recognition_time = time.time() - start_time\n\n            recognition_times.append(recognition_time)\n\n            if recognized_text.lower() == test_case['expected_text'].lower():\n                correct_recognitions += 1\n\n        accuracy = correct_recognitions / len(test_commands) if test_commands else 0\n        avg_response_time = statistics.mean(recognition_times) if recognition_times else 0\n        std_response_time = statistics.stdev(recognition_times) if len(recognition_times) > 1 else 0\n\n        return VoiceSystemMetrics(\n            recognition_accuracy=accuracy,\n            response_time_avg=avg_response_time,\n            response_time_std=std_response_time,\n            command_success_rate=accuracy,  # Simplified\n            false_positive_rate=0,  # Would need more complex evaluation\n            false_negative_rate=0   # Would need more complex evaluation\n        )\n\n    def evaluate_command_understanding(self, test_commands: List[Dict[str, any]]) -> Dict[str, float]:\n        \"\"\"\n        Evaluate command understanding performance\n        test_commands: List of {'spoken_command': text, 'expected_intent': intent, 'expected_entities': entities}\n        \"\"\"\n        correct_intents = 0\n        correct_entities = 0\n        total_commands = len(test_commands)\n\n        for test_case in test_commands:\n            from command_parser import VoiceCommandParser\n            parser = VoiceCommandParser()\n\n            parsed = parser.parse_command(test_case['spoken_command'])\n\n            if parsed and parsed.intent == test_case['expected_intent']:\n                correct_intents += 1\n\n            # Check entity matching (simplified)\n            expected_entities = test_case.get('expected_entities', {})\n            if parsed and parsed.entities == expected_entities:\n                correct_entities += 1\n\n        return {\n            'intent_accuracy': correct_intents / total_commands if total_commands > 0 else 0,\n            'entity_accuracy': correct_entities / total_commands if total_commands > 0 else 0,\n            'total_commands': total_commands\n        }\n\n    def _simulate_recognition(self, audio_file: str) -> str:\n        \"\"\"Simulate speech recognition (in real system, this would process the audio)\"\"\"\n        # This is a placeholder - in real implementation, this would use actual recognition\n        return \"simulated recognition result\"\n\ndef run_comprehensive_evaluation():\n    \"\"\"Run comprehensive evaluation of the voice command system\"\"\"\n    evaluator = VoiceSystemEvaluator()\n\n    print(\"Running comprehensive voice system evaluation...\")\n\n    # Test recognition performance\n    test_commands = [\n        {'audio_file': 'command1.wav', 'expected_text': 'go to kitchen'},\n        {'audio_file': 'command2.wav', 'expected_text': 'pick up cup'},\n        {'audio_file': 'command3.wav', 'expected_text': 'what time is it'}\n    ]\n\n    recognition_metrics = evaluator.evaluate_recognition_performance(test_commands)\n    print(f\"Recognition Accuracy: {recognition_metrics.recognition_accuracy:.2f}\")\n    print(f\"Average Response Time: {recognition_metrics.response_time_avg:.3f}s\")\n\n    # Test command understanding\n    understanding_tests = [\n        {\n            'spoken_command': 'go to the kitchen',\n            'expected_intent': 'navigation',\n            'expected_entities': {'location': 'kitchen'}\n        },\n        {\n            'spoken_command': 'pick up the red cup',\n            'expected_intent': 'object_interaction',\n            'expected_entities': {'object': 'red cup'}\n        }\n    ]\n\n    understanding_metrics = evaluator.evaluate_command_understanding(understanding_tests)\n    print(f\"Intent Accuracy: {understanding_metrics['intent_accuracy']:.2f}\")\n    print(f\"Entity Accuracy: {understanding_metrics['entity_accuracy']:.2f}\")\n\n    print(\"Evaluation completed!\")\n\nif __name__ == \"__main__\":\n    run_comprehensive_evaluation()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"lab-deliverables",children:"Lab Deliverables"}),"\n",(0,o.jsx)(e.p,{children:"Complete the following tasks to finish the lab:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Implement the basic voice recognition system"})," with real-time processing"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Create command parsing and intent recognition"})," functionality"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Build the complete speech-to-intent pipeline"})," with error handling"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Implement advanced processing"})," with ML models (optional)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Integrate with ROS 2"})," for robotic applications"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Test and validate"})," your implementations with sample commands"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Document your results"})," including:","\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Recognition accuracy achieved"}),"\n",(0,o.jsx)(e.li,{children:"Response times measured"}),"\n",(0,o.jsx)(e.li,{children:"Challenges encountered and solutions"}),"\n",(0,o.jsx)(e.li,{children:"Suggestions for improvement"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,o.jsx)(e.p,{children:"Your lab implementation will be assessed based on:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Functionality"}),": Does the voice command system work correctly?"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Accuracy"}),": How well does it recognize and understand commands?"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robustness"}),": How well does it handle noise and variations?"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Integration"}),": How well is it integrated with robotic systems?"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Code Quality"}),": Is the code well-structured and documented?"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"extensions-optional",children:"Extensions (Optional)"}),"\n",(0,o.jsx)(e.p,{children:"For advanced students, consider implementing:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Wake word detection"})," to activate the system"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Speaker identification"})," for personalized responses"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Noise reduction"})," algorithms for better recognition"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Multilingual support"})," for different languages"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Emotion recognition"})," from voice patterns"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,o.jsx)(e.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Microphone access errors:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Check microphone permissions in your OS"}),"\n",(0,o.jsx)(e.li,{children:"Verify microphone is not used by other applications"}),"\n",(0,o.jsx)(e.li,{children:"Try different audio input devices"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Poor recognition accuracy:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Calibrate microphone for ambient noise"}),"\n",(0,o.jsx)(e.li,{children:"Speak clearly and at consistent distance"}),"\n",(0,o.jsx)(e.li,{children:"Check internet connection for cloud-based recognition"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"High latency issues:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Use local speech recognition models"}),"\n",(0,o.jsx)(e.li,{children:"Optimize audio processing pipeline"}),"\n",(0,o.jsx)(e.li,{children:"Reduce model complexity if needed"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:["\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Integration problems:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Verify ROS 2 message formats"}),"\n",(0,o.jsx)(e.li,{children:"Check audio data encoding compatibility"}),"\n",(0,o.jsx)(e.li,{children:"Use appropriate sample rates and formats"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"This lab provided hands-on experience with voice command processing for robotics applications. You learned to capture, process, and understand spoken commands, creating systems that can respond intelligently to natural voice interactions. These capabilities are essential for creating intuitive and accessible human-robot interaction systems."})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function a(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:r(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);