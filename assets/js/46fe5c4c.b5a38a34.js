"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[273],{2425(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-3/lab-7-perception-pipeline","title":"Lab 7: Basic Perception Pipeline with Isaac","description":"Overview","source":"@site/docs/module-3/lab-7-perception-pipeline.md","sourceDirName":"module-3","slug":"/module-3/lab-7-perception-pipeline","permalink":"/hackathon/docs/module-3/lab-7-perception-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/Fatimaali424/hackathon/edit/main/website/docs/module-3/lab-7-perception-pipeline.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Edge Deployment & Optimization","permalink":"/hackathon/docs/module-3/edge-deployment"},"next":{"title":"Lab 8: Motion Planning and Control Implementation","permalink":"/hackathon/docs/module-3/lab-8-motion-control"}}');var a=i(4848),s=i(8453);const r={sidebar_position:5},o="Lab 7: Basic Perception Pipeline with Isaac",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Hardware and Software Requirements",id:"hardware-and-software-requirements",level:2},{value:"Required Hardware",id:"required-hardware",level:3},{value:"Required Software",id:"required-software",level:3},{value:"Lab Setup",id:"lab-setup",level:2},{value:"Environment Configuration",id:"environment-configuration",level:3},{value:"Camera Configuration",id:"camera-configuration",level:3},{value:"Implementation Steps",id:"implementation-steps",level:2},{value:"Step 1: Basic Object Detection Pipeline",id:"step-1-basic-object-detection-pipeline",level:3},{value:"Step 2: Enhanced Perception with 3D Information",id:"step-2-enhanced-perception-with-3d-information",level:3},{value:"Step 3: Perception Pipeline Launch File",id:"step-3-perception-pipeline-launch-file",level:3},{value:"Testing and Validation",id:"testing-and-validation",level:2},{value:"Basic Testing",id:"basic-testing",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"Lab Deliverables",id:"lab-deliverables",level:2},{value:"Assessment Criteria",id:"assessment-criteria",level:2},{value:"Extensions (Optional)",id:"extensions-optional",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"lab-7-basic-perception-pipeline-with-isaac",children:"Lab 7: Basic Perception Pipeline with Isaac"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This lab provides hands-on experience with NVIDIA Isaac's perception capabilities by implementing a basic perception pipeline. You will learn to configure Isaac ROS nodes for object detection, semantic segmentation, and sensor data processing, then integrate these components into a cohesive perception system."}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"After completing this lab, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Configure Isaac ROS perception nodes for object detection"}),"\n",(0,a.jsx)(n.li,{children:"Integrate multiple perception algorithms into a single pipeline"}),"\n",(0,a.jsx)(n.li,{children:"Process and visualize perception results"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate perception pipeline performance"}),"\n",(0,a.jsx)(n.li,{children:"Troubleshoot common perception pipeline issues"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Completion of Module 1 (ROS 2 fundamentals)"}),"\n",(0,a.jsx)(n.li,{children:"Completion of Module 2 (Simulation concepts)"}),"\n",(0,a.jsx)(n.li,{children:"Basic understanding of computer vision concepts"}),"\n",(0,a.jsx)(n.li,{children:"Access to a Jetson platform or Isaac-compatible development environment"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"hardware-and-software-requirements",children:"Hardware and Software Requirements"}),"\n",(0,a.jsx)(n.h3,{id:"required-hardware",children:"Required Hardware"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Jetson Orin AGX/NX or equivalent development platform"}),"\n",(0,a.jsx)(n.li,{children:"RGB-D camera (Intel RealSense D435 or equivalent)"}),"\n",(0,a.jsx)(n.li,{children:"Monitor and input devices for development"}),"\n",(0,a.jsx)(n.li,{children:"Adequate power supply and cooling for Jetson platform"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"required-software",children:"Required Software"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"JetPack 5.1+ with Isaac ROS packages"}),"\n",(0,a.jsx)(n.li,{children:"ROS 2 Humble Hawksbill"}),"\n",(0,a.jsx)(n.li,{children:"Isaac Sim for testing (optional but recommended)"}),"\n",(0,a.jsx)(n.li,{children:"Docker for containerized deployment"}),"\n",(0,a.jsx)(n.li,{children:"Basic development tools (Git, Python 3.10+)"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"lab-setup",children:"Lab Setup"}),"\n",(0,a.jsx)(n.h3,{id:"environment-configuration",children:"Environment Configuration"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Verify Isaac ROS Installation"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Check for Isaac ROS packages\napt list --installed | grep isaac-ros\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Set up workspace"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/isaac_ws/src\ncd ~/isaac_ws\ncolcon build\nsource install/setup.bash\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Verify GPU acceleration"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"nvidia-smi\n# Should show GPU utilization and available memory\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"camera-configuration",children:"Camera Configuration"}),"\n",(0,a.jsx)(n.p,{children:"For this lab, we'll use an Intel RealSense camera. Configure the camera launch file:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- perception_pipeline.launch.xml --\x3e\n<launch>\n  \x3c!-- Launch RealSense camera --\x3e\n  <include file="$(find-pkg-share realsense2_camera)/launch/rs_launch.py">\n    <arg name="enable_rgbd" value="true"/>\n    <arg name="align_depth.enable" value="true"/>\n    <arg name="pointcloud.enable" value="true"/>\n  </include>\n\n  \x3c!-- Launch Isaac object detection --\x3e\n  <node pkg="isaac_ros_detectnet" exec="isaac_ros_detectnet" name="detectnet">\n    <param name="input_topic" value="/camera/color/image_raw"/>\n    <param name="output_topic" value="/detectnet/detections"/>\n    <param name="model_name" value="ssd_mobilenet_v2_coco"/>\n    <param name="confidence_threshold" value="0.7"/>\n  </node>\n\n  \x3c!-- Launch Isaac segmentation --\x3e\n  <node pkg="isaac_ros_segmentation" exec="isaac_ros_segmentation" name="segmentation">\n    <param name="input_image_topic" value="/camera/color/image_raw"/>\n    <param name="output_topic" value="/segmentation/masks"/>\n    <param name="model_name" value="deeplabv3_pascal_voc"/>\n  </node>\n\n  \x3c!-- Launch Isaac depth processing --\x3e\n  <node pkg="isaac_ros_depth_preprocessor" exec="isaac_ros_depth_preprocessor" name="depth_preprocessor">\n    <param name="image_topic" value="/camera/aligned_depth_to_color/image_raw"/>\n    <param name="output_topic" value="/depth_preprocessor/depth_filtered"/>\n    <param name="fill_holes" value="true"/>\n    <param name="max_depth" value="3.0"/>\n  </node>\n</launch>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"implementation-steps",children:"Implementation Steps"}),"\n",(0,a.jsx)(n.h3,{id:"step-1-basic-object-detection-pipeline",children:"Step 1: Basic Object Detection Pipeline"}),"\n",(0,a.jsx)(n.p,{children:"Create the core object detection node:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# perception_pipeline.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray\nfrom geometry_msgs.msg import Point\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass IsaacPerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__('isaac_perception_pipeline')\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Subscribers for camera data\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/color/image_raw',\n            self.image_callback,\n            10\n        )\n\n        self.detection_sub = self.create_subscription(\n            Detection2DArray,\n            '/detectnet/detections',\n            self.detection_callback,\n            10\n        )\n\n        # Publisher for annotated images\n        self.annotated_pub = self.create_publisher(\n            Image,\n            '/perception_pipeline/annotated_image',\n            10\n        )\n\n        # Storage for detections\n        self.latest_detections = None\n        self.latest_image = None\n\n        self.get_logger().info('Isaac Perception Pipeline initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming camera image\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, 'bgr8')\n            self.latest_image = cv_image.copy()\n\n            # If we have detections, annotate the image\n            if self.latest_detections is not None:\n                annotated_image = self.annotate_image(\n                    cv_image, self.latest_detections\n                )\n                annotated_msg = self.cv_bridge.cv2_to_imgmsg(\n                    annotated_image, encoding='bgr8'\n                )\n                annotated_msg.header = msg.header\n                self.annotated_pub.publish(annotated_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def detection_callback(self, msg):\n        \"\"\"Process incoming detections\"\"\"\n        self.latest_detections = msg.detections\n\n    def annotate_image(self, image, detections):\n        \"\"\"Annotate image with detection results\"\"\"\n        annotated = image.copy()\n\n        for detection in detections:\n            # Get bounding box\n            bbox = detection.bbox\n            x = int(bbox.center.position.x - bbox.size_x / 2)\n            y = int(bbox.center.position.y - bbox.size_y / 2)\n            w = int(bbox.size_x)\n            h = int(bbox.size_y)\n\n            # Draw bounding box\n            cv2.rectangle(annotated, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n            # Add label and confidence\n            label = detection.results[0].hypothesis.class_id\n            confidence = detection.results[0].hypothesis.score\n            text = f'{label}: {confidence:.2f}'\n            cv2.putText(\n                annotated, text, (x, y - 10),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2\n            )\n\n        return annotated\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_pipeline = IsaacPerceptionPipeline()\n\n    try:\n        rclpy.spin(perception_pipeline)\n    except KeyboardInterrupt:\n        perception_pipeline.get_logger().info('Shutting down perception pipeline')\n    finally:\n        perception_pipeline.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-2-enhanced-perception-with-3d-information",children:"Step 2: Enhanced Perception with 3D Information"}),"\n",(0,a.jsx)(n.p,{children:"Now let's create a node that combines 2D detections with depth information:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# perception_3d.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray\nfrom geometry_msgs.msg import PointStamped\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass Isaac3DPerception(Node):\n    def __init__(self):\n        super().__init__(\'isaac_3d_perception\')\n\n        self.cv_bridge = CvBridge()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/color/image_raw\', self.image_callback, 10\n        )\n        self.depth_sub = self.create_subscription(\n            Image, \'/camera/aligned_depth_to_color/image_raw\', self.depth_callback, 10\n        )\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, \'/detectnet/detections\', self.detection_callback, 10\n        )\n        self.info_sub = self.create_subscription(\n            CameraInfo, \'/camera/color/camera_info\', self.info_callback, 10\n        )\n\n        # Publishers\n        self.object_3d_pub = self.create_publisher(\n            PointStamped, \'/perception_3d/object_position\', 10\n        )\n\n        # Camera parameters\n        self.camera_matrix = None\n        self.latest_detections = None\n        self.latest_depth = None\n\n        self.get_logger().info(\'Isaac 3D Perception node initialized\')\n\n    def info_callback(self, msg):\n        """Get camera intrinsic parameters"""\n        self.camera_matrix = np.array(msg.k).reshape(3, 3)\n\n    def depth_callback(self, msg):\n        """Process depth image"""\n        try:\n            depth_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\n            self.latest_depth = np.array(depth_image, dtype=np.float32)\n        except Exception as e:\n            self.get_logger().error(f\'Depth callback error: {e}\')\n\n    def detection_callback(self, msg):\n        """Process detections and estimate 3D positions"""\n        self.latest_detections = msg.detections\n\n        if self.latest_depth is not None and self.camera_matrix is not None:\n            for detection in msg.detections:\n                # Calculate 3D position from 2D detection and depth\n                x_center = int(detection.bbox.center.position.x)\n                y_center = int(detection.bbox.center.position.y)\n\n                # Get depth at detection center (with some averaging)\n                depth_roi = self.latest_depth[\n                    max(0, y_center-10):min(self.latest_depth.shape[0], y_center+10),\n                    max(0, x_center-10):min(self.latest_depth.shape[1], x_center+10)\n                ]\n\n                if depth_roi.size > 0:\n                    avg_depth = np.nanmedian(depth_roi[depth_roi > 0])\n                    if not np.isnan(avg_depth) and avg_depth > 0:\n                        # Convert pixel coordinates to 3D world coordinates\n                        point_3d = self.pixel_to_world(\n                            x_center, y_center, avg_depth, self.camera_matrix\n                        )\n\n                        # Publish 3D position\n                        point_msg = PointStamped()\n                        point_msg.header = msg.header\n                        point_msg.point.x = point_3d[0]\n                        point_msg.point.y = point_3d[1]\n                        point_msg.point.z = point_3d[2]\n                        self.object_3d_pub.publish(point_msg)\n\n    def pixel_to_world(self, u, v, depth, camera_matrix):\n        """Convert pixel coordinates to 3D world coordinates"""\n        # Camera intrinsic parameters\n        fx = camera_matrix[0, 0]\n        fy = camera_matrix[1, 1]\n        cx = camera_matrix[0, 2]\n        cy = camera_matrix[1, 2]\n\n        # Convert to world coordinates\n        x = (u - cx) * depth / fx\n        y = (v - cy) * depth / fy\n        z = depth\n\n        return [x, y, z]\n\n    def image_callback(self, msg):\n        """Process image (for synchronization)"""\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    perception_3d = Isaac3DPerception()\n\n    try:\n        rclpy.spin(perception_3d)\n    except KeyboardInterrupt:\n        perception_3d.get_logger().info(\'Shutting down 3D perception node\')\n    finally:\n        perception_3d.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-3-perception-pipeline-launch-file",children:"Step 3: Perception Pipeline Launch File"}),"\n",(0,a.jsx)(n.p,{children:"Create a comprehensive launch file that brings everything together:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# perception_pipeline_launch.py\n\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Declare launch arguments\n    namespace = LaunchConfiguration('namespace')\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n\n    # Declare launch arguments\n    declare_namespace = DeclareLaunchArgument(\n        'namespace',\n        default_value='',\n        description='Top-level namespace'\n    )\n\n    # RealSense camera node\n    realsense_node = Node(\n        package='realsense2_camera',\n        executable='realsense2_camera_node',\n        name='realsense_camera',\n        parameters=[{\n            'enable_color': True,\n            'enable_depth': True,\n            'align_depth.enable': True,\n            'pointcloud.enable': True,\n            'depth_module.profile': '640x480x30',\n            'color_module.profile': '640x480x30'\n        }]\n    )\n\n    # Isaac object detection node\n    detectnet_node = Node(\n        package='isaac_ros_detectnet',\n        executable='isaac_ros_detectnet',\n        name='isaac_detectnet',\n        parameters=[{\n            'input_topic': '/color/image_raw',\n            'output_topic': '/detectnet/detections',\n            'model_name': 'ssd_mobilenet_v2_coco',\n            'confidence_threshold': 0.7,\n            'debug_mode': False\n        }],\n        remappings=[\n            ('/color/image_raw', '/camera/color/image_raw'),\n            ('/detectnet/detections', '/detectnet/detections')\n        ]\n    )\n\n    # Custom perception pipeline node\n    perception_pipeline = Node(\n        package='your_perception_package',\n        executable='perception_pipeline',\n        name='perception_pipeline',\n        parameters=[{\n            'use_sim_time': use_sim_time\n        }]\n    )\n\n    # 3D perception node\n    perception_3d = Node(\n        package='your_perception_package',\n        executable='perception_3d',\n        name='perception_3d',\n        parameters=[{\n            'use_sim_time': use_sim_time\n        }]\n    )\n\n    return LaunchDescription([\n        declare_namespace,\n        realsense_node,\n        detectnet_node,\n        perception_pipeline,\n        perception_3d\n    ])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"testing-and-validation",children:"Testing and Validation"}),"\n",(0,a.jsx)(n.h3,{id:"basic-testing",children:"Basic Testing"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Launch the perception pipeline:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ros2 launch your_perception_package perception_pipeline_launch.py\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Visualize results:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# In another terminal\nrqt_image_view /perception_pipeline/annotated_image\n"})}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Monitor detection topics:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /detectnet/detections\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,a.jsx)(n.p,{children:"Create a simple evaluation script to measure pipeline performance:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n# evaluate_perception.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray\nimport time\n\nclass PerceptionEvaluator(Node):\n    def __init__(self):\n        super().__init__('perception_evaluator')\n\n        self.sub_image = self.create_subscription(\n            Image, '/camera/color/image_raw', self.image_callback, 10\n        )\n        self.sub_detections = self.create_subscription(\n            Detection2DArray, '/detectnet/detections', self.detection_callback, 10\n        )\n\n        self.last_image_time = None\n        self.last_detection_time = None\n        self.latency_samples = []\n\n    def image_callback(self, msg):\n        self.last_image_time = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n\n    def detection_callback(self, msg):\n        if self.last_image_time is not None:\n            detection_time = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n            latency = detection_time - self.last_image_time\n            self.latency_samples.append(latency)\n\n            # Print average latency every 10 samples\n            if len(self.latency_samples) % 10 == 0:\n                avg_latency = sum(self.latency_samples[-10:]) / 10\n                self.get_logger().info(f'Average latency (last 10): {avg_latency:.3f}s')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    evaluator = PerceptionEvaluator()\n\n    try:\n        rclpy.spin(evaluator)\n    except KeyboardInterrupt:\n        # Print final statistics\n        if evaluator.latency_samples:\n            avg_latency = sum(evaluator.latency_samples) / len(evaluator.latency_samples)\n            min_latency = min(evaluator.latency_samples)\n            max_latency = max(evaluator.latency_samples)\n\n            print(f'\\nPerception Pipeline Performance:')\n            print(f'  Average latency: {avg_latency:.3f}s')\n            print(f'  Min latency: {min_latency:.3f}s')\n            print(f'  Max latency: {max_latency:.3f}s')\n            print(f'  Total samples: {len(evaluator.latency_samples)}')\n\n    evaluator.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,a.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"No detections appearing:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Verify camera is publishing images: ",(0,a.jsx)(n.code,{children:"ros2 topic echo /camera/color/image_raw"})]}),"\n",(0,a.jsxs)(n.li,{children:["Check Isaac DetectNet node is running: ",(0,a.jsx)(n.code,{children:"ros2 run isaac_ros_detectnet isaac_ros_detectnet"})]}),"\n",(0,a.jsx)(n.li,{children:"Verify model files are properly installed"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"High latency:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Check GPU utilization: ",(0,a.jsx)(n.code,{children:"nvidia-smi"})]}),"\n",(0,a.jsx)(n.li,{children:"Reduce input resolution"}),"\n",(0,a.jsx)(n.li,{children:"Use lighter model (e.g., MobileNet instead of ResNet)"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Memory errors:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Monitor GPU memory: ",(0,a.jsx)(n.code,{children:"nvidia-smi"})]}),"\n",(0,a.jsx)(n.li,{children:"Reduce batch size in model parameters"}),"\n",(0,a.jsx)(n.li,{children:"Clear unused GPU memory"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Synchronization issues:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use message filters for proper timestamp synchronization"}),"\n",(0,a.jsx)(n.li,{children:"Add delays to allow nodes to initialize"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"lab-deliverables",children:"Lab Deliverables"}),"\n",(0,a.jsx)(n.p,{children:"Complete the following tasks to finish the lab:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Implement the basic perception pipeline"})," with object detection"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Enhance with 3D position estimation"})," using depth information"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Evaluate performance"})," using the provided evaluation script"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Document your results"})," including:","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Average detection latency"}),"\n",(0,a.jsx)(n.li,{children:"Detection accuracy in your test environment"}),"\n",(0,a.jsx)(n.li,{children:"Any challenges encountered and solutions"}),"\n",(0,a.jsx)(n.li,{children:"Suggestions for improvement"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"assessment-criteria",children:"Assessment Criteria"}),"\n",(0,a.jsx)(n.p,{children:"Your lab implementation will be assessed based on:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Functionality"}),": Does the perception pipeline work correctly?"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Performance"}),": Are latency and accuracy within acceptable ranges?"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Code Quality"}),": Is the code well-structured and documented?"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Problem Solving"}),": How effectively did you troubleshoot issues?"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Analysis"}),": Quality of performance evaluation and insights provided"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"extensions-optional",children:"Extensions (Optional)"}),"\n",(0,a.jsx)(n.p,{children:"For advanced students, consider implementing:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Semantic segmentation"})," integration with object detection"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-object tracking"})," using Isaac's tracking nodes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"3D object detection"})," using point cloud data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perception quality metrics"})," and confidence estimation"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This lab provided hands-on experience with Isaac's perception capabilities, from basic object detection to 3D position estimation. You learned to configure, integrate, and evaluate perception components for robotic applications. These skills form the foundation for more complex perception systems in real-world robotic applications."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>o});var t=i(6540);const a={},s=t.createContext(a);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);