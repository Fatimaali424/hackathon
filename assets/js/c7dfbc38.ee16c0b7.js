"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3267],{454(e,n,i){i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module-4/index","title":"Module 4: Vision-Language-Action (VLA)","description":"\ud83d\udccb Module Overview","source":"@site/docs/module-4/index.md","sourceDirName":"module-4","slug":"/module-4/","permalink":"/hackathon/docs/module-4/","draft":false,"unlisted":false,"editUrl":"https://github.com/Fatimaali424/hackathon/edit/main/website/docs/module-4/index.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: Hardware Specifications for AI-Robot Brain","permalink":"/hackathon/docs/module-3/hardware"},"next":{"title":"Vision-Language Integration","permalink":"/hackathon/docs/module-4/vision-language"}}');var o=i(4848),l=i(8453);const t={sidebar_position:1},r="Module 4: Vision-Language-Action (VLA)",a={},d=[{value:"\ud83d\udccb Module Overview",id:"-module-overview",level:2},{value:"\ud83c\udfaf Learning Objectives",id:"-learning-objectives",level:2},{value:"\ud83d\udcda Topics Covered",id:"-topics-covered",level:2},{value:"\ud83e\uddea Labs in this Module",id:"-labs-in-this-module",level:2},{value:"\ud83d\udcdd Assignment",id:"-assignment",level:2},{value:"\ud83d\udee0\ufe0f Tools and Technologies",id:"\ufe0f-tools-and-technologies",level:2},{value:"\ud83d\udd04 Next Steps",id:"-next-steps",level:2}];function c(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,o.jsxs)("div",{style:{display:"flex",alignItems:"center",gap:"1rem",padding:"1rem",backgroundColor:"#f0f8ff",borderRadius:"8px",borderLeft:"4px solid #1a73e8"},children:[(0,o.jsx)(n.h2,{id:"-module-overview",children:"\ud83d\udccb Module Overview"}),(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Duration"}),": Weeks 10-12 | ",(0,o.jsx)(n.strong,{children:"Prerequisites"}),": Module 1, 2 & 3 | ",(0,o.jsx)(n.strong,{children:"Difficulty"}),": Advanced"]})]}),"\n",(0,o.jsx)(n.p,{children:"This module integrates visual perception, natural language understanding, and physical action to create intelligent robotic systems capable of understanding and responding to human commands in real-world environments."}),"\n",(0,o.jsx)(n.h2,{id:"-learning-objectives",children:"\ud83c\udfaf Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this module, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement VLA models for robotic tasks"}),"\n",(0,o.jsx)(n.li,{children:"Process natural language commands for robotic action"}),"\n",(0,o.jsx)(n.li,{children:"Integrate vision and language processing"}),"\n",(0,o.jsx)(n.li,{children:"Design human-robot interaction systems"}),"\n",(0,o.jsx)(n.li,{children:"Create end-to-end trainable robotic systems"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"-topics-covered",children:"\ud83d\udcda Topics Covered"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Vision-Language-Action models and architectures"}),"\n",(0,o.jsx)(n.li,{children:"Natural language processing for robotics"}),"\n",(0,o.jsx)(n.li,{children:"Multimodal perception and reasoning"}),"\n",(0,o.jsx)(n.li,{children:"Task planning from natural language commands"}),"\n",(0,o.jsx)(n.li,{children:"Human-robot interaction and communication"}),"\n",(0,o.jsx)(n.li,{children:"End-to-end learning for VLA systems"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"-labs-in-this-module",children:"\ud83e\uddea Labs in this Module"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"./lab-10-vision-language",children:"Lab 10: Vision-Language Integration"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"./lab-11-voice-command",children:"Lab 11: Voice Command Processing"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"./lab-12-vla-system",children:"Lab 12: VLA System Integration"})}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"-assignment",children:"\ud83d\udcdd Assignment"}),"\n",(0,o.jsxs)(n.p,{children:["Complete the ",(0,o.jsx)(n.a,{href:"./assignment",children:"Module 4 Assignment"})," to apply the concepts learned in this module."]}),"\n",(0,o.jsx)(n.h2,{id:"\ufe0f-tools-and-technologies",children:"\ud83d\udee0\ufe0f Tools and Technologies"}),"\n",(0,o.jsx)(n.p,{children:"This module utilizes:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"OpenVLA"})," - Open-source Vision-Language-Action models"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"CLIP"})," - Vision-language models"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Transformers"})," - Natural language processing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS 2"})," - Integration with robotic systems"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"NVIDIA Isaac"})," - AI inference"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"-next-steps",children:"\ud83d\udd04 Next Steps"}),"\n",(0,o.jsxs)(n.p,{children:["After completing this module, continue to the ",(0,o.jsx)(n.a,{href:"/docs/capstone",children:"Capstone: The Autonomous Humanoid"})," project."]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453(e,n,i){i.d(n,{R:()=>t,x:()=>r});var s=i(6540);const o={},l=s.createContext(o);function t(e){const n=s.useContext(l);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),s.createElement(l.Provider,{value:n},e.children)}}}]);