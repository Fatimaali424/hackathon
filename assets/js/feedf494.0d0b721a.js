"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[9822],{478(n,e,t){t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"capstone/voice-command","title":"Voice Command Processing","description":"Overview","source":"@site/docs/capstone/voice-command.md","sourceDirName":"capstone","slug":"/capstone/voice-command","permalink":"/hackathon/docs/capstone/voice-command","draft":false,"unlisted":false,"editUrl":"https://github.com/Fatimaali424/hackathon/edit/main/website/docs/capstone/voice-command.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Capstone Overview and Requirements","permalink":"/hackathon/docs/capstone/overview"},"next":{"title":"Planning System Integration","permalink":"/hackathon/docs/capstone/planning"}}');var o=t(4848),r=t(8453);const a={sidebar_position:2},s="Voice Command Processing",c={},l=[{value:"Overview",id:"overview",level:2},{value:"System Architecture",id:"system-architecture",level:2},{value:"Voice Command Processing Pipeline",id:"voice-command-processing-pipeline",level:3},{value:"Core Components",id:"core-components",level:2},{value:"1. Audio Input Acquisition",id:"1-audio-input-acquisition",level:3},{value:"2. Speech Recognition Engine",id:"2-speech-recognition-engine",level:3},{value:"3. Natural Language Understanding",id:"3-natural-language-understanding",level:3},{value:"4. Command Interpretation and Classification",id:"4-command-interpretation-and-classification",level:3},{value:"Voice Command Processing Pipeline",id:"voice-command-processing-pipeline-1",level:2},{value:"Complete Voice Command System",id:"complete-voice-command-system",level:3},{value:"Integration with Robot Systems",id:"integration-with-robot-systems",level:2},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Performance and Optimization",id:"performance-and-optimization",level:2},{value:"Real-time Processing Considerations",id:"real-time-processing-considerations",level:3},{value:"Error Handling and Recovery",id:"error-handling-and-recovery",level:2},{value:"Robust Command Processing",id:"robust-command-processing",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"voice-command-processing",children:"Voice Command Processing"})}),"\n",(0,o.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(e.p,{children:"The voice command processing system serves as the primary interface between users and the autonomous humanoid robot. This system enables natural language interaction, allowing users to communicate tasks and commands using everyday language rather than formal programming languages or specific command structures."}),"\n",(0,o.jsx)(e.p,{children:"The voice command processing pipeline consists of several interconnected components: speech recognition, natural language understanding, command interpretation, and action mapping. Each component plays a crucial role in transforming spoken language into executable robot behaviors."}),"\n",(0,o.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,o.jsx)(e.h3,{id:"voice-command-processing-pipeline",children:"Voice Command Processing Pipeline"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"User Speaks Command\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Audio Input   \u2502\n\u2502   Acquisition   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Preprocessing  \u2502\n\u2502   & Filtering   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Speech-to-Text  \u2502\n\u2502   Conversion    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Natural Lang   \u2502\n\u2502   Processing    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Command Inter-  \u2502\n\u2502 pretation &     \u2502\n\u2502 Classification  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Intent-Action   \u2502\n\u2502   Mapping       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u2502 Executable Robot Commands\n"})}),"\n",(0,o.jsx)(e.h2,{id:"core-components",children:"Core Components"}),"\n",(0,o.jsx)(e.h3,{id:"1-audio-input-acquisition",children:"1. Audio Input Acquisition"}),"\n",(0,o.jsx)(e.p,{children:"The audio input acquisition system handles the capture and initial processing of voice commands:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import pyaudio\nimport numpy as np\nimport threading\nimport queue\nimport time\n\nclass AudioInputManager:\n    def __init__(self,\n                 sample_rate=16000,\n                 chunk_size=1024,\n                 channels=1,\n                 sensitivity_threshold=500):\n        self.sample_rate = sample_rate\n        self.chunk_size = chunk_size\n        self.channels = channels\n        self.sensitivity_threshold = sensitivity_threshold\n\n        # Initialize PyAudio\n        self.pyaudio_instance = pyaudio.PyAudio()\n\n        # Audio stream\n        self.stream = None\n        self.is_recording = False\n        self.audio_queue = queue.Queue()\n\n        # Energy detection parameters\n        self.energy_threshold = 1000  # Initial threshold\n        self.dynamic_threshold = True\n        self.min_energy_threshold = 300\n\n        # Recording parameters\n        self.silence_duration = 1.0  # Seconds of silence to stop recording\n        self.max_recording_duration = 10.0  # Max recording time\n\n    def start_recording(self):\n        """Start audio recording"""\n        if self.stream is not None:\n            return\n\n        self.stream = self.pyaudio_instance.open(\n            format=pyaudio.paInt16,\n            channels=self.channels,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n\n        self.is_recording = True\n        self.recording_thread = threading.Thread(target=self._recording_worker)\n        self.recording_thread.daemon = True\n        self.recording_thread.start()\n\n    def stop_recording(self):\n        """Stop audio recording"""\n        self.is_recording = False\n        if self.stream:\n            self.stream.stop_stream()\n            self.stream.close()\n            self.stream = None\n\n    def _recording_worker(self):\n        """Worker thread for audio recording"""\n        silence_frames = 0\n        recording_started = False\n        recording_buffer = []\n        start_time = time.time()\n\n        while self.is_recording:\n            try:\n                # Read audio data\n                data = self.stream.read(self.chunk_size, exception_on_overflow=False)\n                audio_chunk = np.frombuffer(data, dtype=np.int16)\n\n                # Calculate energy\n                energy = np.sum(audio_chunk.astype(np.float32) ** 2) / len(audio_chunk)\n\n                # Check for speech activity\n                if energy > self.energy_threshold:\n                    recording_started = True\n                    recording_buffer.extend(audio_chunk.tolist())\n                    silence_frames = 0\n                elif recording_started:\n                    # Accumulate silence\n                    silence_frames += 1\n                    recording_buffer.extend(audio_chunk.tolist())\n\n                    # Check if enough silence to stop\n                    silence_duration = (silence_frames * self.chunk_size) / self.sample_rate\n                    if silence_duration > self.silence_duration:\n                        # Stop recording and put audio in queue\n                        audio_data = np.array(recording_buffer, dtype=np.int16)\n                        self.audio_queue.put(audio_data)\n                        break\n\n                # Check maximum recording duration\n                if time.time() - start_time > self.max_recording_duration:\n                    audio_data = np.array(recording_buffer, dtype=np.int16)\n                    self.audio_queue.put(audio_data)\n                    break\n\n            except Exception as e:\n                print(f"Error in audio recording: {e}")\n                break\n\n    def get_audio_data(self):\n        """Get recorded audio data"""\n        try:\n            return self.audio_queue.get_nowait()\n        except queue.Empty:\n            return None\n'})}),"\n",(0,o.jsx)(e.h3,{id:"2-speech-recognition-engine",children:"2. Speech Recognition Engine"}),"\n",(0,o.jsx)(e.p,{children:"The speech recognition engine converts audio to text using pre-trained models:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'import torch\nimport torchaudio\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n\nclass SpeechRecognitionEngine:\n    def __init__(self, model_name="facebook/wav2vec2-large-960h-lv60-self"):\n        # Load pre-trained model\n        self.processor = Wav2Vec2Processor.from_pretrained(model_name)\n        self.model = Wav2Vec2ForCTC.from_pretrained(model_name)\n\n        # Move to GPU if available\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        self.model.to(self.device)\n        self.model.eval()\n\n        # Configuration\n        self.sample_rate = 16000\n\n    def preprocess_audio(self, audio_data):\n        """Preprocess audio for speech recognition"""\n        # Convert to tensor\n        if isinstance(audio_data, np.ndarray):\n            audio_tensor = torch.from_numpy(audio_data).float()\n        else:\n            audio_tensor = audio_data.float()\n\n        # Resample if necessary\n        if self.sample_rate != 16000:\n            resampler = torchaudio.transforms.Resample(orig_freq=self.sample_rate, new_freq=16000)\n            audio_tensor = resampler(audio_tensor)\n\n        return audio_tensor.unsqueeze(0)  # Add batch dimension\n\n    def recognize_speech(self, audio_data):\n        """Recognize speech from audio data"""\n        # Preprocess audio\n        input_tensor = self.preprocess_audio(audio_data)\n\n        # Process with processor\n        inputs = self.processor(\n            input_tensor.squeeze(0),\n            sampling_rate=16000,\n            return_tensors="pt",\n            padding=True\n        )\n\n        # Move to device\n        input_values = inputs.input_values.to(self.device)\n        attention_mask = inputs.attention_mask.to(self.device) if "attention_mask" in inputs else None\n\n        # Recognize speech\n        with torch.no_grad():\n            logits = self.model(input_values, attention_mask=attention_mask).logits\n\n        # Decode predictions\n        predicted_ids = torch.argmax(logits, dim=-1)\n        transcription = self.processor.batch_decode(predicted_ids)[0]\n\n        return transcription\n'})}),"\n",(0,o.jsx)(e.h3,{id:"3-natural-language-understanding",children:"3. Natural Language Understanding"}),"\n",(0,o.jsx)(e.p,{children:"The natural language understanding component interprets the recognized text and extracts meaning:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import re\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass CommandInterpretation:\n    intent: str\n    entities: Dict[str, str]\n    confidence: float\n    original_text: str\n    normalized_text: str\n\nclass NaturalLanguageUnderstanding:\n    def __init__(self):\n        # Define command patterns and intents\n        self.intent_patterns = {\n            'navigation': [\n                r'go to (the )?(?P<location>[\\w\\s]+?)(?: room| area| spot| place)?',\n                r'move to (the )?(?P<location>[\\w\\s]+?)(?: room| area| spot| place)?',\n                r'get to (the )?(?P<location>[\\w\\s]+?)(?: room| area| spot| place)?',\n                r'go to (?:the )?(?P<location>[\\w\\s]+?)(?: please| now)?',\n                r'navigate to (?:the )?(?P<location>[\\w\\s]+?)(?: please| now)?',\n                r'bring me to (?:the )?(?P<location>[\\w\\s]+?)(?: please| now)?'\n            ],\n            'object_interaction': [\n                r'(?:pick up|grab|get|take|fetch) (?:the )?(?P<object>[\\w\\s]+?)(?: from (?:the )?(?P<location>[\\w\\s]+?))?',\n                r'(?:pick up|grab|get|take|fetch) (?:the )?(?P<object>[\\w\\s]+?)(?: there| over there)?',\n                r'(?:pick up|grab|get|take|fetch) (?:the )?(?P<object>[\\w\\s]+?)(?: for me| please)?',\n                r'bring me (?:the )?(?P<object>[\\w\\s]+?)(?: from (?:the )?(?P<location>[\\w\\s]+?))?',\n                r'hand me (?:the )?(?P<object>[\\w\\s]+?)(?: from (?:the )?(?P<location>[\\w\\s]+?))?'\n            ],\n            'manipulation': [\n                r'(?:pick up|lift|raise|hold) (?:the )?(?P<object>[\\w\\s]+?)',\n                r'(?:put down|place|set) (?:the )?(?P<object>[\\w\\s]+?)(?: on (?:the )?(?P<surface>[\\w\\s]+?))?',\n                r'(?:move|relocate) (?:the )?(?P<object>[\\w\\s]+?)(?: to (?:the )?(?P<location>[\\w\\s]+?))?',\n                r'(?:open|close) (?:the )?(?P<object>[\\w\\s]+?)',\n                r'(?:grasp|hold|squeeze) (?:the )?(?P<object>[\\w\\s]+?)'\n            ],\n            'action': [\n                r'(?:stop|halt|pause|freeze)',\n                r'(?:start|begin|go|proceed|continue)',\n                r'(?:follow|escort|accompany) (?:me|him|her|them)',\n                r'(?:wait|stand by|hold on|stay)',\n                r'(?:help|assist|aid) (?:me|him|her|them)',\n                r'(?:what can you do|what are you capable of|how can you help)'\n            ],\n            'question': [\n                r'what is (?:this|that|the (?:\\w+ ))(?P<object>[\\w\\s]+?)',\n                r'where is (?:the )?(?P<object>[\\w\\s]+?)',\n                r'where(?:\\'s| is) (?:the )?(?P<object>[\\w\\s]+?)',\n                r'how (?:many|much|long|far|big|small|tall|wide) (?:is|are|does|do) (?:the )?(?P<object>[\\w\\s]+?)',\n                r'what time is it',\n                r'what day is it',\n                r'how are you',\n                r'are you ready'\n            ]\n        }\n\n        # Location synonyms mapping\n        self.location_synonyms = {\n            'kitchen': ['kitchen', 'cooking area', 'cooking room', 'food prep area'],\n            'bedroom': ['bedroom', 'sleeping room', 'bed room', 'sleeping area'],\n            'living room': ['living room', 'living area', 'sitting room', 'lounge', 'family room'],\n            'office': ['office', 'study', 'work room', 'desk area'],\n            'bathroom': ['bathroom', 'restroom', 'toilet', 'bath', 'washroom'],\n            'dining room': ['dining room', 'dining area', 'dining hall', 'eat room'],\n            'hallway': ['hallway', 'corridor', 'passage', 'hall', 'entry'],\n            'garage': ['garage', 'car area', 'parking area'],\n            'garden': ['garden', 'yard', 'outdoor area', 'patio', 'lawn']\n        }\n\n        # Object synonyms mapping\n        self.object_synonyms = {\n            'water': ['water', 'bottle of water', 'water bottle', 'drinking water'],\n            'book': ['book', 'reading book', 'textbook', 'novel', 'magazine'],\n            'cup': ['cup', 'coffee cup', 'mug', 'glass', 'drinking glass'],\n            'phone': ['phone', 'mobile', 'cell phone', 'smartphone', 'cell'],\n            'keys': ['keys', 'key', 'house keys', 'car keys', 'apartment keys'],\n            'coffee': ['coffee', 'cup of coffee', 'hot drink', 'coffee mug'],\n            'snack': ['snack', 'food', 'cookie', 'crackers', 'chips'],\n            'medicine': ['medicine', 'pills', 'medication', 'drugs', 'prescription']\n        }\n\n    def interpret_command(self, text: str) -> Optional[CommandInterpretation]:\n        \"\"\"Interpret natural language command\"\"\"\n        original_text = text\n        text = text.lower().strip()\n\n        # Normalize text\n        normalized_text = self._normalize_text(text)\n\n        # Try to match each intent pattern\n        for intent, patterns in self.intent_patterns.items():\n            for pattern in patterns:\n                match = re.search(pattern, normalized_text)\n                if match:\n                    entities = match.groupdict()\n\n                    # Normalize entities using synonyms\n                    normalized_entities = {}\n                    for key, value in entities.items():\n                        if value:\n                            normalized_value = self._normalize_entity(key, value.strip())\n                            normalized_entities[key] = normalized_value\n\n                    # Calculate confidence based on match quality\n                    confidence = self._calculate_confidence(text, pattern, match)\n\n                    return CommandInterpretation(\n                        intent=intent,\n                        entities=normalized_entities,\n                        confidence=confidence,\n                        original_text=original_text,\n                        normalized_text=normalized_text\n                    )\n\n        # If no pattern matches, try to classify as general command\n        return self._classify_general_command(original_text)\n\n    def _normalize_text(self, text: str) -> str:\n        \"\"\"Normalize text for better matching\"\"\"\n        # Remove extra spaces\n        text = re.sub(r'\\s+', ' ', text.strip())\n\n        # Expand contractions (simple version)\n        contractions = {\n            \"what's\": \"what is\",\n            \"where's\": \"where is\",\n            \"who's\": \"who is\",\n            \"that's\": \"that is\",\n            \"there's\": \"there is\",\n            \"here's\": \"here is\",\n            \"how's\": \"how is\",\n            \"it's\": \"it is\",\n            \"i'm\": \"i am\",\n            \"you're\": \"you are\",\n            \"we're\": \"we are\",\n            \"they're\": \"they are\",\n            \"i've\": \"i have\",\n            \"you've\": \"you have\",\n            \"we've\": \"we have\",\n            \"they've\": \"they have\",\n            \"i'll\": \"i will\",\n            \"you'll\": \"you will\",\n            \"he'll\": \"he will\",\n            \"she'll\": \"she will\",\n            \"it'll\": \"it will\",\n            \"we'll\": \"we will\",\n            \"they'll\": \"they will\",\n            \"isn't\": \"is not\",\n            \"aren't\": \"are not\",\n            \"wasn't\": \"was not\",\n            \"weren't\": \"were not\",\n            \"haven't\": \"have not\",\n            \"hasn't\": \"has not\",\n            \"hadn't\": \"had not\",\n            \"won't\": \"will not\",\n            \"wouldn't\": \"would not\",\n            \"don't\": \"do not\",\n            \"doesn't\": \"does not\",\n            \"didn't\": \"did not\",\n            \"can't\": \"cannot\",\n            \"couldn't\": \"could not\",\n            \"shouldn't\": \"should not\",\n            \"mightn't\": \"might not\",\n            \"mustn't\": \"must not\"\n        }\n\n        for contraction, expansion in contractions.items():\n            text = text.replace(contraction, expansion)\n\n        return text\n\n    def _normalize_entity(self, entity_type: str, value: str) -> str:\n        \"\"\"Normalize entity value using synonym mapping\"\"\"\n        if entity_type == 'location':\n            for canonical, synonyms in self.location_synonyms.items():\n                if value in synonyms:\n                    return canonical\n        elif entity_type == 'object' or entity_type == 'surface':\n            for canonical, synonyms in self.object_synonyms.items():\n                if value in synonyms:\n                    return canonical\n\n        return value\n\n    def _calculate_confidence(self, text: str, pattern: str, match) -> float:\n        \"\"\"Calculate confidence score for the match\"\"\"\n        # Calculate confidence based on text coverage\n        matched_text = match.group(0)\n        confidence = len(matched_text) / len(text) if len(text) > 0 else 0.0\n\n        # Boost confidence if it has captured entities\n        captured_groups = len([g for g in match.groups() if g is not None])\n        if captured_groups > 0:\n            confidence *= 1.2  # Boost for successful entity extraction\n\n        return min(confidence, 1.0)  # Clamp to [0, 1]\n\n    def _classify_general_command(self, text: str) -> CommandInterpretation:\n        \"\"\"Classify commands that don't match specific patterns\"\"\"\n        text_lower = text.lower()\n\n        if any(word in text_lower for word in ['hello', 'hi', 'hey', 'greetings', 'good morning', 'good evening']):\n            return CommandInterpretation(\n                intent='greeting',\n                entities={},\n                confidence=0.8,\n                original_text=text,\n                normalized_text=text_lower\n            )\n        elif any(word in text_lower for word in ['thank', 'thanks', 'thank you', 'appreciate']):\n            return CommandInterpretation(\n                intent='acknowledgment',\n                entities={},\n                confidence=0.8,\n                original_text=text,\n                normalized_text=text_lower\n            )\n        elif any(word in text_lower for word in ['sorry', 'excuse me', 'pardon']):\n            return CommandInterpretation(\n                intent='apology',\n                entities={},\n                confidence=0.7,\n                original_text=text,\n                normalized_text=text_lower\n            )\n        else:\n            return CommandInterpretation(\n                intent='unknown',\n                entities={},\n                confidence=0.3,\n                original_text=text,\n                normalized_text=text_lower\n            )\n"})}),"\n",(0,o.jsx)(e.h3,{id:"4-command-interpretation-and-classification",children:"4. Command Interpretation and Classification"}),"\n",(0,o.jsx)(e.p,{children:"The command interpretation system classifies commands and extracts relevant entities:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class CommandInterpreter:\n    def __init__(self):\n        self.nlu = NaturalLanguageUnderstanding()\n        self.context_manager = ContextManager()\n\n    def interpret(self, text: str, context: Dict = None) -> Dict:\n        \"\"\"Interpret command with context awareness\"\"\"\n        # Get basic interpretation\n        interpretation = self.nlu.interpret_command(text)\n\n        # Enhance with context\n        enhanced_interpretation = self._enhance_with_context(interpretation, context)\n\n        return {\n            'interpretation': enhanced_interpretation,\n            'valid': self._validate_interpretation(enhanced_interpretation),\n            'suggested_corrections': self._suggest_corrections(enhanced_interpretation, text)\n        }\n\n    def _enhance_with_context(self, interpretation: CommandInterpretation, context: Dict) -> CommandInterpretation:\n        \"\"\"Enhance interpretation with contextual information\"\"\"\n        if not context:\n            return interpretation\n\n        enhanced_entities = interpretation.entities.copy()\n\n        # Use context to disambiguate entities\n        if 'current_location' in context and interpretation.intent == 'navigation':\n            if 'location' in enhanced_entities:\n                # Could use context to validate or correct location\n                pass\n\n        # Use context to infer missing information\n        if interpretation.intent == 'object_interaction' and 'location' not in enhanced_entities:\n            if 'current_location' in context:\n                enhanced_entities['location'] = context['current_location']\n\n        return CommandInterpretation(\n            intent=interpretation.intent,\n            entities=enhanced_entities,\n            confidence=interpretation.confidence,\n            original_text=interpretation.original_text,\n            normalized_text=interpretation.normalized_text\n        )\n\n    def _validate_interpretation(self, interpretation: CommandInterpretation) -> bool:\n        \"\"\"Validate if interpretation makes sense\"\"\"\n        if interpretation.confidence < 0.5:\n            return False\n\n        if interpretation.intent == 'navigation':\n            # Validate location exists\n            return 'location' in interpretation.entities and interpretation.entities['location']\n\n        if interpretation.intent == 'object_interaction':\n            # Validate object exists\n            return 'object' in interpretation.entities and interpretation.entities['object']\n\n        return True\n\n    def _suggest_corrections(self, interpretation: CommandInterpretation, original_text: str) -> List[str]:\n        \"\"\"Suggest possible corrections or clarifications\"\"\"\n        suggestions = []\n\n        if interpretation.intent == 'unknown' or interpretation.confidence < 0.6:\n            suggestions.append(f\"I didn't understand '{original_text}'. Could you rephrase?\")\n\n        elif interpretation.intent == 'navigation' and 'location' not in interpretation.entities:\n            suggestions.append(\"I heard a navigation command but didn't catch the destination. Where would you like me to go?\")\n\n        elif interpretation.intent == 'object_interaction' and 'object' not in interpretation.entities:\n            suggestions.append(\"I heard an object interaction command but didn't catch what object. What would you like me to interact with?\")\n\n        return suggestions\n\nclass ContextManager:\n    \"\"\"Manages conversation and environmental context\"\"\"\n    def __init__(self):\n        self.conversation_history = []\n        self.environment_context = {}\n        self.user_preferences = {}\n        self.robot_state = {}\n\n    def update_context(self, user_input: str, robot_response: str, environment_state: Dict):\n        \"\"\"Update context with new information\"\"\"\n        self.conversation_history.append({\n            'user': user_input,\n            'robot': robot_response,\n            'timestamp': time.time()\n        })\n\n        self.environment_context.update(environment_state)\n\n    def get_context(self) -> Dict:\n        \"\"\"Get current context for command interpretation\"\"\"\n        return {\n            'conversation_history': self.conversation_history[-5:],  # Last 5 exchanges\n            'environment': self.environment_context,\n            'user_preferences': self.user_preferences,\n            'robot_state': self.robot_state\n        }\n"})}),"\n",(0,o.jsx)(e.h2,{id:"voice-command-processing-pipeline-1",children:"Voice Command Processing Pipeline"}),"\n",(0,o.jsx)(e.h3,{id:"complete-voice-command-system",children:"Complete Voice Command System"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class VoiceCommandSystem:\n    def __init__(self):\n        # Initialize components\n        self.audio_manager = AudioInputManager()\n        self.speech_recognizer = SpeechRecognitionEngine()\n        self.command_interpreter = CommandInterpreter()\n        self.context_manager = ContextManager()\n\n        # State management\n        self.is_active = False\n        self.listening = False\n        self.wake_word = \"robot\"\n        self.activation_threshold = 0.7\n\n        # Callbacks\n        self.command_callbacks = {}\n\n    def start_listening(self):\n        \"\"\"Start listening for voice commands\"\"\"\n        if not self.is_active:\n            self.audio_manager.start_recording()\n            self.is_active = True\n            self.listening = True\n            print(\"Voice command system activated\")\n\n    def stop_listening(self):\n        \"\"\"Stop listening for voice commands\"\"\"\n        if self.is_active:\n            self.audio_manager.stop_recording()\n            self.is_active = False\n            self.listening = False\n            print(\"Voice command system deactivated\")\n\n    def process_audio_command(self, audio_data: np.ndarray) -> Dict:\n        \"\"\"Process audio command through complete pipeline\"\"\"\n        # Step 1: Recognize speech\n        recognized_text = self.speech_recognizer.recognize_speech(audio_data)\n\n        # Step 2: Interpret command\n        context = self.context_manager.get_context()\n        interpretation_result = self.command_interpreter.interpret(recognized_text, context)\n\n        # Step 3: Validate and handle\n        interpretation = interpretation_result['interpretation']\n\n        if interpretation_result['valid']:\n            # Execute appropriate action based on intent\n            action_result = self._execute_action(interpretation)\n\n            # Update context\n            self.context_manager.update_context(\n                recognized_text,\n                action_result.get('response', 'Action executed'),\n                {}\n            )\n\n            return {\n                'success': True,\n                'interpretation': interpretation,\n                'action_result': action_result,\n                'corrections': []\n            }\n        else:\n            # Request clarification\n            return {\n                'success': False,\n                'interpretation': interpretation,\n                'action_result': None,\n                'corrections': interpretation_result['suggested_corrections']\n            }\n\n    def _execute_action(self, interpretation: CommandInterpretation) -> Dict:\n        \"\"\"Execute action based on command interpretation\"\"\"\n        intent = interpretation.intent\n        entities = interpretation.entities\n\n        # Route to appropriate handler\n        handlers = {\n            'navigation': self._handle_navigation,\n            'object_interaction': self._handle_object_interaction,\n            'manipulation': self._handle_manipulation,\n            'action': self._handle_action,\n            'question': self._handle_question,\n            'greeting': self._handle_greeting,\n            'acknowledgment': self._handle_acknowledgment\n        }\n\n        handler = handlers.get(intent)\n        if handler:\n            return handler(entities, interpretation.original_text)\n        else:\n            return {\n                'status': 'unknown_intent',\n                'message': f'Unknown intent: {intent}',\n                'command': interpretation.original_text\n            }\n\n    def _handle_navigation(self, entities: Dict, original_command: str) -> Dict:\n        \"\"\"Handle navigation commands\"\"\"\n        location = entities.get('location', 'unknown')\n        return {\n            'action': 'navigate',\n            'target': location,\n            'original_command': original_command,\n            'response': f'Navigating to {location}'\n        }\n\n    def _handle_object_interaction(self, entities: Dict, original_command: str) -> Dict:\n        \"\"\"Handle object interaction commands\"\"\"\n        obj = entities.get('object', 'unknown')\n        location = entities.get('location', 'current location')\n        return {\n            'action': 'interact_with_object',\n            'object': obj,\n            'location': location,\n            'original_command': original_command,\n            'response': f'Interacting with {obj} in {location}'\n        }\n\n    def _handle_manipulation(self, entities: Dict, original_command: str) -> Dict:\n        \"\"\"Handle manipulation commands\"\"\"\n        obj = entities.get('object', 'unknown')\n        action_type = original_command.split()[0] if original_command.split() else 'manipulate'\n        return {\n            'action': 'manipulate_object',\n            'object': obj,\n            'manipulation_type': action_type,\n            'original_command': original_command,\n            'response': f'{action_type.title()}ing {obj}'\n        }\n\n    def _handle_action(self, entities: Dict, original_command: str) -> Dict:\n        \"\"\"Handle action commands\"\"\"\n        return {\n            'action': 'execute_action',\n            'command': original_command,\n            'original_command': original_command,\n            'response': f'Executing: {original_command}'\n        }\n\n    def _handle_question(self, entities: Dict, original_command: str) -> Dict:\n        \"\"\"Handle question commands\"\"\"\n        return {\n            'action': 'answer_question',\n            'question': original_command,\n            'entities': entities,\n            'original_command': original_command,\n            'response': f'I will answer your question: {original_command}'\n        }\n\n    def _handle_greeting(self, entities: Dict, original_command: str) -> Dict:\n        \"\"\"Handle greeting commands\"\"\"\n        return {\n            'action': 'greet',\n            'original_command': original_command,\n            'response': 'Hello! How can I assist you today?'\n        }\n\n    def _handle_acknowledgment(self, entities: Dict, original_command: str) -> Dict:\n        \"\"\"Handle acknowledgment commands\"\"\"\n        return {\n            'action': 'acknowledge',\n            'original_command': original_command,\n            'response': 'You\\'re welcome!'\n        }\n"})}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-robot-systems",children:"Integration with Robot Systems"}),"\n",(0,o.jsx)(e.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n# voice_command_ros_integration.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom sensor_msgs.msg import AudioData\nfrom capstone_interfaces.msg import VoiceCommand, RobotAction\n\nclass VoiceCommandROSNode(Node):\n    def __init__(self):\n        super().__init__('voice_command_system')\n\n        # Initialize voice command system\n        self.voice_system = VoiceCommandSystem()\n\n        # Publishers\n        self.command_pub = self.create_publisher(RobotAction, '/robot_commands', 10)\n        self.response_pub = self.create_publisher(String, '/voice_response', 10)\n        self.navigation_pub = self.create_publisher(Pose, '/navigation_goal', 10)\n\n        # Subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData, '/microphone/audio_raw', self.audio_callback, 10\n        )\n        self.voice_command_sub = self.create_subscription(\n            String, '/user_voice_input', self.voice_text_callback, 10\n        )\n\n        # Timer for periodic processing\n        self.process_timer = self.create_timer(0.1, self.process_commands)\n\n        # Start voice system\n        self.voice_system.start_listening()\n\n        self.get_logger().info('Voice Command System Node Initialized')\n\n    def audio_callback(self, msg):\n        \"\"\"Handle audio data from microphone\"\"\"\n        # Convert audio message to numpy array\n        audio_data = np.frombuffer(msg.data, dtype=np.int16)\n\n        # Process through voice command system\n        result = self.voice_system.process_audio_command(audio_data)\n\n        # Handle result\n        self._handle_command_result(result)\n\n    def voice_text_callback(self, msg):\n        \"\"\"Handle text-based voice commands (for testing)\"\"\"\n        # For testing purposes, treat text as recognized speech\n        # In real system, this would come from speech recognition\n\n        # Create a mock interpretation for testing\n        from dataclasses import dataclass\n        @dataclass\n        class MockInterpretation:\n            intent: str\n            entities: dict\n            confidence: float\n            original_text: str\n            normalized_text: str\n\n        interpretation = MockInterpretation(\n            intent='navigation',\n            entities={'location': 'kitchen'},\n            confidence=0.9,\n            original_text=msg.data,\n            normalized_text=msg.data.lower()\n        )\n\n        result = {\n            'success': True,\n            'interpretation': interpretation,\n            'action_result': {\n                'action': 'navigate',\n                'target': 'kitchen',\n                'response': f'Navigating to kitchen'\n            },\n            'corrections': []\n        }\n\n        self._handle_command_result(result)\n\n    def process_commands(self):\n        \"\"\"Periodic command processing\"\"\"\n        # In a real system, this would process queued commands\n        pass\n\n    def _handle_command_result(self, result):\n        \"\"\"Handle command processing result\"\"\"\n        if result['success']:\n            action_result = result['action_result']\n\n            # Publish robot command\n            robot_cmd = RobotAction()\n            robot_cmd.action_type = action_result['action']\n            robot_cmd.target_location = action_result.get('target', '')\n            robot_cmd.object_name = action_result.get('object', '')\n\n            self.command_pub.publish(robot_cmd)\n\n            # Publish response\n            response_msg = String()\n            response_msg.data = action_result['response']\n            self.response_pub.publish(response_msg)\n\n            self.get_logger().info(f\"Command executed: {action_result['response']}\")\n        else:\n            # Handle invalid interpretations\n            for correction in result['corrections']:\n                response_msg = String()\n                response_msg.data = correction\n                self.response_pub.publish(response_msg)\n\n                self.get_logger().info(f\"Suggestion: {correction}\")\n\n    def destroy_node(self):\n        \"\"\"Cleanup before shutdown\"\"\"\n        self.voice_system.stop_listening()\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    voice_node = VoiceCommandROSNode()\n\n    try:\n        rclpy.spin(voice_node)\n    except KeyboardInterrupt:\n        voice_node.get_logger().info('Shutting down Voice Command System')\n    finally:\n        voice_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"performance-and-optimization",children:"Performance and Optimization"}),"\n",(0,o.jsx)(e.h3,{id:"real-time-processing-considerations",children:"Real-time Processing Considerations"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class OptimizedVoiceCommandSystem(VoiceCommandSystem):\n    \"\"\"Optimized version for real-time performance\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        # Caching for frequently accessed data\n        self.command_cache = {}\n        self.max_cache_size = 100\n\n        # Threading for non-blocking operations\n        self.processing_queue = queue.Queue()\n        self.result_queue = queue.Queue()\n\n        # Performance monitoring\n        self.processing_times = []\n        self.max_processing_samples = 100\n\n    def process_audio_command(self, audio_data: np.ndarray) -> Dict:\n        \"\"\"Optimized command processing with performance monitoring\"\"\"\n        start_time = time.time()\n\n        try:\n            # Use cached results when possible\n            audio_hash = hash(tuple(audio_data[:100]))  # Hash first 100 samples\n            if audio_hash in self.command_cache:\n                return self.command_cache[audio_hash]\n\n            # Process normally\n            result = super().process_audio_command(audio_data)\n\n            # Cache result\n            if len(self.command_cache) < self.max_cache_size:\n                self.command_cache[audio_hash] = result\n\n            return result\n\n        finally:\n            # Track performance\n            processing_time = time.time() - start_time\n            self.processing_times.append(processing_time)\n\n            # Keep only recent samples\n            if len(self.processing_times) > self.max_processing_samples:\n                self.processing_times = self.processing_times[-self.max_processing_samples:]\n\n    def get_performance_stats(self) -> Dict:\n        \"\"\"Get performance statistics\"\"\"\n        if not self.processing_times:\n            return {\n                'avg_processing_time_ms': 0,\n                'min_processing_time_ms': 0,\n                'max_processing_time_ms': 0,\n                'processing_samples': 0\n            }\n\n        avg_time = sum(self.processing_times) / len(self.processing_times)\n        min_time = min(self.processing_times)\n        max_time = max(self.processing_times)\n\n        return {\n            'avg_processing_time_ms': avg_time * 1000,\n            'min_processing_time_ms': min_time * 1000,\n            'max_processing_time_ms': max_time * 1000,\n            'processing_samples': len(self.processing_times),\n            'cache_hits': len([k for k, v in self.command_cache.items() if v is not None])\n        }\n"})}),"\n",(0,o.jsx)(e.h2,{id:"error-handling-and-recovery",children:"Error Handling and Recovery"}),"\n",(0,o.jsx)(e.h3,{id:"robust-command-processing",children:"Robust Command Processing"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"class RobustVoiceCommandSystem(OptimizedVoiceCommandSystem):\n    \"\"\"Robust version with comprehensive error handling\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.error_handlers = {\n            'speech_recognition_error': self._handle_speech_recognition_error,\n            'command_interpretation_error': self._handle_command_interpretation_error,\n            'action_execution_error': self._handle_action_execution_error\n        }\n\n    def process_audio_command(self, audio_data: np.ndarray) -> Dict:\n        \"\"\"Process command with comprehensive error handling\"\"\"\n        try:\n            return super().process_audio_command(audio_data)\n        except Exception as e:\n            error_type = self._classify_error(e)\n            return self.error_handlers[error_type](e, audio_data)\n\n    def _classify_error(self, error: Exception) -> str:\n        \"\"\"Classify error type\"\"\"\n        error_msg = str(error).lower()\n\n        if 'speech' in error_msg or 'recognit' in error_msg:\n            return 'speech_recognition_error'\n        elif 'command' in error_msg or 'interpret' in error_msg:\n            return 'command_interpretation_error'\n        elif 'action' in error_msg or 'execut' in error_msg:\n            return 'action_execution_error'\n        else:\n            return 'unknown_error'\n\n    def _handle_speech_recognition_error(self, error: Exception, audio_data: np.ndarray) -> Dict:\n        \"\"\"Handle speech recognition errors\"\"\"\n        return {\n            'success': False,\n            'interpretation': None,\n            'action_result': None,\n            'corrections': ['I couldn\\'t understand your voice command. Could you speak more clearly?'],\n            'error': f'Speech recognition error: {str(error)}'\n        }\n\n    def _handle_command_interpretation_error(self, error: Exception, audio_data: np.ndarray) -> Dict:\n        \"\"\"Handle command interpretation errors\"\"\"\n        return {\n            'success': False,\n            'interpretation': None,\n            'action_result': None,\n            'corrections': ['I didn\\'t understand that command. Could you rephrase it?'],\n            'error': f'Command interpretation error: {str(error)}'\n        }\n\n    def _handle_action_execution_error(self, error: Exception, audio_data: np.ndarray) -> Dict:\n        \"\"\"Handle action execution errors\"\"\"\n        return {\n            'success': False,\n            'interpretation': None,\n            'action_result': None,\n            'corrections': ['I encountered an error executing that command. Would you like me to try again?'],\n            'error': f'Action execution error: {str(error)}'\n        }\n"})}),"\n",(0,o.jsx)(e.p,{children:"This voice command processing system provides a comprehensive foundation for natural language interaction with the autonomous humanoid robot. The system handles audio input, speech recognition, natural language understanding, and action mapping in a robust and efficient manner, suitable for real-time robotic applications."})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453(n,e,t){t.d(e,{R:()=>a,x:()=>s});var i=t(6540);const o={},r=i.createContext(o);function a(n){const e=i.useContext(r);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),i.createElement(r.Provider,{value:e},n.children)}}}]);